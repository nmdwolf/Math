\chapter{Calculus}
\section{Sequences}\index{sequence}

	\newdef{Limit superior}{\index{limit}
        	Let $(x_i)_{i\in\mathbb{N}}$ be a sequence of real numbers. The limit superior is defined as follows:
        	\begin{equation}
			\label{calculus:limit_superior}
                	\limsup_{i\rightarrow+\infty}x_i = \inf_{i\geq1}\left\{\sup_{k\geq i}x_k\right\}
		\end{equation}
        }
        \newdef{Limit inferior}{
        	Let $(x_i)_{i\in\mathbb{N}}$ be a sequence of real numbers. The limit superior is defined as follows:
	        \begin{equation}
			\label{calculus:limit_inferior}
        	        \liminf_{i\rightarrow+\infty}x_i = \sup_{i\geq1}\left\{\inf_{k\geq i}x_k\right\}
		\end{equation}
        }
        
        \begin{theorem}
		A sequence $(x_i)_{i\in\mathbb{N}}$ converges pointwise if and only if $\limsup_{i\rightarrow+\infty} x_i = \liminf_{i\rightarrow+\infty} x_i$.
	\end{theorem}

\section{Continuity}\index{continuity}
        
        \newdef{Lipschitz continuity}{\index{Lipschitz!continuity}
        	A function $f:\mathbb{R}\rightarrow\mathbb{R}$ is Lipschitz continuous if there exists a constant $C>0$ such that
	        \begin{equation}
        	    	\label{calculus:lipschitz_continuity}
        	    	|f(x) - f(x')|\leq C|x-x'|
        	\end{equation}
	        for all $x, x'\in\mathbb{R}$.
        }
	
	\begin{theorem}[Darboux's theorem]\index{Darboux!theorem for differentiable functions}
		Let $f$ be a differentiable function on a closed interval $I$. Then $f'$ has the intermediate value property\footnote{This means that the function satisfies the conclusion of the intermediate value theorem \ref{topology:theorem:intermediate_value_theorem}.}.
	\end{theorem}
	\begin{remark}[Darboux function]\index{Darboux!function}
		Functions that have the intermediate value property are called Darboux functions.
	\end{remark}
        
        \begin{result}[Bolzano]\index{Bolzano}
        	If $f(a)<0$ and $f(b)>0$ (or vice versa) then there exists at least one point $x_0$ where $f(x_0)=0$.
        \end{result}
        \begin{result}
		The image of a compact set is also a compact set.
	\end{result}
        
        \begin{theorem}[Weierstrass' extreme value theorem]\index{Weierstrass!extreme value theorem}
		Let $I=[a,b]\subset\mathbb{R}$ be a compact interval. Let $f$ be a continuous function defined on $I$. Then $f$ attains a minimum and maximum at least once on $I$.
 	\end{theorem}
        
\section{Convergence}\index{convergence}

    	\newdef{Pointwise convergence}{
        	Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions. The sequence is said to converge pointwise to a limit function $f(x)$ if
	        \begin{equation}
			\forall x\in\operatorname{dom}(f_n):\lim_{n\rightarrow+\infty}f_n(x) = f(x)
		\end{equation}
        }
        \newdef{Uniform convergence}{
        	Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions. The sequence is said to converge uniformly to a limit function $f(x)$ if
        	\begin{equation}
			\sup_{x\in\operatorname{dom}(f_n)}\left\{|\lim_{n\rightarrow+\infty}f_n(x) - f(x)|\right\} = 0
		\end{equation}
        }

\section{Derivative}\index{derivative}
\subsection{Single variable}
        	
	\newformula{Derivative}{
        	\begin{equation}
        	        \label{calculus:derivative}
        	        \boxed{f'(x) = \lim_{h\rightarrow0}\stylefrac{f(x+h) - f(x)}{h}}
                \end{equation}
	}
            
        \begin{theorem}[Mean value theorem]\index{mean!value theorem}
            	Let $f$ be continuous on the closed interval $[a, b]$ and differentiable on the open interval $]a, b[$. Then there exists a point $c\in]a, b[$ such that:
            	\begin{equation}
            		\label{calculus:mean_value_theorem}
            		f'(c) = \stylefrac{f(b) - f(a)}{b-a}
            	\end{equation}
	\end{theorem}
            
        \newdef{Differentiablity class}{\label{calculus:differentiablity_class}
                Let $I$ be a set. Let $f$ be a function defined on $I$. If $f$ is $n$ times continuously differentiable on $I$ (i.e. $f^{(i)}$ exists and is continuous for $i=1,\dotso,n$) then $f$ is said to be of class \textbf{C}$^n(I)$.
	}
        
        \newdef{Smooth function}{\index{smooth}\label{calculus:smooth}
                A function $f$ is said to be smooth if it is of class \textbf{C$^\infty$}.
	}
        \newdef{Analytic function}{\index{analytic}\label{calculus:analytic}
        	\nomenclature[S_set_analytic]{C$^\omega(V)$}{Set of all analytic function on a set $V$.}
                A function $f$ is said to be analytic if it is smooth and if its Taylor series expansion around any point $x_0$ converges to $f$ in some neighbourhood of $x_0$. The set of analytic functions defined on $V$ is denoted by C$^\omega(V)$.
	}

	\begin{theorem}[Schwarz's theorem\footnotemark]\index{Schwarz!theorem for second derivatives}\index{Clairaut's theorem}
		\footnotetext{Also called \textbf{Clairaut's theorem}.}
		Let $f\in C^2(\mathbb{R}^n, \mathbb{R})$, then:
		\begin{equation}
			\pderiv{}{x_i}\left(\pderiv{f}{x_j}\right) = \pderiv{}{x_j}\left(\pderiv{f}{x_i}\right)
		\end{equation}
		for all indices $i, j\leq n$.
	\end{theorem}

	\begin{method}[Derivative of $f(x)^{g(x)}$]
		Let us consider a function of the form $u(x)=f(x)^{g(x)}$. To find the derivative of this function we can use the derivative of the natural logarithm:\par
				
		\noindent First we take a look at the natural logarithm of the function: \[\ln[u(x)] = g(x)\ln[f(x)]\] Then we look at the derivative of the natural logarithm:
		\[\deriv{\ln[u(x)]}{x} = \stylefrac{1}{u(x)}\deriv{u(x)}{x} \implies \deriv{u(x)}{x} = u(x)\deriv{\ln[u(x)]}{x}\]
		But according to the first equation we also have:\[\deriv{\ln[u(x)]}{x} = \deriv{}{x}g(x)\ln[f(x)] = \deriv{g(x)}{x}\ln[f(x)] + \stylefrac{g(x)}{f(x)}\deriv{f(x)}{x}\] Combining these two equations gives:
		\begin{equation}
			\label{calculus:derivative_f^g}
			\boxed{\deriv{}{x}\left[f(x)^{g(x)}\right] = f(x)^{g(x)}\left[\deriv{g}{x}(x)\ln[f(x)] + \stylefrac{g(x)}{f(x)}\deriv{f}{x}(x)\right]}
		\end{equation}
	\end{method}
	
	\begin{theorem}[Euler's homogeneous function theorem]\index{homogeneous}\index{Euler!homogeneous function theorem}
		Let $f$ be a homogeneous function, i.e. $f(ax_1, ..., ax_n) = a^nf(x_1, ..., x_n)$. Then f satisfies following equality:
		\begin{equation}
			\label{calculus:theorem:euler_homogeneous_functions}
			\sum_kx_k\pderiv{f}{x_k} = nf(x_1, ..., x_n)
		\end{equation}
	\end{theorem}

\section{Riemann integral}\index{Riemann!integral}

    	\newdef{Improper Riemann integral}{
        	\begin{equation}
			\label{calculus:improper_integral}
	                \boxed{\int_{-\infty}^{+\infty}f(x)dx = \lim_{\substack{a\rightarrow-\infty\\ b\rightarrow+\infty}}\int_a^bf(x)dx}
		\end{equation}
        }

\section{Fundamental theorems}\index{Fundamental theorem!of calculus}
    
	\begin{theorem}[First fundamental theorem of calculus]
		Let $f(x)$ be a continuous  function defined on the open interval $I$. Let $c \in I$. The following theorem establishes a link between integration and differentiation:
		\begin{equation}
			\label{calculus:first_fundamental_theorem}
        	        \boxed{\exists F(x) = \int_c^xf(x')dx':F'(x) = f(x)}
		\end{equation}
		Furthermore this function $F(x)$ is uniformly continuous on $I$.
	\end{theorem}
        
        \begin{remark}
		The function $F(x)$ in the previous theorem is called a \textbf{primitive function} of $f(x)$. Remark that $F(x)$ is just 'a' primitive function as adding a constant to $F(x)$ does not change anything because the derivative of a constant is zero.
	\end{remark}
    
    	\begin{theorem}[Second fundamental theorem of calculus]
		Let $f(x)$ be a function defined on the interval $[a, b]$. Furthermore, let $f(x) \in C^1[a, b]$. We then find the following important theorem:
        	\begin{equation}
			\label{calculus:second_fundamental_theorem}
                	\boxed{\int_a^bf'(x)dx = f(b) - f(a)}
		\end{equation}
	\end{theorem}
        
        \begin{theorem}[Differentiation under the integral sign\footnotemark]
			\begin{equation}
            	\label{calculus:diff_under_integral}
            	\boxed{\deriv{}{x}\int_{a(x)}^{b(x)}f(x, y)dy = f(x, b(x))\cdot b'(x) - f(x, a(x))\cdot a'(x) + \int_{a(x)}^{b(x)}\pderiv{f(x, y)}{x}dy}
			\end{equation}
		\end{theorem}
        \footnotetext{This is a more general version of the so called 'Leibnitz integral rule'.}
        
\section{Taylor expansion}\index{Taylor!series}

       	\newformula{Exponential function}{
            	\begin{equation}
			\label{calculus:taylor_exponential_function}
	                e^x = \sum_{n=0}^\infty \stylefrac{x^n}{n!}
		\end{equation}
        }
    
\section{Euler integrals}
\subsection{Euler integral of the first kind}

       	\newformula{Beta function}{\index{beta function}\index{Euler!integral of the first kind}
            	\begin{equation}
			\label{calculus:beta_function}
	                \boxed{B(x, y) = \int_0^1t^{x-1}(1-t)^{y-1}dt}
		\end{equation}
        }
            
\subsection{Euler integral of the second kind}

       	\newformula{Gamma function}{\index{gamma!function}\index{Euler!integral of the second kind}
            	\begin{equation}
			\label{calculus:gamma_function}
	                \boxed{\Gamma(x) = \int_0^{+\infty}t^{x-1}e^{-t}dt}
		\end{equation}
        }
    		
        \begin{formula}
		B(x, y) = \stylefrac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
	\end{formula}

        \newformula{Recursion formula}{
            	Let $n!$ denote the factorial for integer numbers.
                \begin{equation}
                	\label{calculus:gamma_factorial_relation}
			\Gamma(n) = (n-1)!
		\end{equation}
        }
        \newformula{Stirling}{\index{Stirling}
        	\begin{equation}
        		\label{calculus:stirling}
        		\ln(n!) \approx n\ln n - n
        	\end{equation}
        }
        
\section{Gaussian integrals}\index{Gauss!integral}

	\newformula{$n$-dimensional Gaussian integral}{
		A general Gaussian integral is of the form
		\begin{equation}
			I(A, \vector{b}) = \int_{-\infty}^{+\infty}d^nx\exp\left(-\frac{1}{2}\vector{x}\cdot A\vector{x} + \vector{b}\cdot\vector{x}\right)
		\end{equation}
		where $A$ is a real symmetric matrix. By performing the transformation $\vector{x}\rightarrow A^{-1}\vector{b} - \vector{x}$ and diagonalising $A$ one can obtain the following result:
		\begin{equation}
			\label{calculus:gaussian_integral}
			I(A, \vector{b}) = (2\pi)^{n/2}\det(A)^{-1/2}\exp\left(\frac{1}{2}\vector{b}\cdot A^{-1}\vector{b}\right)
		\end{equation}
	}
	\begin{result}
		A functional generalisation is given by:
		\begin{align}
			I(iA, iJ) &= \int[d\varphi]\exp\left(-i\int d^nxd^ny\ \varphi(x)A(x, y)\varphi(y) + i\int d^nx\ \varphi(x)J(x)\right)\nonumber\\
			&= C\det(A)^{-1/2}\exp\left(\frac{i}{2}\int d^nxd^ny\ J(x)A^{-1}(x, y)J(y)\right)
		\end{align}
		where we used the analytic continuation $I(iA, iJ)$ of equation \ref{calculus:gaussian_integral}. One should pay attention to the normalisation factor $C$ which is infinite in general.
	\end{result}

\chapter{Series}

\section{Convergence tests}\index{convergence}

	\begin{theorem}
		A series $\sum_{i=1}^{+\infty} a_i$ can only converge if $\lim_{i\rightarrow+\infty} a_i = 0$.
	\end{theorem}
    	\newprop{Absolute/conditional convergence}{
    		If $S' = \sum_{i=1}^{+\infty}|a_i|$ converges then so does the series $S = \sum_{i=1}^{+\infty}a_i$ and $S$ is said to be absolutely convergent. If $S$ converges but $S'$ does not, then $S$ is said to be conditionally convergent.
	}

	\newdef{Majorizing series}{\index{majorization}
	    	Let $S_a = \sum_{i=1}^{+\infty}a_i$ and $S_b = \sum_{i=1}^{+\infty}b_i$ be two series. The series $S_a$ is said to majorize $S_b$ if for every $k>0$ the partial sum $S_{a, k}\geq S_{b, k}$.
	}
	\begin{method}[Comparison test]\index{convergence!comparison test}
		Let $S_a, S_b$ be two series such that $S_a$ majorizes $S_b$. We have the following cases:
	        \begin{itemize}
			\item If $S_b$ diverges, then $S_a$ diverges.
        		\item If $S_a$ converges, then $S_b$ converges.
		        \item If $S_b$ converges, nothing can be said about $S_a$.
		        \item If $S_a$ diverges, nothing can be said about $S_b$. 
		\end{itemize}
	\end{method}

	\begin{method}[MacLaurin-Cauchy integral test]\index{convergence!integral test}
		Let $f$ be a non-negative continuous monotone decreasing function on the interval $[n,+\infty[$. If $\int_n^{+\infty}f(x)dx$ is convergent then so is $\sum_{k=n}^{+\infty}f(k)$. On the other hand, if the integral is divergent, so is the series.
	\end{method}
    	\begin{remark}
    		The function does not have to be non-negative and decreasing on the complete interval. As long as it does on the interval $[N,+\infty[$ for some $N\geq n$. This can be seen by writing $\sum_{k=n}^{+\infty}f(k) = \sum_{k=n}^Nf(k) + \sum_{k=N}^{+\infty}f(k)$ and noting that the first term is always finite (the same argument applies for the integral).
    	\end{remark}
    
	\begin{property}
		If the integral in the previous theorem converges, then the series has following lower and upper bounds:
	        \begin{equation}
			\int_n^{+\infty}f(x)dx \leq \sum_{i=n}^{+\infty}a_i \leq f(n) + \int_n^{+\infty}f(x)dx
		\end{equation}
	\end{property}

	\begin{method}[d'Alembert's ratio test]\index{convergence!ratio test}\index{d'Alembert!test}
		\begin{equation}
			R = \lim_{n\rightarrow+\infty}\left|\stylefrac{a_{n+1}}{a_n}\right|
		\end{equation}
		Following cases arise:
	        \begin{itemize}
			\item $R < 1$: the series converges absolutely
		        \item $R > 1$: the series does not converge
		        \item $R = 1$: the test is inconclusive
		\end{itemize}
	\end{method}

	\begin{method}[Cauchy's root test]\index{convergence!root test}\index{Cauchy!root test}
		\begin{equation}
			R = \limsup_{n\rightarrow+\infty}\sqrt[n]{|a_n|}
		\end{equation}
	        We have the following cases:
	        \begin{itemize}
			\item $R < 1$: the series converges absolutely
		        \item $R > 1$: the series does not converge
		        \item $R = 1$ and the limit approaches strictly from above: the series diverges
		        \item $R = 1$: the test is inconclusive
		\end{itemize}
	\end{method}
	\newdef{Radius of convergences}{\index{convergence!radius}
    		The number $\stylefrac{1}{R}$ is called the radius of convergence.
	}
	\begin{remark}
		The root test is stronger than the ratio test. Whenever the ratio test determines the convergence/divergence of a series, the radius of convergence of both tests will coincide.
	\end{remark}

	\begin{method}[Gauss's test]\index{convergence!Gauss's test}
		If $u_n>0$ for all $n$ then we can write the ratio of successive terms as follows:
		\begin{equation}
			\label{series:gauss_test}
		        \left|\stylefrac{u_n}{u_{n+1}}\right| = 1 + \stylefrac{h}{n} + \stylefrac{B(n)}{n^k}
		\end{equation}
		where $k > 1$ and $B(n)$ is a bounded function when $n\rightarrow\infty$. The series converges if $h > 1$ and diverges otherwise.
    	\end{method}
    
\section{Asymptotic expansions}
	
	\newdef{Asysmptotic expansion}{\index{asymptotic!expansion}
    		Let $f(x)$ be a continuous function. A series expansion of order $N$ is called an asymptotic expansion of $f(x)$ if it satisfies:
        	\begin{equation}
        		\label{series:asymptotic_expansion}
        		f(x) - \sum_{n = 0}^N = O(x^{N+1})
	        \end{equation}
	}
    
	\newmethod{Borel transform$^\dag$}{\index{Borel!transform}\label{calculus:borel_transform}
	    	Define the function $F(x) = \ds\sum_{n=0}^{+\infty}\stylefrac{a_n}{n!}x^n$. If the integral
	        \begin{equation}
	        	\int_0^{+\infty}e^{-t}F(xt)dt < +\infty
	        \end{equation}
	        for all $x\in\mathbb{R}$ then $F(x)$ is called the Borel transform of $f(x)$. Furthermore the integral will give a convergent expression for $f(x)$.
	}

	\begin{theorem}[Watson]\index{Watson's theorem}
    		The uniqueness of the function $F(x)$ is guaranteed if the function $f(x)$ is holomorphic on the domain $\{z\in\mathbb{C} : |\arg(z)| < \frac{\pi}{2} + \varepsilon\}$.
	\end{theorem}

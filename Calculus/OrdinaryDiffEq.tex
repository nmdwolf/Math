\chapter{Ordinary differential equations}

\section{Boundary conditions}\index{boundary!condition}

    Unique solutions of a differential equation are obtained by supplying additional conditions. These are called boundary conditions.

    \newdef{Periodic boundary conditions}{
        Boundary conditions of the following form:
        \begin{gather}
            \label{diffeq:conditions:periodic}
            y(x) = y(x + \varphi).
        \end{gather}
        By induction it follows that for every $n$
        \begin{gather}
            \label{diffeq:conditions:periodic_n}
            y(x) = y(x + n\varphi).
        \end{gather}
    }

    \newdef{Dirichlet boundary conditions}{
        Boundary conditions of the following form:
        \begin{gather}
            \label{diffeq:conditions:dirichlet}
            y(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$ where $\Omega$ is the domain on which the problem is defined.
    }

    \newdef{Neumann boundary conditions}{
        Boundary conditions of the following form:
        \begin{gather}
            \label{diffeq:conditions:neumann}
            \pderiv{y}{\hat{n}}(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$ where $\Omega$ is the domain on which the problem is defined.
    }

\section{Existence and uniqueness}

    \begin{theorem}[Picard-Lindel\"of]\index{Picard-Lindel\"of}\label{diffeq:picard_lindelof}
        Consider an ordinary differential equation of the form
        \begin{gather}
            \dot{x}(t) = f(t, x(t))
        \end{gather}
        where $f$ is defined on a subset $I\times U\subset\mathbb{R}\times\mathbb{R}^n$.\footnote{Generalizations to arbitrary Banach spaces exist, see e.g. \cite{AMP1}.} If $f$ is continuous on $I$ and locally Lipschitzian on $U$, then for every point $(t_0, x_0)\in I\times U$ there exists a maximal interval $J\supseteq I$ such that there is a unique solution $x:J\rightarrow\mathbb{R}^n$ of the ODE with initial condition $(t_0, x_0)$.
    \end{theorem}

\section{First order ODE's}

    \newdef{First order ODE}{
        \begin{gather}
            \label{diffeq:first_order_ODE}
            y'(t) + a(t)y(t) = R(t)
        \end{gather}
        If the function $R(t)$ is identically zero, then the ODE is said to be \textbf{homogenous}.
    }
    \begin{formula}
        Let $U\subseteq\mathbb{R}$ be an open set. Let the functions $a(t), R(t):U\rightarrow\mathbb{R}$ be continuous. The solutions $\varphi(t):U\rightarrow\mathbb{R}$ of equation \ref{diffeq:first_order_ODE} are given by:
        \begin{gather}
            \label{diffeq:first_order_general_solution}
            \varphi(t) = e^{-\int a(t)dt}\left(c + \int R(t)e^{\int a(t)dt}dt\right)
        \end{gather}
        where $c$ is a constant (in general determined by some kind of boundary condition).
    \end{formula}

\section{Second order ODE's}

    \newdef{Second order ODE}{
        \begin{gather}
            \label{diffeq:second_order_ODE}
            y''(t) + a(t)y'(t) + b(t)y(t) = R(t)
        \end{gather}
        If the function $R(t)$ is identically zero, then the ODE is said to be \textbf{homogenous}.
    }

\subsection{General solution}

    \begin{formula}
        Let $\varphi:U\rightarrow\mathbb{R}$ be a nowhere zero solution of the homogeneous equation. The general solution of equation \ref{diffeq:second_order_ODE} is then given by
        \begin{gather}
            \label{diffeq:second_order_general_solution}
            y(t) = c_1\ \varphi +  c_2\ \varphi\int\stylefrac{e^{-\int a}}{\varphi^2} + \psi_0
        \end{gather}
        where $\psi_0$ is a particular solution of equation \ref{diffeq:second_order_ODE}.
    \end{formula}

    \begin{property}
        Let $\psi_0$ be a solution of equation \ref{diffeq:second_order_ODE}. The set of all solutions is given by the affine space
        \begin{gather}
            \big\{\psi_0 + \chi:\chi\text{ is a solution of the homogeneous equation}\big\}.
        \end{gather}
    \end{property}
    \begin{property}\index{Wronskian}
        Two solutions of the homogeneous equation are independent if the \textbf{Wronskian} is nonzero:
        \begin{gather}
            \label{diffeq:wronskian}
            W\left(\varphi_1(x), \varphi_2(x)\right) := \left|
            \begin{array}{cc}
                \varphi_1(x)&\varphi_2(x)\\
                \varphi_1'(x)&\varphi_2'(x)
            \end{array}
            \right|\neq 0.
        \end{gather}
    \end{property}

    \newformula{Abel's identity}{\index{Abel!identity for ODE's}
        An explicit formula for the Wronskian is given by
        \begin{gather}
            \label{diffeq:abels_identity}
            W(x) = W(x_0)\exp\left(-\int^x_{x_0}a(x')dx'\right).
        \end{gather}
    }

\subsection{Constant coefficients}

    \begin{property}
        A map $\varphi:U\rightarrow \mathbb{C}$ is a complex solution of the homogeneous equation if and only if $\text{Re}(\varphi)$ and $\text{Im}(\varphi)$ are real solutions of the homogeneous equation.
    \end{property}

    \newformula{Characteristic equation}{\index{characteristic!equation}
        When having an ODE of the form\footnote{Any other form of homogeneous second order ODE's with constant coefficients can be rewritten in this form.}
        \begin{gather}
            \label{diffeq:homogeneous_2_ODE_constant_coeff}
            y''(t) + py'(t) + qy(t) = 0
        \end{gather}
        where $p$ and $q$ are constants, we define the characteristic equation as follows:
        \begin{gather}
            \label{diffeq:characteristic_equation}
            \lambda^2 + p\lambda + q = 0.
        \end{gather}
        This polynomial equation generally\footnote{See theorem \ref{linalgebra:fundamental_theorem_of_algebra}\ ("Fundamental theorem of algebra").} has two distinct (complex) roots $\lambda_1$ and $\lambda_2$. From these roots we can derive the solutions of equation \ref{diffeq:homogeneous_2_ODE_constant_coeff} using the following rules ($c_1$ and $c_2$ are constants):
        \begin{itemize}
            \item $\lambda_1 \neq \lambda_2$ with $\lambda_1,\lambda_2\in\mathbb{R}$: $y(t) = c_1\ e^{\lambda_1t} + c_2\ e^{\lambda_2t}$.
            \item $\lambda_1 = \lambda_2$: $y(t) = c_1\ e^{\lambda t} + c_2\ te^{\lambda t}$.
            \item $\lambda_1 = \lambda_2^*$ with $\lambda_1 = a + ib$: $y(t) = c_1\ e^{at}\cos(bt) + c_2\ e^{at}\sin(bt)$.
        \end{itemize}
    }

\subsection{Method of Frobenius}

    \begin{method}[Frobenius]\index{Frobenius!series}
        To find a solution of the homogeneous equation we assume a solution of the form
        \begin{gather}
            \label{diffeq:frobenius_power_series}
            y(x) := \sum_{i=0}^\infty a_i(x-x_0)^{i+k}
        \end{gather}
        where $k$ is a constant.
    \end{method}
    \newdef{Indicial equation}{\index{indicial equation}
        After inserting the solution \ref{diffeq:frobenius_power_series} into the homogeneous equation we obtain, after collecting all terms in $x^i$, an equation of the form $\sum_{i=n}^\infty H_i(k)x^i= 0$ where $n\in\mathbb{R}$ and $H_i(k)$ is a polynomial in $k$. This means that for every $i$ we obtain an equation of the form $H_i(k) = 0$, due to the independence of polynomial terms. The equation for the lowest power will be quadratic in $k$ and it is called the indicial equation.
    }

    \begin{property}
        The indicial equation generally has two roots $k_1, k_2$. We list the different possibilities:
        \begin{itemize}
            \item $k_1 = k_2$: Only one solution will be found with the method of Frobenius (another one can be found as in the second term of equation \ref{diffeq:second_order_general_solution}).
            \item $k_1 - k_2 \in\mathbb{Z}$: A second independent solution might be obtained using this method. If not, then a second solution can be found as mentioned in the previous case.
            \item $k_1 - k_2 \not\in\mathbb{Z}$: Two independent solutions can be found using this method.
        \end{itemize}
    \end{property}

    \begin{theorem}[Fuchs]\index{Fuchs}
        If $a(x)$ and $b(x)$ are analytic at $x=x_0$, then the general solution $y(x)$ can be expressed as a Frobenius series.
    \end{theorem}

\section{Sturm-Liouville theory}\index{Sturm-Liouville theory}

    \newdef{Sturm-Liouville boundary value problem}{
        The following ODE, subject to mixed boundary conditions (given below), is called a Sturm-Liouville boundary value problem:
        \begin{gather}
            \label{ode:Sturm_Liouville}
            \deriv{}{x}\left[p(x)\deriv{y}{x}\right] + \left[g(x) + \lambda r(x)\right]y(x) = 0
        \end{gather}
        where
        \begin{itemize}
            \item $p(x), q(x)$ and $r(x)$ are continuous on $[a,b]$.
            \item $p(x)\in C^1([a,b])$ with $p(x)<0$ or $p(x)>0$ on $[a,b]$.
            \item $r(x)\geq0$ or $r(x)\leq0$ on $[a,b]$.
            \item $r(x)$ is not identically zero on any subinterval.
        \end{itemize}

        The boundary conditions are given by
        \begin{gather}
            \begin{cases}{c}
                \alpha_1y(a) + \beta_1y'(a) &= 0\\
                \alpha_2y(b) + \beta_2y'(b) &= 0.
            \end{cases}
        \end{gather}
        where at least one of the constants $\alpha_1,\alpha_2,\beta_1$ or $\beta_2$ is nonzero.
    }

    \begin{formula}
        The solutions of a Sturm-Liouville problem are of the form
        \begin{gather}
            y(x) = c_1u_1(x;\lambda) + c_2u_2(x;\lambda).
        \end{gather}
        Only for certain values of $\lambda$ will these solutions $(u_1,u_2)$ be non-trivial. The values of $\lambda$ for which the solutions are nontrivial are called \textbf{eigenvalues} and the associated solutions are called \textbf{eigenfunctions}. Substituting this form in the boundary conditions gives the following determinant condition for nontrivial solutions, which is also the defining equation of the eigenvalues $\lambda$:
        \begin{gather}
            \left|
            \begin{array}{cc}
                \alpha_1u_1(a;\lambda) + \beta_1u_1'(a;\lambda)&\alpha_1u_2(a;\lambda) + \beta_1u_2'(a;\lambda)\\
                \alpha_1u_1(b;\lambda) + \beta_1u_1'(b;\lambda)&\alpha_1u_2(b;\lambda) + \beta_1u_2'(b;\lambda)
            \end{array}
            \right|=0.
        \end{gather}
        The independent eigenfunctions can be found by substituting the found eigenvalues in the ODE \ref{ode:Sturm_Liouville}.
    \end{formula}

    \begin{definition}[Self-adjoint form]
        The Sturm-Liouville problem can be rewritten as\footnote{This explains the name ''eigenvalue'' for $\lambda$.} \[\hat{\mathcal{L}}y(x) = \lambda y(x).\] The operator
        \begin{gather}
            \hat{\mathcal{L}} = -\frac{1}{r(x)}\left(\deriv{}{x}\left[p(x)\deriv{}{x}\right] + g(x)\right)
        \end{gather}
        is called the self-adjoint form (since $\hat{\mathcal{L}}$ is a self-adjoint operator). Now, consider the following general linear ODE
        \begin{gather}
            \left[a_2(x)\mderiv{2}{}{x} + a_1(x)\deriv{}{x} + a_0(x)\right]y(x) = 0.
        \end{gather}
        This equation can be rewritten in a self-adjoint form by setting
        \begin{gather*}
            p(x) := e^{\int\frac{a_1}{a_2}dx}\qquad\text{and}\qquad g(x) := \stylefrac{a_0}{a_2}e^{\int\frac{a_1}{a_2}dx}.
        \end{gather*}
    \end{definition}

    \begin{property}
        The eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function $r(x)$. This can be seen as an instance of property \ref{linalgebra:diagonalizable_hermitian}.
    \end{property}

    \begin{theorem}[Oscillation theorem]\index{oscillation theorem}
        The $n^{th}$ eigenfunction of a Sturm-Liouville problem has $n-1$ roots.
    \end{theorem}

\section{Bessel functions}
\subsection{Bessel's differential equation (BDE)}

    A Bessel's differential equation is an ordinary differential equation of the following form:
    \begin{gather}\index{Bessel!differential equation}
        \label{bessel:differential_equation}
        z^2y'' + zy' + (z^2 - n^2)y = 0.
    \end{gather}
    The solutions of this ODE are the Bessel functions of the first and second kind (also called respectively Bessel and Neumann functions):\index{Bessel!function}\index{Neumann!function}
    \begin{align}
        \label{bessel:bessel_function}
        J_n(z) &= \sum_{m = 0}^\infty\frac{(-1)^m}{m!(m + n)!}\left(\frac{z}{2}\right)^{2m + n}\\
        \label{bessel:neumann_function}
        N_n(z) &= \lim_{\nu\rightarrow n}\frac{\cos(\nu \pi)J_n(z) - J_{-n}(z)}{\sin(\nu\pi)}.
    \end{align}
    \sremark{Solution \ref{bessel:bessel_function} can be found using Frobenius' method.}

    \begin{property}
        For $n\not\in\mathbb{N}$ the solutions $J_n(z)$ and $J_{-n}(z)$ are independent.
    \end{property}
    \remark{For $n\not\in\mathbb{N}$ the limiting operation in function \ref{bessel:neumann_function} is not necessary because $\sin(n\pi)$ will never become 0 in this case.}

    \newformula{Generating function}{
        Consider the following function:
        \begin{gather}\index{Bessel!generating function}
            \label{bessel:generating_function}
            g(x, t) := \exp\left[\stylefrac{x}{2}\left(t - \stylefrac{1}{t}\right)\right].
        \end{gather}
        If we expand this function as a Laurent series, we obtain an expression of the form
        \begin{gather}
            \label{bessel:generating_function_expansion}
            g(x, t) = \sum_{n=-\infty}^{+\infty}J_n(x)t^n.
        \end{gather}
        By applying the residue theorem \ref{complexcalculus:residue_theorem} we can express the functions $J_n(x)$ as follows:
        \begin{gather}
            \label{bessel:generating_function_integral}
            J_n(x) = \stylefrac{1}{2\pi i}\oint_C\stylefrac{g(x, t)}{t^{n+1}}dt.
        \end{gather}
        One can show that these functions are exactly the Bessel functions \ref{bessel:bessel_function}. Therefore $g(x, t)$ is called the generating function of the Bessel functions. ?? SHOW THAT THESE ARE REALLY THE BESSEL FUNCTIONS ??
    }

\section{Applications}
\subsection{Laplace equation}\index{Laplace!equation}

    When solving the Laplace equation in cylindrical coordinates we obtain a BDE with integer $n$, which has the cylindrical Bessel functions \ref{bessel:bessel_function} and \ref{bessel:neumann_function} as solutions.

\subsection{Helmholtz equation}

    When solving the Helmholtz equation in spherical coordinates we obtain a variant of the BDE for the radial part:\index{Helmholtz!equation}
    \begin{gather}
        z^2y'' + 2zy' + \left[z^2 - n(n+1)\right]y = 0
    \end{gather}
    where $n$ is an integer. The solutions, called \textbf{spherical Bessel functions}, are related to the cylindrical Bessel functions in the following way (and similarly for the Neumann functions):
    \begin{gather}
        j_n(r) = \sqrt{\stylefrac{\pi}{2x}}J_{n + \frac{1}{2}}(r).
    \end{gather}
\chapter{Partial differential equations}

    For a rigorous treatment of partial differential equations we need the language of distributions. For an introduction we refer to chapter \ref{chapter:distributions}.

\section{General linear equations}

    \newdef{Characteristic curve}{\index{characteristic!curve}
        A curve along which the highest-order partial derivatives are not uniquely defined.
    }

\section{First order PDE}

    \newformula{First order quasilinear PDE}{
        \begin{gather}
            \label{pde:first_order_pde}
            P(x, y, z)\pderiv{z}{x} + Q(x, y, z)\pderiv{z}{y} = R(x, y, z)
        \end{gather}
    }

    \newformula{Characteristic curve}{
        The above PDE will have no unique solution if
        \begin{gather}
            \left|
            \begin{array}{cc}
                P&Q\\
                dx&dy
            \end{array}
            \right|=0
        \end{gather}
        and will have a non-unique solution if
        \begin{gather}
            \left|
            \begin{array}{cc}
                P&R\\
                dx&dz
            \end{array}
            \right|=0.
        \end{gather}
        The characteristic curves are thus defined by $\frac{dx}{P} = \frac{dy}{Q}$ and along these curves the condition $\frac{dx}{P} = \frac{dz}{R}$ should hold to ensure a solution.
    }

    \begin{formula}[Lagrange-Charpit equations]\index{Lagrange-Charpit equations}
        The general solution of \ref{pde:first_order_pde} is implicitly given by $F(\xi,\eta) = 0$ with $F(\xi, \eta)$ an arbitrary differentiable function where $\xi(x, y, z) = c_1$ and $\eta(x, y, z) = c_2$ are solutions of the Lagrange-Charpit equations:
        \begin{gather}
            \stylefrac{dx}{P} = \stylefrac{dy}{Q} = \stylefrac{dz}{R}
        \end{gather}
        where $c_1, c_2$ are constants which are fixed by the boundary conditions.
    \end{formula}
    \remark{Looking at the defining equations of the characteristic curve, it is clear that these fix the general solution of the PDE.}

\section{Characteristics}

    \newformula{Second order quasilinear PDE}{
        Consider the following pseudolinear differential equation for the function $u(x, y)$:
        \begin{gather}
            \label{pde:general_2order_pde}
            R(x,y)u_{xx} + S(x,y)u_{xy} + T(x, y)u_{yy} = W(x, y, u, p, q)
        \end{gather}
        where $p = u_x$ and $q = u_y$.
    }

    \newformula{Equation of characteristics}{\index{characteristic}
        Characteristic curves were defined as those curves $\psi(x, y)=\text{const}$ along which the highest-order terms in a PDE are not uniquely defined. For quasilinear second-order PDEs we can find an algebraic characterization as follows. Let us first consider the following system:
        \begin{gather}
            \begin{cases}
                u_{xx}dx + u_{xy}dy = dp&\\
                u_{xy}dx + u_{yy}dy = dq.&
            \end{cases}
        \end{gather}
        According to Cramer's rule \ref{linalgebra:cramers_rule} these equations, together with the PDE \ref{pde:general_2order_pde}, give the following condition for the characteristic curves:
        \begin{gather}
            \left|
            \begin{array}{ccc}
                R(x, y)&S(x, y)&T(x, y)\\
                dx&dy&0\\
                0&dx&dy\\
            \end{array}
            \right| = 0.
        \end{gather}
        This is equivalent to the following equation:
        \begin{gather}
            \label{pde:defining_equation_characteristic_curves}
            R\left(\deriv{y}{x}\right)^2 - S\left(\deriv{y}{x}\right) + T = 0.
        \end{gather}
        Accordingly this equation is often called the \textbf{characteristic equation} (of the PDE \ref{pde:general_2order_pde}).
    }

    \newdef{Types of characteristics}{\index{elliptic!PDE}\index{hyperbolic!PDE}
        Equation \ref{pde:defining_equation_characteristic_curves} is quadratic in $\deriv{y}{x}$. If this equation has two distinct real roots then the PDE is said to be \textbf{hyperbolic}. If the equation has only one root, the PDE is said to be \textbf{parabolic}. In the remaining case, where the equation has two distinct complex roots, the PDE is said to be \textbf{elliptic}.
    }

    \newformula{Canonical form}{\index{characteristic}
        Consider the general change of variables $\xi = \xi(x, y)$, $\eta = \eta(x, y)$ and $\zeta\equiv u$. With this change, the PDE \ref{pde:general_2order_pde} becomes
        \begin{gather}
            A(\xi_x,\xi_y)\mpderiv{2}{\zeta}{\xi} + B(\xi_x,\xi_y,\eta_x,\eta_y)\stylefrac{\partial^2\zeta}{\partial\xi\partial\eta} + A(\eta_x,\eta_y)\mpderiv{2}{\zeta}{\eta} = F(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)
        \end{gather}
        where
        \begin{itemize}
            \item $A(a,b) = Ra^2 + Sab + Tb^2$
            \item $B(a,b,c,d) = 2Rac + S(bc+ad) + 2Tbd$.
        \end{itemize}

        The discriminant $\Delta$ of the quadratic equation \ref{pde:defining_equation_characteristic_curves} lets us rephrase the classification of characteristics in terms of canonical forms. The fact that this classficiation is well-defined follows from the result that the discriminant of equation \ref{pde:defining_equation_characteristic_curves} is, up to the square of the Jacobian of $(x, y)\rightarrow(\xi, \eta)$, equal to the discriminant $B(\xi_x,\xi_y,\eta_x,\eta_y)^2-4A(\xi_x,\xi_y)A(\eta_x,\eta_y)$.

        To bring the differential equation to a simpler form we want the coefficients $A(a,b)$ to vanish. Curves $\xi(x, y)=\text{const}$ along which the coefficient $A(\xi_x, \xi_y)$ vanishes are called \textbf{characteristic (curves)}. Along such curves we have \[\deriv{\xi}{x} = \xi_x + \xi_i\deriv{y}{x} = 0\] and hence we can relate the slope $\deriv{y}{x}$ to the ratio $-\frac{\xi_x}{\xi_y}$. Solving $A(a,b)=0$ is then equivalent to solving equation \ref{pde:defining_equation_characteristic_curves} (and we see that both notions of characteristic curves coincide).

        \begin{itemize}
            \item \textbf{hyperbolic PDE} ($\Delta>0$): The sign of the discriminant implies that the quadratic equation $A=0$ has two real solutions $f_1(x, y)$ and $f_2(x, y)$. By choosing the transformation $\xi=f_1(x, y)$ and $\eta=f_2(x, y)$ we make the coefficients $A(a,b)$ vanish and hence we obtain the canonical hyperbolic form
                \begin{gather}
                    \label{pde:hyperbolic_canonical_form}
                    \stylefrac{\partial^2\zeta}{\partial\xi\partial\eta} = H(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)
                \end{gather}
                where $H = \stylefrac{F}{2B}$.
            \item \textbf{parabolic PDE} ($\Delta=0$): As in the hyperbolic case we perform the change of variables $\xi = f(x, y)$, however there is only one root of the defining equation, so the second variable can be chosen at will under the constraint that it should be independent of $f_1(x, y)$. From the condition $\Delta=0$ it is also possible to derive the condition that $B(\xi_x,\xi_y\eta_x\eta_y) = 0$ and $A(\eta_x,\eta_y)\neq0$. This gives the parabolic canonical form
                \begin{gather}
                    \label{pde:parabolic_canonical_form}
                    \mpderiv{2}{\zeta}{\eta} = G(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)
                \end{gather}
                where $G = \stylefrac{F}{A(\eta_x,\eta_y)}$.
            \item \textbf{elliptic PDE} ($\Delta<0$): In this case there are two complex roots. Writing $\xi = \alpha + i\beta$ and $\eta = \alpha - i\beta$ gives the following (real) equation \[\stylefrac{\partial^2\zeta}{\partial\xi\partial\eta} = \frac{1}{4}\left(\mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta}\right).\] Substituting this in the PDE (together with $A(a,b)=0$) results in the following elliptic canonical form:
                \begin{gather}
                    \label{pde:elliptic_canonical_form}
                    \mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta} = K(\alpha,\beta,\zeta,\zeta_\alpha,\zeta_\beta).
                \end{gather}
        \end{itemize}
    }

    \begin{theorem}[Maximum principle]\index{maximum!principle}\label{pde:theorem:maximum_principle}
        Consider a differential equation of the parabolic or elliptic type. The maximum of the solution on a domain is to be found on the boundary of that domain.
    \end{theorem}

\subsection{D'Alemberts method}

    Consider the \textbf{wave equation}\index{wave!equation}
    \begin{gather}
        \label{pde:wave_equation}
        \mpderiv{2}{u}{x}(x,t) = \stylefrac{1}{c^2}\mpderiv{2}{u}{t}(x,t).
    \end{gather}
    By applying the method from the previous subsection it is clear that the characteristics are given by
    \begin{gather}
        \xi = x + ct\qquad\text{and}\qquad \eta = x - ct.
    \end{gather}
    Furthermore, it follows that the wave equation is a hyperbolic equation that can be rewritten in the canonical form
    \begin{gather}
        \label{pde:canonical_wave_equation}
        \stylefrac{\partial^2u}{\partial\xi\partial\eta}(\xi,\eta) = 0
    \end{gather}
    Integration with respect to $\xi$ and $\eta$ and rewriting the solution in terms of $x$ and $t$ gives
    \begin{gather}
        \label{pde:wave_solution}
        u(x,t) = f(x+ct) + g(x-ct)
    \end{gather}
    where $f,g$ are arbitrary functions. This solution represents a superposition of a left-moving wave and a right-moving wave.

    Now, consider the wave equation subject to the general boundary conditions
    \begin{gather}
        u(x,0) = v(x)\qquad\text{and}\qquad \pderiv{u}{t}(x,0) = q(x).
    \end{gather}
    By inserting these conditions in the solution \ref{pde:wave_solution} it can be shown that the general solution subject to the given boundary conditions is given by
    \begin{gather}
        \label{pde:dalembert_solution}
        u(x,t) := \frac{1}{2}\left[v(x+ct) + v(x-ct)\right] + \frac{1}{2c}\int_{x-ct}^{x+ct}q(z)dz.
    \end{gather}
    \remark{Because $x$ is not bounded this solution is only valid for infinite strings.}

\section{Separation of variables}\label{section:separation_of_variables}

    \begin{remark*}
        We begin this section by remarking that solutions obtained by separation of variables are generalized Fourier series, which tend to converge rather slowly. For numerical purposes, other techniques are recommended. However, the series solutions often give a good insight in the properties of the obtained solutions.
    \end{remark*}

\subsection{Cartesian coordinates}

    \begin{method}[Separation of variables]
        Let $\hat{\mathcal{L}}$ be the operator associated with a partial differential equation such that $\hat{\mathcal{L}}u(x)=0$ where $x := (x_1,\ldots,x_n)$ is the set of variables. A useful method to find solutions is to assume a solution of the form
        \begin{gather}
            u(x) = \prod_{i=1}^nu_i(x_i).
        \end{gather}
        By substituting this form in the PDE and using some (basic) algebra it is sometimes (!!) possible to reduce the partial differential equation to a system of $n$ ordinary differential equations.
    \end{method}

    \begin{example}
        Consider the following PDE:
        \begin{gather}
            \pderiv{u}{t} - a\mpderiv{2}{u}{x} = 0.
        \end{gather}
        Substituting a solution of the form $u(x,t) = X(x)T(t)$ gives \[X(x)\deriv{T(t)}{t} - aT(t)\mderiv{2}{X(x)}{x} = 0\] which can be rewritten as (the arguments are dropped for convenience) \[\stylefrac{1}{aT}\deriv{T}{t} = \stylefrac{1}{X}\mderiv{2}{X}{x}.\] Because both sides are independent, they must be equal to a constant $\lambda$. This results in the following system of ordinary differential equations:
        \begin{gather}
            \begin{cases}
                X''(x) = \lambda X(x)&\\
                T'(t) = a\lambda T(t).&
            \end{cases}
        \end{gather}
    \end{example}

\subsection{Dirichlet problem}\index{Dirichlet!problem}

    The (interior) Dirichlet problem\footnote{Think of the Dirichlet boundary condition \ref{diffeq:conditions:dirichlet}.} is the problem of finding a solution to a PDE in a finite region, given the value of the function on the boundary of the region. The uniqueness of this solution can be proven with the maximum principle \ref{pde:theorem:maximum_principle} if the PDE is of the elliptic kind (!!) such as the Laplace equation\footnote{The Dirichlet boundary problem originated with the Laplace equation.}.

    \begin{proof}
        Let $\phi,\psi$ be two solutions of the interior Dirichlet problem. Due to the linearity both $\psi-\phi$ and $\phi-\psi$ are solutions too (without applying the boundary conditions). According to the maximum principle, these solutions achieve their maximum on the boundary of the domain. Furthermore, due to the Dirichlet boundary conditions, $\phi(x)=\psi(x)$ for all $x\in\partial\Omega$. Combining these two facts gives $\max(\psi-\phi) = \max(\phi-\psi) = 0$ or alternatively $\psi\leq\phi$ and $\phi\leq\psi$ in the complete domain. Which means that $\phi=\psi$ in the complete domain.\qed
    \end{proof}

    \sremark{There is also an exterior Dirichlet problem, where we have to find the solution of the PDE, given the boundary conditions, outside of the boundary.}

\section{Boundary conditions}

    \newformula{Nonhomogeneous boundary condition}{\label{pde:non_homogeneous_boundary_condition}
        \begin{gather}
            \alpha u(a,t) + \beta\pderiv{u}{x}(a,t) = h(t)
        \end{gather}
        When $h(t)$ is identically zero, the boundary condition becomes homogeneous.
    }

    \newmethod{Steady-state solution}{\index{steady-state!method for boundary conditions}
        Assume that the function $h(t)$ is constant. In this case it is useful to rewrite the solution as \[u(x,t) = v(x) + w(x,t).\] The ''time-independent'' function is called the steady-state solution and the function $w(x,t)$  represents the deviation of this steady-state scenario.

        As the PDE is linear, we require the partial solutions $v(x)$ and $w(x,t)$ to individually satisfy the equation. Furthermore we require the function $v(x)$ to also satisfy the given nonhomogeneous boundary conditions. This results in $w(x, t)$ being the solution of a homogeneous PDE with homogeneous boundary conditions. This can be seen in the following proof:
    }
    \begin{proof}
        Assume a boundary condition of the form $\alpha u(a,t) + \beta\pderiv{u}{x}(a,t) = u_0$. Due to the requirements of a steady-state solution we also have $\alpha v(a) + \beta\pderiv{v}{x}(a) = u_0$. Combining these two conditions gives \[\alpha\left[v(a) + w(a,t)\right] + \beta\left[\pderiv{v}{x}(a) + \pderiv{w}{x}(a,t)\right] = \alpha v(a) + \beta\pderiv{v}{x}(a).\] Using the conditions this can be rewritten as \[\alpha w(a,t) + \beta\pderiv{w}{x}(a,t) = 0.\] The steady-state deviation $w(x,t)$ thus satisfies homogeneous boundary conditions.\qed
    \end{proof}

    \begin{method}
        If the function $h(t)$ is not a constant, we use a different method. Rewrite the solution as $u(x,t) = v(x,t) + w(x,t)$ where we only require $v(x,t)$ to be some function that satisfies the boundary conditions (and not the PDE)\footnote{As there are infinitely many possible functions that satisfy the boundary conditions, the best choice for $v(x,t)$ is the one that makes the equation for $w(x,t)$ as simple as possible.}. This will lead to $w(x,t)$ satisfying the homogeneous boundary conditions as in the previous method. After substituting the function $v(x,t)$ in the PDE, we obtain a differential equation for $w(x,t)$ but it can be nonhomogeneous.
    \end{method}

    \begin{method}
        A third, sometimes useful, method is the following. If the problem consists of 3 homogeneous and 1 nonhomogeneous boundary condition then the problem can be solved by first applying the homogeneous conditions to restrict the values of the separation constant and obtain a series expansion. Afterwards the obtained series can be fitted to the nonhomogeneous condition to obtain the final remaining coefficients.

        If there is more than 1 nonhomogeneous boundary condition, the method can be extended. Let there be $j$ boundary conditions. Rewrite the general solution as $u(x,t) = \sum_{i=1}^jv_j(x,t)$ where $v_j(x,t)$ satisfies the $j^{th}$ nonhomogeneous condition and the homogeneous versions of the other conditions. This way the general solution still satisfies all conditions and the first part of the method can be applied to all functions $v_j(x,t)$ to obtain a series expansion.
    \end{method}

    \begin{method}[Nonhomogeneous PDE]\index{PDE!nonhomogeneous}
        A possible way to solve nonhomogeneous second order partial differential equations of the form \[\hat{\mathcal{L}}u(x,t) = f(x,t)\] given a set of homogeneous boundary conditions and initial value conditions $w(x,0) = \psi(x)$, is the following method (where we assume all involved functions to admit a generalized Fourier expansion):
        \begin{enumerate}
            \item Solve the homogeneous version of the PDE, which will result in a series expansion $\sum_nw_n(t)e_n(x)$, where $e_n(x)$ are a complete set of eigenfunctions in the variable $x$. This solution should satisfy the (homogeneous\footnote{Nonhomogeneous boundary conditions can be turned into homogeneous ones by the previous two methods.}) boundary conditions.
            \item Expand the function $f(x,t)$ in the same way as $u(x,t)$. The coefficients $f_n$ can be found by using the orthogonality relations of the functions $e_n(x)$.
            \item Inserting these expansions in the original PDE and rewriting the equation will lead to a summation of the form:
            \[\sum_n\left[\left(Dw_n(t)\right)e_n(x)\right] = 0\] where $D$ is a linear first-order differential operator. As all terms are independent, this gives $n$ first order ODEs to obtain the functions $w_n(t)$. These can be generally solved by using formula \ref{diffeq:first_order_general_solution}.
            \item Initial value conditions for the functions $w_n(t)$ are applied by setting $t=0$ in the series expansion of $u(x,t)$ and equating it with the series expansion of $\psi(x)$. This results in $w_n(0) =: \Psi_n$.
            \item The obtained ODEs together with the found boundary conditions $w_n(0) = \Psi_n$ will give the general solutions of $w_n(t)$.
            \item Entering these solutions in the series expansion of $u(x,t)$ will give the general solution of the nonhomogeneous PDE.
        \end{enumerate}
    \end{method}
    \remark{It is clear that the requirement that all involved functions admit a generalized Fourier expansion is restricting. Not all nonhomogeneous PDEs are solvable with this method.}

\section{Higher dimensions}\label{pde:section:higher_dimensions}
\subsection{Symbols}

    \newdef{Symbol}{\index{symbol}
        Consider a general $k^{th}$-order differential operator (we use multi-indices $\alpha$)
        \begin{gather}
            \hat{P} = \sum_{|\alpha|\leq k}c_\alpha(x)D^\alpha.
        \end{gather}
        The symbol of this operator is defined by replacing the partial derivatives by indeterminates $\xi^i$:
        \begin{gather}
            p(\hat{P}, \xi) := \sum_{|\alpha|\leq k}c_\alpha(x)\xi^\alpha.
        \end{gather}
    }
    \newdef{Principal symbol}{\index{principal!symbol}
        The principal symbol of a $k^{th}$-order differential operator $\hat{P}$ is defined as the highest degree component of $p(\hat{P}, \xi)$:
        \begin{gather}
            \sigma_{\hat{P}}(\xi) := \sum_{|\alpha|=k}c_\alpha(x)\xi^\alpha.
        \end{gather}
        If we have a system of partial differential equations we replace the functions $c_\alpha$ by matrix-valued functions $\big(c_i^j\big)_\alpha$.
    }

    \begin{property}
        The principal symbol of a differential operator transforms as a tensor.
    \end{property}
    \begin{definition}[Ellipticity]\index{elliptic!operator}
        A system of PDEs \[\hat{P}f(x) = 0\] is elliptic if and only if $\sigma_P$ is invertible. Notice that this is only possible if the number of variables is smaller than the number of equations, hence if the system is at most determined.
    \end{definition}

\section{Sobolev spaces}

    Using the theory of $L^p$-spaces (as defined in chapter \ref{chapter:lebesgue}) and distributions we can define an interesting collection of function spaces which are ubiquitous in the field of numerical analysis (and beyond).

    \newdef{Sobolev space}{\index{Sobolev!space}
        \nomenclature[S_Sob]{$W^{m,p}(U)$}{The Sobolov space in $L^p$ of order $m$.}
        For all nonnegative integers $m, p\in\mathbb{N}$, with $p\geq1$, we define the Sobolev space $W^{m,p}(U)$ as the space of functions in $L^p(U)$ for which their (weak\footnote{See definition \ref{distributions:weak_derivative}.}) derivatives up to order $m$ are also in $L^p(U)$. When $p=2$, i.e. when we restrict to square-integrable functions, we also use the notation $H^m(U)$.

        This space can be turned into a normed space by equipping it with the following norm:
        \begin{gather}
            ||f|| := \left(\sum_{|\alpha|\leq m}||f^{(\alpha)}||_{L^p}\right)^{1/p}.
        \end{gather}
    }

    The Sobolev spaces inherit the following property from the $L^p$-spaces:
    \begin{property}
        Every Sobolev space is a Banach space. Moreover, we can show that the spaces $H^m$ are Hilbert spaces.
    \end{property}
    The Sobolev spaces also satisfy the following density theorem:
    \begin{property}
        For all $m$ we can prove that $\mathcal{D}(\mathbb{R}^n)$ is dense in $W^{m,p}(\mathbb{R}^n)$. However, only for $m=0$ can we prove this for open subsets of $\mathbb{R}^n$. The closure of $\mathcal{D}(U)$ in $W^{m,p}(U)$ is denoted by $W_0^{m,p}(U)$.
    \end{property}

    The Sobolev norm is not always easy to work with, especially in practical applications. Luckily there exists a lemma showing that we can equivalently restrict to partial derivatives of order $m$:
    \begin{theorem}[Friedrich]\index{Friedrich}
        For all bounded $U$ we can introduce an equivalent norm on $H^m_0(U)$ as follows:
        \begin{gather}
            \langle f|g \rangle := \sum_{|\alpha|=m}\int_U f^{(\alpha)}\overline{g^{(\alpha)}}dx.
        \end{gather}
    \end{theorem}

    Property \ref{lebesgue:Lp_duals} allows us to define the dual Sobolev spaces:
    \begin{definition}
        The space $W^{-m, p}(U)\subset\mathcal{D}'(U)$ is defined as the dual of $W^{m, p}_0(U)$. It can be shown that all elements in $W^{-m, p}(U)$ can be written as follows:
        \begin{gather}
            T = \sum_{|\alpha|\leq m}f^{(\alpha)}_\alpha
        \end{gather}
        where $f_\alpha\in L^{p'}(U),\forall\alpha$ with $p'$ the H\"older conjugate of $p$.
    \end{definition}

    ?? COMPLETE (continue in AMP1) ??
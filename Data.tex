\chapter{Data Analysis}

\section{Regression}

	\begin{method}[Normal equation\footnotemark]\index{normal!equation}
		\footnotetext{The name stems from the fact that the equation $A^TAx = A^Tb$ implies that the residual is orthogonal (normal) to the range of $A$.}
		Given a linear problem \[Ax=b\] one can try to solve this problem by minimizing the $\ell^2$-norm $||Ax-b||^2$:
		\begin{gather}
			\min_x(Ax-b)^T(Ax-b)\longrightarrow\hat{x}.
		\end{gather}
		This can be formally be solved by $x=(A^TA)^{-1}A^Tb$ where $(A^TA)^{-1}A^T$ is the pseudoinverse of $A$.
	\end{method}

\subsection{Collinearity}

	\newdef{Multicollinearity}{\index{collinearity}
		Consider a finite set of random variables $\{X_i\}_{1\leq i\leq n}$. These random variables are set to be perfectly (multi)collinear if there exists an affine relation between them, i.e. there exists variables $\{\lambda_i\}_{0\leq i\leq n}$ such that
		\begin{gather}
			\lambda_0 + \lambda_1X_1 + \cdots + \lambda_nX_n = 0.
		\end{gather}
		The same concept can be applied to data samples by requiring this equation to hold for all entries of the data set. However in this case one also defines "near multicollinearity" if the variables $X_i$ are related as above up to some error term $\varepsilon$. If the variance of $\varepsilon$ is small then the matrix $X^TX$ might have an ill-conditioned inverse which might render the algorithms unstable.
	}
	
	\newdef{Variance inflation factor}{\index{VIF}
		The VIF is an estimate for how much the variance of a coefficient is inflated by the multicollinearity in the model. The VIF of a coefficient $\beta_i$ is defined as follows:
		\begin{gather}
			\text{VIF}_i = \frac{1}{1-R_i^2}
		\end{gather}
		where $R_i^2$ is the $R^2$-value obtained after regressing the predictor $X_i$ on all other predictors. The rule of thumb is that $\text{VIF}\geq10$ implies that a significant amount of multicollinearity is present in the model.
	}
	
	\begin{method}[Tikhonov regularization]{\index{Tikhonov!regularization}
		Consider a linear (regression) problem \[Ax = b\] where we try to find $x$ in terms of $A, b$. The most straightforward way to solve for $x$ is the least squares method introduced above. Formally a solution is given by the normal equation: $x=(A^TA)^{-1}A^Tb$. However sometimes it might happen that $A$ is nearly singular so that we may wish for $x$ to have a specific property.
		
		In this case we can add a regularization term to the minimization problem:
		\begin{gather}
			||Ax-b||^2+||\Gamma x||^2
		\end{gather}
		where $\Gamma$ is called the \textbf{Tikhonov matrix}. In the case that $\Gamma=\lambda\mathbbm{1}$ we speak of \textbf{$\ell^2$-regularization}. This regularization technique will benefit solutions with smaller norms.
	}
	\end{method}
	\begin{remark}\index{lasso}\index{ridge}
		One can generalize the $\ell^2$-regulatization as defined above. By replacing the 2-norm $||\cdot||$ by any $p$-norm $||\cdot||_p$ one obtains an $\ell^p$-regularization. In this cases of $p=1$ and $p=2$ one often speaks of \textbf{lasso} and \textbf{ridge} regression. For general $p\geq0$ one also speaks of \textbf{bridge} regression.
		
		The minimization procedures for $p\leq1$ have the interesting feature that they not only shrink the coefficients but that they even perform feature selection, i.e. some coefficients become identically zero. However it can be shown that the optimization problem for $p<1$ is nonconvex and hence is harder to solve. In general it is found that lasso regression gives the best results.
	\end{remark}

\section{Trees and forests}

\section{Support-vector machines}

\section{Time series analysis}\index{time series}

	\newdef{Autocorrelation function}{\index{autocorrelation}\index{correlation|seealso{autocorrelation}}
		Consider a time series $\{X_t\}_{t\in\mathbb{N}}$. The autocovariance function of this time series is defined as the covariance function of the random variables $\{X_t\}_{t\in\mathbb{N}}$. Similarly, the autocorrelation function of the time series is defined as the correlation function of its associated collection of random variables.
	}
	
	\newdef{Spectral density}{\index{spectral!density}\index{memory}
		Consider a time series $\{X_t\}_{t\in\mathbb{N}}$. If we require the associated autocovariance time series to be in $\ell^1$, i.e. require it to be absolutely summable, then we can define the spectral density as the discrete Fourier transform of the autocovariance function:
		\begin{gather}
			f(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^{+\infty}\gamma(k)e^{i\omega k}
		\end{gather}
		where $\gamma(k)$ is the autocovariance function at lag $k$.
		
		Under the assumption that the spectral density exists, we say that a time series has \textbf{short memory} if $f(0)$ is finite. Otherwise the series is said to have \textbf{long memory}.
	}
	
	\newdef{Lag operator\footnotemark}{\index{lag}\index{backshift|see{lag}}
		\footnotetext{Also called the \textbf{backshift operator}.}
		The lag operator sends an entry of a time series to the preceding value:
		\begin{gather}
			BX_t = X_{t-1}.
		\end{gather}
		An important concept (especially in the context of autoregressive models) is that of a \textbf{lag polynomial}\footnote{The notation for these is not completely fixed in the literature. The $\theta$-notation is however a frequent choice.}:
		\begin{align}
			\theta(B) &= 1 + \sum_{i=1}^k \theta_iB^i\\
			\varphi(B) &= 1 - \sum_{i=1}^k \varphi_iB^i
		\end{align}
	}
	\newnot{Difference operator}{\index{difference}
		In terms of the lag operator $B$ one defines the difference operator $\Delta$ as follows:
		\begin{gather}
			\Delta = 1 - B.
		\end{gather}
		In the same way one can define the \textbf{seasonal} difference operator:
		\begin{gather}
			\Delta_s = 1 - B^s.
		\end{gather}
	}
	
	\begin{method}[Ljung-Box test]\index{Ljung-Box}
		The Ljung-Box test checks if a given set of autocorrelations of a time series is different from zero. Consider a data sample of $n$ elements and let $\{\rho_i\}_{1\leq i\leq k}$ be the first $k$ lagged autocorrelation functions. The test statistic is defined as
		\begin{gather}
			Q = n(n+2)\sum_{i=1}^k\frac{\rho_k}{n-k}.
		\end{gather}
		If the null hypothesis ''there is no correlation'' is true then the test statistic will asymptotically follow a $\chi^2$-distribution with $k$ degrees of freedom.
	\end{method}
	
	\begin{method}[Augmented Dickey-Fuller test]\index{Dickey-Fuller}
		Consider a time series $\{X_t\}_{t\leq T}$. The (augmented) Dickey-Fuller test checks if the time series is (trend) stationary. For this test we consider the following regression model (similar to the \textit{ARIMA-models} discussed in the next section):
		\begin{gather}
			\Delta X_t = \alpha + \beta t + \gamma X_{t-1} + \sum_{i=1}^{p-1} \theta_i\Delta X_{t-i} + \varepsilon_t.
		\end{gather}
		The test statistic for this test is
		\begin{gather}
			DF = \frac{\gamma}{SE(\gamma)}
		\end{gather}
		where $SE$ denotes the standard error. The null hypothesis states that $\gamma=0$, i.e. there is a \textit{unit root} $(1-B)$ present in the model. Comparing the test statistic to tabulated critical values will give an indication wheter to reject the hypothesis or not (the more negative the statistic the more significant the result).
	\end{method}

\subsection{Autoregressive models}

	\newdef{AR$(p)$-model}{\index{autoregressive model}
		Consider a time series $\{X_t\}_{t\leq T}$. The autogressive model of order $p$ is defined as the multiple linear regression model of $X_t$ with respect to the first $p$ lagged values $X_{t-1}, ..., X_{t-p}$ of the time series:
		\begin{gather}
			X_t = \beta_0 + \beta_1X_{t-1} + \cdots + \beta_pX_{t-p} + \varepsilon_t.
		\end{gather}
	}
	
	\newdef{Partial autocorrelation function}{
		The $p^{th}$ autocorrelation function is defined as the $p^{th}$ coefficient in the AR$(p)$-model.
	}
	\remark{The optimal order $p$ of an autoregressive model is the one for which all higher partial autocorrelation functions (almost) vanish.}
	
	\newdef{MA$(p)$-model}{\index{moving average}
		Consider a time series $\{X_t\}_{t\leq T}$ where every $X_t$ contains a white noise contribution $\varepsilon_t\sim\mathcal{N}(0, \sigma^2)$. The moving average model of order $p$ is defined as the multiple linear regression model of $X_t$ with respect to the first $p$ lagged values $\varepsilon_{t-1}, ..., \varepsilon_{t-p}$ of the error term:
		\begin{gather}
			X_t = \beta_0 + \beta_1\varepsilon_{t-1} + \cdots + \beta_p\varepsilon_{t-p} + \varepsilon_t.
		\end{gather}
		Since the error terms are assumed to have mean zero we see that the intercept term $\beta_0$ gives the mean of the time series.
	}
	\remark{The optimal order $p$ of an autoregressive model is the one for which all higher autocorrelation functions (almost) vanish.}
	
	\newdef{Invertibility}{\index{invertibility}\index{stationarity}
		An MA$(q)$-model is said to be invertible if all roots of its associated lag polynomial $\theta(B)$ lie outside the unit circle. This condition implies that the polynomial is invertible, i.e. $1/\theta(B)$ can be written as a convergent series in the operator $B$. This then implies\footnote{Sometimes this is used as a definition of invertibility.} that one can write the MA$(q)$-model as an AR$(p)$-model where possibly $p=\infty$. The analogous property for AR$(p)$-models leads to a definition of \textit{stationarity}.
	}
	
	In practice it is not always possible to describe a data set using either an autoregressive or a moving average model. However, nothing stops us from combining these two types of models:
	\newdef{ARMA$(p, q)$-model}{
		\nomenclature[A_ARMA]{ARMA}{Autoregressive moving-average model}
		\begin{gather}
			X_t = \alpha_0 + \sum_{i=1}^p\alpha_iX_{t-i} + \sum_{j=1}^q\beta_j\varepsilon_{t-j} + \varepsilon_t
		\end{gather}
		As above one can find the optimal values for $p$ and $q$ by checking the autocorrelation and partial autocorrelation functions.
	}
	
	Using the lag polynomials one can rewrite the ARMA$(p, q)$-model as follows:
	\begin{gather}
		\varphi(B)X_t = \alpha_0 + \theta(B)\varepsilon_t.
	\end{gather}
	By considering the special case where the polynomial $\mathcal{B}^-_\alpha$ has a unit root $1-B$ with multiplicity $d$ we can obtain a generalization of our model:
	\begin{gather}
		\varphi(B)(1-B)^dX_t = \alpha_0 + \theta(B)\varepsilon_t.
	\end{gather}
	The interpretation of this additional factor $(1-B)^d$ is related to the stationarity of the time series. The operator $1-B$ is a ''differencing operator'':
	\begin{align*}
		(1-B)\phantom{^2}X_t &= X_t - X_{t-1}\\
		(1-B)^2X_t &= (X_t-X_{t-1}) - (X_{t-1}-X_{t-2})\\
		&\cdots
	\end{align*}
	By successively applying it one can obtain a stationary time series from a nonstationary time series. This combination of differencing, autoregression and moving averages is called the \textbf{ARIMA}-model\footnote{The 'I' stands for ''integrated''.}.
	
	\remark{Including so-called \textit{exogenous} variables, i.e. external predictors, leads to an \textbf{ARIMA\underline{X}}-model.}
	
	\begin{remark}[Fitting AR- and MA-models]
		As is clear from the definition of an AR$(p)$-model the parameters $\theta_i$ can easily be found using standard techniques for multivariate linear regression such as ordinary least squares. However in contrast to AR-models where the predictors are known, the estimation of coefficients in MA-models is harder since the error terms $\varepsilon_t$ are by definition unknown.
	\end{remark}
	To estimate the coefficients in a MA-model researchers have introduced multiple techniques (see for example \cite{MA_fit}). One of the most famous ones is Durbin's method:
	\begin{method}[Durbin]\index{Durbin}
		By restricting to invertible MA$(q)$-models (or by approximating a noninvertible model by an invertible one) we can first fit an AR$(p)$-model with $p>q$ to obtain estimates for the errors $\varepsilon_t$. Then in a second step one can again use a least squares-method to solve for the coefficients in the MA-model.
	\end{method}
	
	As a last modification we can introduce seasonal components. Simple trends such as a linear growth are easily removed from the time series by detrending or differencing. However, a periodic pattern is harder to remove and in general ARIMA-models are not made to accompany these features. Luckily one can easily modify the ARIMA-model to incorporate seasonal variations.
	
	The multiplicative SARIMA-model is obtained by inserting operators similar to the ones of the ordinary ARIMA-model but where we replace the lag operator $B$ by a seasonal lag operator $B^s$ (where $s$ is the period of the seasonal variation):
	\newdef{ARIMA$(p,q,d)(P,Q,D)_s$-model}{
		\begin{gather}
			\Phi(B^s)\varphi(B)\Delta_s^D\Delta^dX_t = \theta(B)\Theta(B^s)\varepsilon_t
		\end{gather}
	}
	
\subsection{Causality}

	\newdef{Granger causality}{\index{causality!Granger}
		Consider two time series $\{X_t\}_{t\in\mathbb{N}}$ and $\{Y_t\}_{t\in\mathbb{N}}$. The time series $X_t$ is said to Granger-cause $Y_t$ if past values of $X_t$ help to predict future values of $Y_t$. More formally this can be stated as follows:
		\begin{gather}
			P[Y_{t+k}\in A|\Omega(t)]\neq P[Y_{t+k}\in A|\Omega\backslash X(t)]
		\end{gather}
		for some $k$ where $\Omega(t)$ and $\Omega\backslash X(t)$ denote the available information at time $t$ with and without removing the variable $X$ from the universe.
		
		This formulation of causality was introduced by Granger under the following two assumptions:
		\begin{itemize}
			\item The cause always happens prior to the effect.
			\item The cause has unique information about the effect.
		\end{itemize}
	}
	\remark{A slightly different but for computational purposes often more useful\footnote{In fact this was the original definition by Granger.} notion of Granger-causality is as follows. A time series $X_t$ is said to Granger-cause a time series $Y_t$ if the variance of predictions of $Y_t$ becomes smaller when we take the information contained in $X_t$ into account.}
	
	
	\remark{Assume that we are given two uncorrelated models giving predictions of a time series $X_t$. One way to check if they have the same accuracy is the \textit{Diebold-Mariano test}. However, when testing for Granger-causality one should pay attention. This test is not valid for nested models and hence is not applicable to two models that only differ by a set of extra predictors (in this case an external time series).}

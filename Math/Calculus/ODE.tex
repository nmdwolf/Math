\chapter{Ordinary differential equations}

\section{Boundary conditions}\index{boundary!condition}

    Unique solutions of a differential equation are obtained by supplying additional conditions. These are called \textbf{boundary conditions}.

    \newdef{Periodic boundary condition}{\label{ode:periodic_conditions}
        A boundary condition of the following form:
        \begin{gather}
            y(x) = y(x + \varphi).
        \end{gather}
    }

    \newdef{Dirichlet boundary condition}{\label{ode:conditions:dirichlet}
        A boundary condition of the form
        \begin{gather}
            y(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$, where $\Omega$ is the domain on which the problem is defined.
    }

    \newdef{Neumann boundary condition}{\label{ode:neumann_conditions}
        A boundary condition of the form
        \begin{gather}
            \pderiv{y}{\hat{n}}(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$, where $\Omega$ is the domain on which the problem is defined.
    }

    ?? CHECK (aren't some of these meant for PDEs) ??

\section{Existence and uniqueness}

    \begin{theorem}[Picard-Lindel\"of]\index{Picard-Lindel\"of}\label{ode:picard_lindelof}
        Consider an ordinary differential equation of the form
        \begin{gather}
            \dot{x}(t) = f(t,x(t)),
        \end{gather}
        where $f$ is defined on a subset $I\times U\subset\mathbb{R}\times\mathbb{R}^n$.\footnote{Generalizations to arbitrary Banach spaces exist, see e.g. \cite{AMP1}.} If $f$ is continuous on $I$ and locally Lipschitz on $U$, then for every point $(t_0,x_0)\in I\times U$ there exists a maximal interval $J\supseteq I$ and a unique solution $x:J\rightarrow\mathbb{R}^n$ of the differential equation with initial condition $(t_0,x_0)$.
    \end{theorem}

\section{First-order ODEs}

    \newdef{First-order ODE}{\label{ode:first_order_ODE}
        \begin{gather}
            y'(t) + a(t)y(t) = R(t)
        \end{gather}
        If the function $R$ is identically zero, the ODE is said to be \textbf{homogenous}.
    }
    \begin{formula}\label{ode:first_order_general_solution}
        Let $U\subseteq\mathbb{R}$ be an open set and let the functions $a, R:U\rightarrow\mathbb{R}$ be continuous. The solutions $\varphi:U\rightarrow\mathbb{R}$ of \ref{ode:first_order_ODE} are given by:
        \begin{gather}
            \varphi(t) = e^{-\int a(t)dt}\left(c + \int R(t)e^{\int a(t)dt}dt\right),
        \end{gather}
        where $c$ is a constant (in general determined by some kind of boundary condition).
    \end{formula}

\section{Second-order ODEs}

    \newdef{Second-order ODE}{\label{ode:second_order_ODE}
        \begin{gather}
            y''(t) + a(t)y'(t) + b(t)y(t) = R(t)
        \end{gather}
        If the function $R$ is identically zero, the ODE is said to be \textbf{homogenous}.
    }

\subsection{General solution}

    \begin{formula}\label{ode:second_order_general_solution}
        Let $\varphi:U\rightarrow\mathbb{R}$ be a nowhere zero solution of the homogeneous equation. The general solution of \ref{ode:second_order_ODE} is given by
        \begin{gather}
            y(t) = c_1\varphi +  c_2\varphi\int\frac{e^{-\int a}}{\varphi^2} + \psi_0,
        \end{gather}
        where $\psi_0$ is a particular solution of \ref{ode:second_order_ODE}.
    \end{formula}

    \begin{property}
        Let $\psi_0$ be a solution of \ref{ode:second_order_ODE}. The set of all solutions is given by the affine space
        \begin{gather}
            \big\{\psi_0 + \chi\mid\chi\text{ is a solution of the homogeneous equation}\big\}.
        \end{gather}
    \end{property}
    \begin{property}\index{Wronskian}
        Two solutions of the homogeneous equation are independent if the \textbf{Wronskian} is nonzero:
        \begin{gather}
            \label{ode:wronskian}
            W\left(\varphi_1(x),\varphi_2(x)\right) := \det
            \begin{pmatrix}
                \varphi_1(x)&\varphi_2(x)\\
                \varphi_1'(x)&\varphi_2'(x)
            \end{pmatrix}
            \neq 0.
        \end{gather}
    \end{property}

    \newformula{Abel's identity}{\index{Abel}
        An explicit formula for the Wronskian is given by
        \begin{gather}
            \label{ode:abels_identity}
            W(x) = W(x_0)\exp\left(-\int^x_{x_0}a(x')dx'\right).
        \end{gather}
    }

\subsection{Constant coefficients}

    \begin{property}
        A function $\varphi:U\rightarrow\mathbb{C}$ is a complex solution of the homogeneous equation if and only if $\mathrm{Re}(\varphi)$ and $\mathrm{Im}(\varphi)$ are real solutions of the homogeneous equation.
    \end{property}

    \newformula{Characteristic equation}{\index{characteristic!equation}
        When studying an ODE of the form\footnote{Any other form of homogeneous, second-order ODEs with constant coefficients can be rewritten in this form.}
        \begin{gather}
            \label{ode:homogeneous_2_ODE_constant_coeff}
            y''(t) + py'(t) + qy(t) = 0,
        \end{gather}
        where $p$ and $q$ are constants, the characteristic equation is defined as follows:
        \begin{gather}
            \label{ode:characteristic_equation}
            \lambda^2 + p\lambda + q = 0.
        \end{gather}
        By teh fundamental theorem of algebra \ref{alggeom:fundamental_theorem_of_algebra}, this polynomial equation has two (complex) roots $\lambda_1$ and $\lambda_2$. From these roots one can derive the solutions of Equation \eqref{ode:homogeneous_2_ODE_constant_coeff} using the following rules ($c_1$ and $c_2$ are constants):
        \begin{itemize}
            \item $\lambda_1\neq\lambda_2$ with $\lambda_1,\lambda_2\in\mathbb{R}$: $y(t) = c_1e^{\lambda_1t} + c_2e^{\lambda_2t}$,
            \item $\lambda_1=\lambda_2$: $y(t) = c_1e^{\lambda t} + c_2te^{\lambda t}$, and
            \item $\lambda_1=\lambda_2^*$ with $\lambda_1 = a + ib$: $y(t) = c_1e^{at}\cos(bt) + c_2e^{at}\sin(bt)$.
        \end{itemize}
    }

\subsection{Method of Frobenius}

    \begin{method}[Frobenius]\index{Frobenius!series}
        To find a solution of the homogeneous equation one can assume a solution of the form
        \begin{gather}
            \label{ode:frobenius_power_series}
            y(x) = \sum_{i=0}^\infty a_i(x-x_0)^{i+k},
        \end{gather}
        where $k\in\mathbb{Z}$ is a constant.
    \end{method}
    \newdef{Indicial equation}{\index{indicial equation}
        After inserting the ansatz \eqref{ode:frobenius_power_series} into the homogeneous equation and collecting all terms in $x^i$, an equation of the form $\sum_{i=n}^\infty H_i(k)x^i=0$ is obtained, where $n\in\mathbb{N}$ and $H_i(k)$ is a polynomial in $k$. This means that for every $i\in\mathbb{N}$, one obtains an equation of the form $H_i(k) = 0$, due to the independence of polynomial terms. The equation for the lowest-degree term will be quadratic in $k$ and it is called the indicial equation.
    }

    \begin{property}
        The indicial equation generally has two roots $k_1,k_2$.
        \begin{itemize}
            \item $k_1=k_2$: Only one solution will be found with the method of Frobenius (another one can be found as in the second term of Formula \ref{ode:second_order_general_solution}.
            \item $k_1-k_2 \in\mathbb{Z}$: A second independent solution might be obtained using this method. If not, a second solution can be found as mentioned in the previous case.
            \item $k_1-k_2\not\in\mathbb{Z}$: Two independent solutions can be found using this method.
        \end{itemize}
    \end{property}

    \begin{theorem}[Fuchs]\index{Fuchs}
        If $a(x)$ and $b(x)$ are analytic at $x=x_0$, the general solution $y(x)$ can be expressed as a Frobenius series.
    \end{theorem}

\section{Sturm-Liouville theory}\index{Sturm-Liouville theory}

    \newdef{Sturm-Liouville problem}{
        An ODE of the following form, subject to mixed boundary conditions:
        \begin{gather}
            \label{ode:sturm_liouville}
            \deriv{}{x}\left(p(x)\deriv{y}{x}\right) + \Big(g(x) + \lambda r(x)\Big)y(x) = 0,
        \end{gather}
        where
        \begin{itemize}
            \item $p(x),q(x)$ and $r(x)$ are continuous on $[a,b]$,
            \item $p(x)\in C^1([a,b])$ with $p(x)<0$ or $p(x)>0$ on $[a,b]$,
            \item $r(x)\geq0$ or $r(x)\leq0$ on $[a,b]$, and
            \item $r(x)$ is not identically zero on any subinterval.
        \end{itemize}
        The boundary conditions are given by
        \begin{align*}
            \alpha_1y(a) + \beta_1y'(a) &= 0\\
            \alpha_2y(b) + \beta_2y'(b) &= 0,
        \end{align*}
        where at least one of the constants $\alpha_1,\alpha_2,\beta_1$ or $\beta_2$ is nonzero.
    }

    \begin{formula}
        The solutions of a Sturm-Liouville problem are of the form
        \begin{gather}
            y(x) = c_1u_1(x;\lambda) + c_2u_2(x;\lambda).
        \end{gather}
        Only for certain values of $\lambda$ will these solutions $(u_1,u_2)$ be nontrivial. The values of $\lambda$ for which the solutions are nontrivial are called \textbf{eigenvalues} and the associated solutions are called \textbf{eigenfunctions}. Substituting this form in the boundary conditions gives the following determinant condition for nontrivial solutions:
        \begin{gather}
            \det
            \begin{pmatrix}
                \alpha_1u_1(a;\lambda) + \beta_1u_1'(a;\lambda)&\alpha_1u_2(a;\lambda) + \beta_1u_2'(a;\lambda)\\
                \alpha_1u_1(b;\lambda) + \beta_1u_1'(b;\lambda)&\alpha_1u_2(b;\lambda) + \beta_1u_2'(b;\lambda)
            \end{pmatrix}
            =0.
        \end{gather}
        The independent eigenfunctions can be found by substituting the solutions $\lambda$ of this condition in the ODE \eqref{ode:sturm_liouville}.
    \end{formula}

    \begin{definition}[Self-adjoint form]
        A Sturm-Liouville problem can be rewritten as follows:\footnote{This formulation explains the name ``eigenvalue'' for the quantity $\lambda$.} \[\hat{\mathcal{L}}y(x) = \lambda y(x).\] The operator
        \begin{gather}
            \hat{\mathcal{L}} = -\frac{1}{r(x)}\left[\deriv{}{x}\left(p(x)\deriv{}{x}\right) + g(x)\right]
        \end{gather}
        is called the self-adjoint form (because $\hat{\mathcal{L}}$ is a self-adjoint operator). Now, consider the following general linear ODE
        \begin{gather}
            \left(a_2(x)\mderiv{2}{}{x} + a_1(x)\deriv{}{x} + a_0(x)\right)y(x) = 0.
        \end{gather}
        This equation can be rewritten in a self-adjoint form by setting
        \begin{gather*}
            p(x) := \exp\left(\int\frac{a_1(x)}{a_2(x)}dx\right) \qquad\text{and}\qquad g(x) := \frac{a_0(x)}{a_2(x)}\exp\left(\int\frac{a_1(x)}{a_2(x)}dx\right).
        \end{gather*}
    \end{definition}

    \begin{property}
        The eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function $r(x)$. This can be seen as an instance of Property \ref{linalgebra:diagonalizable_hermitian}.
    \end{property}

    \begin{theorem}[Oscillation theorem]\index{oscillation theorem}
        The $n^{th}$ eigenfunction of a Sturm-Liouville problem has $n-1$ roots.
    \end{theorem}

\section{Bessel functions}\index{Bessel!function}

    A Bessel's differential equation is an ordinary differential equation of the following form:
    \begin{gather}
        \label{ode:differential_equation}
        x^2y''(x) + xy'(x) + (x^2 - n^2)y(x) = 0.
    \end{gather}
    The solutions of this ODE are the Bessel functions of the first and second kind (also called respectively \textbf{Bessel} and \textbf{Neumann functions}):\index{Neumann!function}
    \begin{align}
        \label{ode:bessel_function}
        J_n(x) &= \sum_{m=0}^\infty\frac{(-1)^m}{m!(m+n)!}\left(\frac{x}{2}\right)^{2m+n},\\\nonumber\\
        \label{ode:neumann_function}
        N_n(x) &= \lim_{\nu\rightarrow n}\frac{\cos(\nu\pi)J_n(x) - J_{-n}(x)}{\sin(\nu\pi)}.
    \end{align}

    \sremark{Solution \eqref{ode:bessel_function} can be found using the Frobenius method.}

    \begin{property}
        For $n\not\in\mathbb{N}$ the solutions $J_n$ and $J_{-n}$ are independent.
    \end{property}
    \remark{For $n\not\in\mathbb{N}$ the limiting operation in \eqref{ode:neumann_function} is not necessary because $\sin(n\pi)$ will never become 0 in this case.}

    \newformula{Generating function}{
        Consider the following function:
        \begin{gather}
            \label{ode:generating_function}
            g(x,t) := \exp\left[\frac{x}{2}\left(t - \frac{1}{t}\right)\right].
        \end{gather}
        If this function is expanded as a Laurent series, an expression of the form
        \begin{gather}
            \label{ode:generating_function_expansion}
            g(x,t) = \sum_{n=-\infty}^\infty J_n(x)t^n
        \end{gather}
        is obtained. By applying the residue theorem \ref{complex:residue_theorem}, one can express the functions $J_n$ as follows:
        \begin{gather}
            \label{ode:generating_function_integral}
            J_n(x) = \frac{1}{2\pi i}\oint_C\frac{g(x,t)}{t^{n+1}}dt.
        \end{gather}
        One can show that these functions are exactly the Bessel functions \eqref{ode:bessel_function}. Therefore, $g(x,t)$ is called the generating function of the Bessel functions.

        ?? SHOW THAT THESE ARE REALLY THE BESSEL FUNCTIONS ??
    }

\section{Applications}
\subsection{Laplace equation}\index{Laplace!equation}

    When solving the Laplace equation in cylindrical coordinates, one obtains Bessel's ODE with integer $n$, which has the cylindrical Bessel functions \eqref{ode:bessel_function} and \eqref{ode:neumann_function} as solutions.

\subsection{Helmholtz equation}\index{Helmholtz!equation}

    When solving the Helmholtz equation in spherical coordinates, one obtains a variant of Bessel's ODE for the radial part:
    \begin{gather}
        x^2y''(x) + 2xy'(x) + \big(x^2 - n(n+1)\big)y(x) = 0,
    \end{gather}
    where $n$ is an integer. The solutions, called the \textbf{spherical Bessel functions}, are related to the cylindrical Bessel functions in the following way (similarly for the Neumann functions):
    \begin{gather}
        j_n(r) = \sqrt{\frac{\pi}{2x}}J_{n + \frac{1}{2}}(r).
    \end{gather}

    ?? COMPLETE ??
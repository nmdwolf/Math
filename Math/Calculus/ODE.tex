\chapter{Ordinary differential equations}

    This chapter is concerned with the study of equations involving univariate functions $f:\mathbb{R}\rightarrow\mathbb{R}$ and their derivatives.

    \minitoc

\section{Boundary conditions}\index{boundary!condition}

    Unique solutions of a differential equation are obtained by supplying additional conditions. These are called \textbf{boundary conditions}.

    \newdef{Periodic boundary condition}{\label{ode:periodic_conditions}
        A condition of the following form:
        \begin{gather}
            f(x) = f(x+a)\,.
        \end{gather}
    }

    \newdef{Dirichlet boundary condition}{\label{ode:conditions:dirichlet}
        A condition of the form
        \begin{gather}
            f(x) = g(x)
        \end{gather}
        for all $x\in\partial\Omega$, where $\Omega$ is the domain on which the problem is defined.
    }

    \newdef{Neumann boundary condition}{\label{ode:neumann_conditions}
        A condition of the form
        \begin{gather}
            \pderiv{f}{\hat{n}}(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$, where $\Omega$ is the domain on which the problem is defined.
    }

    \todo{CHECK (these make more sense for PDEs)}

\section{Existence and uniqueness}

    \begin{theorem}[Picard--Lindel\"of]\index{Picard--Lindel\"of}\label{ode:picard_lindelof}
        Consider an ordinary differential equation of the form
        \begin{gather}
            \dot{f}(x) = g\bigl(x,f(x)\bigr)\,,
        \end{gather}
        where $g$ is defined on a subset $I\times U\subset\mathbb{R}\times\mathbb{R}^n$.\footnote{Generalizations to arbitrary \textit{Banach spaces} exist (to be introduced in \cref{section:banach}), see e.g.~\citet{choquet-bruhat_analysis_1991}.} If $g$ is continuous on $I$ and locally Lipschitz on $U$, then, for every point $(x_0,y_0)\in I\times U$, there exists a maximal interval $J\supseteq I$ and a unique solution $f:J\rightarrow\mathbb{R}^n$ of the differential equation with initial condition $(x_0,y_0)$.
    \end{theorem}

\section{First-order ODEs}

    \newdef{First-order ODE}{\index{homogeneous!equation}
        \begin{gather}
            \label{ode:first_order_ODE}
            f'(x) + a(x)f(x) = R(x)
        \end{gather}
        If the function $R$ is identically zero, the ODE is said to be \textbf{homogeneous}.
    }
    \begin{formula}\label{ode:first_order_general_solution}
        Let $U\subseteq\mathbb{R}$ be an open set and let the functions $a, R:U\rightarrow\mathbb{R}$ be continuous. The solutions $f:U\rightarrow\mathbb{R}$ of \cref{ode:first_order_ODE} are given by\footnote{The integrals represent indefinite integrals or primitives.}
        \begin{gather}
            f(x) = e^{-\int\!a(x)\,dx}\left(c + \Int\!R(x)e^{\int\!a(s)\,ds}\,dx\right)\,,
        \end{gather}
        where $c\in\mathbb{R}$ is a constant (which is, in general, determined by a choice of boundary condition).
    \end{formula}

\section{Second-order ODEs}

    \newdef{Second-order ODE}{
        \begin{gather}
            \label{ode:second_order_ODE}
            f''(x) + a(x)f'(x) + b(x)f(x) = R(x)
        \end{gather}
        If the function $R$ is identically zero, the ODE is again said to be \textbf{homogeneous}.
    }

\subsection{General solution}

    \begin{formula}\label{ode:second_order_general_solution}
        Let $\varphi:U\rightarrow\mathbb{R}$ be a nowhere zero solution of the homogeneous equation. The general solution of \cref{ode:second_order_ODE} is given by
        \begin{gather}
            f(x) = c_1\varphi(x) + c_2\varphi(x)\Int\frac{e^{-\int\!a(s)\,ds}}{\varphi^2(x)}\,dx + \psi_0(x)\,,
        \end{gather}
        where $\psi_0$ is a particular solution of \cref{ode:second_order_ODE}.
    \end{formula}

    \begin{property}
        Let $\psi_0$ be a particular solution of \cref{ode:second_order_ODE}. The set of all solutions is given by the affine space
        \begin{gather}
            \bigl\{\psi_0 + \chi\bigm\vert\chi\text{ is a solution of the homogeneous equation}\bigr\}\,.
        \end{gather}

        \todo{CHECK HOW THIS MATCHES WITH THE PREVIOUS PROPERTY}
    \end{property}

    \begin{property}[Wronskian]\index{Wronskian}\label{ode:wronskian}
        Two solutions of the homogeneous equation are independent if the \textbf{Wronskian}
        \begin{gather}
            W\left(\varphi_1(x),\varphi_2(x)\right) := \det
            \begin{pmatrix}
                \varphi_1(x)&\varphi_2(x)\\
                \varphi_1'(x)&\varphi_2'(x)
            \end{pmatrix}
        \end{gather}
        is nonzero.
    \end{property}

    \newformula{Abel's identity}{\index{Abel!identity}\label{ode:abels_identity}
        An explicit formula for the Wronskian is given by
        \begin{gather}
            W(x) = W(x_0)\exp\left(-\Int_{x_0}^xa(x')\,dx'\right)\,.
        \end{gather}
    }

\subsection{Constant coefficients}

    \begin{property}
        A function $\varphi:U\rightarrow\mathbb{C}$ is a complex solution of the homogeneous equation if and only if $\Re(\varphi)$ and $\Im(\varphi)$ are real solutions of the homogeneous equation.
    \end{property}

    \newformula{Characteristic equation}{\index{characteristic!equation}
        When studying an ODE of the form\footnote{Any other form of homogeneous, second-order ODEs with constant coefficients can be rewritten like this.}
        \begin{gather}
            \label{ode:homogeneous_2_ODE_constant_coeff}
            f''(x) + pf'(x) + qf(x) = 0\,,
        \end{gather}
        where $p,q\in\mathbb{R}$ are constants, the characteristic equation is defined as follows:
        \begin{gather}
            \label{ode:characteristic_equation}
            \lambda^2 + p\lambda + q = 0\,.
        \end{gather}
        By the Fundamental Theorem of Algebra~\ref{alggeom:fundamental_theorem_of_algebra}, this polynomial equation has two (complex) roots $\lambda_1$ and $\lambda_2$. From these roots, one can derive the solutions of \cref{ode:homogeneous_2_ODE_constant_coeff} using the following rules ($c_1,c_2\in\mathbb{R}$ are constants):
        \begin{itemize}
            \item $\lambda_1\neq\lambda_2$ with $\lambda_1,\lambda_2\in\mathbb{R}$: $f(x) = c_1e^{\lambda_1x} + c_2e^{\lambda_2x}$,
            \item $\lambda_1=\lambda_2$: $f(x) = c_1e^{\lambda x} + c_2xe^{\lambda x}$, and
            \item $\lambda_1=\lambda_2^*$ with $\lambda_1 = a + ib$: $f(x) = c_1e^{ax}\cos(bx) + c_2e^{ax}\sin(bx)$.
        \end{itemize}
    }

\subsection{Frobenius' method}\index{Frobenius!series}

    To find a solution of the homogeneous equation, one can assume a solution of the form
    \begin{gather}
        \label{ode:frobenius_power_series}
        y(x) = \sum_{i=0}^{+\infty}a_i(x-x_0)^{i+k}\,,
    \end{gather}
    where $k\in\mathbb{R}$ is a constant.

    \newdef{Indicial equation}{\index{indicial equation}
        After inserting the ansatz~\eqref{ode:frobenius_power_series} into the homogeneous equation and collecting all terms in $x^i$, an equation of the form
        \begin{gather}
            \sum_{i=n}^{+\infty}H_i(k)x^i=0
        \end{gather}
        is obtained, where $n\in\mathbb{N}$ and the $H_i(k)$ are polynomials in $k$. This means that for every $i\in\mathbb{N}$, one obtains an equation of the form $H_i(k) = 0$, due to the independence of polynomial terms. The equation for the lowest-degree term will be quadratic in $k$ and is called the indicial equation.
    }

    \begin{property}
        The indicial equation generally has two roots $k_1,k_2$.
        \begin{itemize}
            \item $k_1=k_2$: Only one solution will be found with Frobenius' method (another one can be found as in the second term of \cref{ode:second_order_general_solution}).
            \item $k_1-k_2 \in\mathbb{Z}$: A second independent solution might be obtained using this method. If not, a second solution can be found as mentioned in the previous case.
            \item $k_1-k_2\not\in\mathbb{Z}$: Two independent solutions can be found using this method.
        \end{itemize}
    \end{property}

    \begin{theorem}[Fuchs]\index{Fuchs}
        If the functions $a,b:U\rightarrow\mathbb{R}$ in \cref{ode:second_order_ODE} are analytic at $x=x_0$, the general solution can be expressed as a Frobenius series.
    \end{theorem}

\section{Fourier transform}

    Recall \cref{section:fourier_transform}. The Fourier transform gives well-defined maps on $L^1, L^2,\mathcal{S}$ and $\mathcal{S}^*$.
    
    Consider an ODE of the form
    \begin{gather}
        \sum_{l=0}^na_lf^{(l)}(x) = g(x)\,.
    \end{gather}
    Note that the coefficients of the differential operator are assumed to be constants. Expanding both $f$ and $g$ into Fourier components, gives an algebraic equation:
    \begin{gather}
        \sum_{l=0}^na_l(ik)^l\widetilde{f}(k) = \widetilde{g}(k)\,.
    \end{gather}
    Solving this equation, together with any provided initial value conditions (possibly also Fourier transformed) will give conditions on $k$ and $\widetilde{f}(k)$.

    \begin{example}[Sines and cosines]
        Consider the ODE
        \begin{gather*}
            f''(x) = -f(x)\,.
        \end{gather*}
        It is a standard result that the solution space to this equation is spanned by $\sin(x)$ and $\cos(x)$ or, equivalenty, by $\exp(ix)$ and $\exp(-ix)$.

        Now, taking the Fourier transform, one obtains:
        \begin{gather*}
            \Int_{-\infty}^{+\infty}(1-k^2)\widetilde{f}(k)e^{ikx}\,dk=0\,.
        \end{gather*}
        This implies that
        \begin{gather*}
            (1-k^2)\widetilde{f}(k)=0\qquad(\text{in }\mathcal{S}^*)\,.
        \end{gather*}
        This implies that $\widetilde{f}(k)=\lambda\delta(k-1)+\nu\delta(k+1)$ for some $\lambda,\nu\in\mathbb{C}$. Inserting this into the Fourier transform gives:
        \begin{gather}
            f(x) = \lambda\exp(ix)+\nu\exp(-ix)
        \end{gather}
        as expected.
    \end{example}

    \begin{example}[Affine functions]
        Now, consider the even simpler equation
        \begin{gather*}
            f''(x) = 0\,.
        \end{gather*}
        Taking the Fourier transform gives:
        \begin{gather}
            k^2\widetilde{f}(k)=0\,.
        \end{gather}
        For ordinary functions, this would mean either $\widetilde{f}(k)$ identically zero or $k=0$. However, passing to distributions (and \cref{distribution:delta_equation}), one obtains
        \begin{gather*}
            \widetilde{f}(k) = \lambda\delta'(k) + \kappa\delta(k)
        \end{gather*}
        or, after transforming back to the $x$-domain:
        \begin{gather*}
            f(x) = \lambda x+\kappa\,.
        \end{gather*}
    \end{example}
    In general, combining the two examples above, starting from a homogeneous ODE (with constant coefficients), the resulting Fourier components will be delta distributions (and their derivatives). Transforming back to the $x$-domain, this will lead to a combination of polynomials (for $k=0$) and oscillatory components (for $k\neq0$). If the ODE is not homogeneous, \cref{distribution:delta_equation} is not applicable and a more general (distributional) equation has to be solved.

\section{Sturm--Liouville theory}\index{Sturm--Liouville theory}

    \newdef{Sturm--Liouville problem}{
        An ODE of the form
        \begin{gather}
            \label{ode:sturm_liouville}
            \deriv{}{x}\left(p(x)\deriv{y}{x}\right) + \bigl(g(x) + \lambda r(x)\bigr)y(x) = 0\,,
        \end{gather}
        subject to mixed boundary conditions, where:
        \begin{itemize}
            \item $p,q,r:[a,b]\rightarrow\mathbb{R}$ are continuous,
            \item $p\in C^1([a,b])$ with either $p<0$ or $p>0$,
            \item either $r\geq0$ or $r\leq0$, and
            \item $r$ is not identically zero on any subinterval.
        \end{itemize}
        The boundary conditions are given by:
        \begin{align*}
            \alpha_1y(a) + \beta_1y'(a) &= 0\,,\\
            \alpha_2y(b) + \beta_2y'(b) &= 0\,,
        \end{align*}
        where at least one of the constants $\alpha_1,\alpha_2,\beta_1$ or $\beta_2$ is nonzero.
    }

    \begin{formula}
        The solutions of a Sturm--Liouville problem are of the form
        \begin{gather}
            y(x) = c_1u_1(x;\lambda) + c_2u_2(x;\lambda)\,.
        \end{gather}
        Only for certain values of $\lambda$ will these solutions be nontrivial. The values of $\lambda$ for which the solutions are nontrivial are called \textbf{eigenvalues} and the associated solutions are called \textbf{eigenfunctions}. Substituting this form in the boundary conditions gives the following determinant condition for nontrivial solutions:
        \begin{gather}
            \det
            \begin{pmatrix}
                \alpha_1u_1(a;\lambda) + \beta_1u_1'(a;\lambda)&\alpha_1u_2(a;\lambda) + \beta_1u_2'(a;\lambda)\\
                \alpha_1u_1(b;\lambda) + \beta_1u_1'(b;\lambda)&\alpha_1u_2(b;\lambda) + \beta_1u_2'(b;\lambda)
            \end{pmatrix}
            =0\,.
        \end{gather}
        The independent eigenfunctions can be found by substituting the solutions $\lambda$ of this condition in the ODE~\eqref{ode:sturm_liouville}.
    \end{formula}

    \newprop{Self-adjoint form}{
        A Sturm--Liouville problem can be rewritten as follows:\footnote{This formulation explains the name `eigenvalue' for the quantity $\lambda$.}
        \begin{gather}
            \hat{\mathcal{L}}y(x) = \lambda y(x)\,.
        \end{gather}
        The operator
        \begin{gather}
            \hat{\mathcal{L}} = -\frac{1}{r(x)}\left[\deriv{}{x}\left(p(x)\deriv{}{x}\right) + g(x)\right]
        \end{gather}
        is called the \textbf{self-adjoint form}, since $\hat{\mathcal{L}}$ is a self-adjoint operator. Now, consider the following general linear ODE
        \begin{gather}
            \left(a_2(x)\mderiv{2}{}{x} + a_1(x)\deriv{}{x} + a_0(x)\right)y(x) = 0\,.
        \end{gather}
        This equation can be rewritten in a self-adjoint form by setting
        \begin{gather*}
            p(x) := \exp\left(\Int\frac{a_1(x)}{a_2(x)}\,dx\right) \qquad\text{and}\qquad g(x) := \frac{a_0(x)}{a_2(x)}\exp\left(\Int\frac{a_1(x)}{a_2(x)}\,dx\right)\,.
        \end{gather*}
    }

    \begin{property}
        The eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function $r:[a,b]\rightarrow\mathbb{R}$. This can be seen as an instance of \cref{linalgebra:diagonalizable_hermitian}.
    \end{property}

    \begin{theorem}[Oscillation theorem]\index{oscillation theorem}
        The $n^{\text{th}}$ eigenfunction of a Sturm--Liouville problem has $n-1$ roots.
    \end{theorem}

\section{Applications}
\subsection{Bessel functions}\index{Bessel!function}\index{Neumann!function}

    A Bessel's differential equation is an ordinary differential equation of the following form:
    \begin{gather}
        \label{ode:bessel_ode}
        x^2f''(x) + xf'(x) + (x^2 - n^2)f(x) = 0\,,
    \end{gather}
    where $n$ can actually be any complex number. The solutions of this ODE are the Bessel functions of the first and second kind (also called \textbf{Bessel} and \textbf{Neumann functions}, respectively):
    \begin{align}
        \label{ode:bessel_function}
        J_n(x) &= \sum_{m=0}^{+\infty}\frac{(-1)^m}{m!(m+n)!}\left(\frac{x}{2}\right)^{2m+n}\,,\\\nonumber\\
        \label{ode:neumann_function}
        N_n(x) &= \lim_{\nu\rightarrow n}\frac{\cos(\nu\pi)J_n(x) - J_{-n}(x)}{\sin(\nu\pi)}\,.
    \end{align}
    \sremark{Solution~\eqref{ode:bessel_function} can be found using the Frobenius method.}

    \begin{property}
        For $n\not\in\mathbb{N}$, the solutions $J_n$ and $J_{-n}$ are independent.
    \end{property}
    \remark{For $n\not\in\mathbb{N}$, the limit in \cref{ode:neumann_function} is not necessary since $\sin(n\pi)$ will never become 0 in this case.}

    \newformula{Generating function}{
        Consider the following function:
        \begin{gather}
            \label{ode:generating_function}
            g(x,t) := \exp\left[\frac{x}{2}\left(t - \frac{1}{t}\right)\right]\,.
        \end{gather}
        If this function is expanded as a Laurent series, an expression of the form
        \begin{gather}
            \label{ode:generating_function_expansion}
            g(x,t) = \sum_{n=-\infty}^{+\infty}J_n(x)t^n
        \end{gather}
        is obtained. By applying the Residue Theorem~\ref{complex:residue_theorem}, one can express the functions $J_n$ as follows:
        \begin{gather}
            \label{ode:generating_function_integral}
            J_n(x) = \frac{1}{2\pi i}\bigointsss_C\frac{g(x,t)}{t^{n+1}}\,dt\,.
        \end{gather}
        One can show that these functions are exactly the Bessel functions~\eqref{ode:bessel_function}. For this reason, $g(x,t)$ is called the generating function of the Bessel functions.

        \todo{SHOW THAT THESE ARE REALLY THE BESSEL FUNCTIONS}
    }
\chapter{Partial Differential Equations}\label{chapter:pde}

    For a rigorous treatment of partial differential equations, the language of distributions is required. For an introduction, see \labelref{chapter:distributions}.

    \minitoc

\section{Important examples}

    \begin{example}[Laplace equation]\index{Laplace!equation}\index{harmonic!function}\index{harmonic!cylindrical}\label{pde:laplace_equation}
        \begin{gather}
            \Delta f = \sum_{i=1}^d\mpderiv{2}{f}{x_i}=0
        \end{gather}
        Functions satisfying this equation are said to be \textbf{harmonic}.
    \end{example}

    When solving the Laplace equation in cylindrical coordinates, one obtains Bessel's ODE~\eqref{ode:bessel_ode} with integer $n$. The solutions are the Bessel functions~\eqref{ode:bessel_function} and~\eqref{ode:neumann_function}. For this reason, Bessel functions for integer $n$ are also called \textbf{cylinder functions} or \textbf{cylindrical harmonics}.

    \begin{example}[Poisson equation]\index{Poisson!equation}
        This is the nonhomogeneous version of the Laplace equation:
        \begin{gather}
            \Delta f = \varphi\,,
        \end{gather}
        for some $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$.
    \end{example}

    In the case where $\varphi$ is proportional to $f$, the Poisson equation receives yet another name.
    \begin{example}[Helmholtz equation]\index{Helmholtz!equation}\index{Bessel!function}\label{pde:helmholtz_equation}
        \begin{gather}
            \Delta f = -k^2f
        \end{gather}
    \end{example}

    When solving the Helmholtz equation in spherical coordinates, one obtains a variant of the Bessel ODE~\eqref{ode:bessel_ode} for the radial part:
    \begin{gather}
        r^2y''(r) + 2ry'(r) + \bigl(r^2 - n(n+1)\bigr)y(r) = 0\,,
    \end{gather}
    where $n$ is an integer. The solutions, called the \textbf{spherical Bessel functions}, are related to the cylindrical Bessel functions in the following way (similarly for the Neumann functions):
    \begin{gather}
        j_n(r) = \sqrt{\frac{\pi}{2x}}J_{n + \frac{1}{2}}(r)\,.
    \end{gather}

    \begin{example}[Wave equation]\index{wave!equation}\index{d'Alembert!operator}\label{optics:wave_equation}
        In one spatial dimension, the wave equation reads as follows:
        \begin{gather}
            \mpderiv{2}{f}{x} = \frac{1}{v^2}\mpderiv{2}{f}{t}\,.
        \end{gather}
        In higher dimensions, this can be rewritten using the Laplacian (\cref{vector:laplacian}) as
        \begin{gather}
            \Delta f = \frac{1}{v^2}\mpderiv{2}{f}{t}\,.
        \end{gather}
        or, using the \textbf{d'Alembertian}
        \begin{gather}
            \Box_v := \frac{1}{v^2}\mpderiv{2}{}{t}-\Delta\,,
        \end{gather}
        as
        \begin{gather}
            \Box_v f = 0\,.
        \end{gather}
    \end{example}

    The heat equation is very similar to the wave equation and gives yet another example of the general Poisson equation. However, it is first order in the time derivative.
    \begin{example}[Heat equation]\index{heat equation}\label{pde:heat_equation}
        \begin{gather}
            \pderiv{f}{t} = \Delta f\,.
        \end{gather}
    \end{example}

\section{First-order PDE}

    \newformula{First-order quasilinear PDE}{\label{pde:first_order_pde}
        \begin{gather}
            P(x,y,f)\pderiv{f}{x} + Q(x,y,f)\pderiv{f}{y} = R(x,y,f)
        \end{gather}
        More generally, a PDE is said to be quasilinear if is linear in the highest-order derivatives.
    }

    \newdef{Characteristic curve}{\index{characteristic!curve}
        In the general theory, this is a curve along which the highest-order partial derivatives are discontinuous. Equivalently, it is a curve along which the partial differential equation reduces to an ordinary differential equation
    }

    \newformula{Characteristic curve}{\index{Pfaffian!system}\index{integral!curve}\index{Monge!axis}
        Consider a first-order quasilinear PDE. The idea of the \textbf{method of the characteristics} is to obtain a new coordinate system $(s,t)$ such that the PDE becomes an ODE in the coordinate $s$ and, accordingly, such that the solution has an arbitrary dependence on $t$. The curves of constant $t$ are called the characteristic curves (or \textbf{characteristics}) of the PDE.

        To this end, the coefficients of the PDE are to be interpreted as the coefficients of a vector field $X\equiv(P,Q,R):\mathbb{R}^3\rightarrow\mathbb{R}^3$. This way, the PDE can be rewritten as
        \begin{gather}
            X\cdot\nabla f = 0\,.
        \end{gather}
        Now, by \cref{vector:tangent_gradient}, the gradient vector field is normal to $\mathrm{graph}(f)$ and, consequently, $X$ is tangent to $\mathrm{graph}(f)$. The characteristic curves correspond to the \textit{integral curves} (see \cref{bundle:integral_curve}) of $X$, which foliate $\mathrm{graph}(f)$. The direction determined by $X$ is called the \textbf{Monge axis}.p
        
        Along a general curve, one obtains two linear equations for the partial derivatives of $f$. These can be expressed as the following \textit{Pfaffian system}\footnote{This is formalized in \cref{bundle:pfaff} and \cref{section:pde_variational}.}, where the partial derivatives are now treated as unknowns:
        \begin{gather}
            \begin{cases}
                \displaystyle \pderiv{f}{x}P+\pderiv{f}{y}Q = R\,,\\
                \displaystyle \pderiv{f}{x}\dr x+\pderiv{f}{y}\dr y = \dr f\,.
            \end{cases}
        \end{gather}
        By definition of integral curves, the derivatives with respect to the new coordinate $s$ should correspond to the coefficients of $X$, i.e.~the coefficients of the PDE:
        \begin{gather}
            \deriv{x}{s} = P\qquad\deriv{y}{s} = Q\qquad\deriv{z}{s} = R\,.
        \end{gather}
        It follows that the above system becomes underdetermined (leading to the arbitrary dependence on the parameter $t$). Algebraically, this means that
        \begin{gather}
            \det
            \begin{pmatrix}
                P&Q\\
                \dr x&\dr y
            \end{pmatrix}
            =0\,.
        \end{gather}
        By \textit{Cramer's rule} (see \cref{linalgebra:cramers_rule}), the existence of a (nonunique) solution then also requires that
        \begin{gather}
            \det
            \begin{pmatrix}
                P&R\\
                \dr x&\dr f
            \end{pmatrix}
            =0\,.
        \end{gather}
        The characteristic curves are thus defined by
        \begin{gather}
            \frac{\dr x}{P} = \frac{\dr y}{Q}
        \end{gather}
        and, along these curves, the condition
        \begin{gather}
            \frac{\dr x}{P} = \frac{\dr f}{R}
        \end{gather}
        should hold as a consistency condition. The solution of the ODEs for the integral curves are given by the Picard--Lindel\"of theorem~\ref{ode:picard_lindelof}, provided that initial conditions are known.
    }

    \begin{formula}[Lagrange--Charpit equations]\index{Lagrange--Charpit equations}
        The general solution of \cref{pde:first_order_pde} is implicitly given by $F(\xi,\eta)=0$, where $F(\xi,\eta)$ is an arbitrary differentiable function and $\xi(x,y,f) = c_1,\eta(x,y,f) = c_2$ are solutions of the Lagrange--Charpit equations
        \begin{gather}
            \frac{\dr x}{P} = \frac{\dr y}{Q} = \frac{\dr f}{R}\,,
        \end{gather}
        where $c_1,c_2$ are constants fixed by the boundary conditions.
    \end{formula}

\section{Second-order PDE}\label{section:characteristics}

    \newformula{Second order quasilinear PDE}{\label{pde:general_2order_pde}
        Consider the following quasilinear differential equation for a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$:
        \begin{gather}
            R(x,y)f_{xx} + S(x,y)f_{xy} + T(x,y)f_{yy} = W(x,y,f,p,q)\,,
        \end{gather}
        where $p:=f_x$ and $q:=f_y$.
    }

    \newformula{Characteristic equation}{\index{characteristic}
        Similar to the case of first-order PDEs, characteristic curves are characterized by the following condition:
        \begin{gather}
            \det
            \begin{pmatrix}
                R&S&T\\
                \dr x&\dr y&0\\
                0&\dr x&\dr y\\
            \end{pmatrix}
            =0\,.
        \end{gather}
        This is equivalent to the following equation:
        \begin{gather}
            \label{pde:defining_equation_characteristic_curves}
            R\left(\deriv{y}{x}\right)^2 - S\left(\deriv{y}{x}\right) + T = 0\,.
        \end{gather}
        Accordingly, this equation is often called the \textbf{characteristic equation} of the PDE in \cref{pde:general_2order_pde}. The PDE has been reduced to an ODE (as before, Cramer's rule gives rise to additional ODEs).
    }

    \newdef{Types of characteristics}{\index{elliptic!PDE}\index{hyperbolic equation}
        \Cref{pde:defining_equation_characteristic_curves} is quadratic in $\deriv{y}{x}$. The associated PDE is classified based on the roots of this polynomial:
        \begin{itemize}
            \item\textbf{Hyperbolic PDE}: Two distinct real roots.
            \item\textbf{Parabolic PDE}: One distinct root.
            \item\textbf{Elliptic PDE}: Two distinct complex roots.
        \end{itemize}
    }

    \newformula{Canonical form}{\index{characteristic}
        Consider the general change of variables
        \begin{gather}
            \xi\equiv\xi(x,y) \qquad \eta\equiv\eta(x,y) \qquad \zeta\equiv f(x,y)\,.
        \end{gather}
        After this transformation, the PDE in \cref{pde:general_2order_pde} becomes
        \begin{gather}
            A(\xi_x,\xi_y)\mpderiv{2}{\zeta}{\xi} + B(\xi_x,\xi_y,\eta_x,\eta_y)\frac{\partial^2\zeta}{\partial\xi\partial\eta} + A(\eta_x,\eta_y)\mpderiv{2}{\zeta}{\eta} = F(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
        \end{gather}
        where:
        \begin{itemize}
            \item $A(a,b) = Ra^2 + Sab + Tb^2$, and
            \item $B(a,b,c,d) = 2Rac + S(bc+ad) + 2Tbd$.
        \end{itemize}
        The discriminant $\Delta$ of the quadratic equation~\eqref{pde:defining_equation_characteristic_curves} allows to rephrase the classification of characteristics in terms of canonical forms. The fact that this classification is well defined follows from the result that the discriminant of \cref{pde:defining_equation_characteristic_curves} is, up to the square of the Jacobian of $(x,y)\longrightarrow(\xi,\eta)$, equal to $B(\xi_x,\xi_y,\eta_x,\eta_y)^2-4A(\xi_x,\xi_y)A(\eta_x,\eta_y)$.
        \begin{itemize}
            \item\textbf{Hyperbolic PDE} ($\Delta>0$): The sign of the discriminant implies that the quadratic equation $A=0$ has two real solutions $\varphi_1(x,y)$ and $\varphi_2(x,y)$. By choosing the transformation $\xi=\varphi_1(x,y)$ and $\eta=\varphi_2(x,y)$, the coefficients $A(a,b)$ are made to vanish and, hence, the canonical hyperbolic form is obtained:
                \begin{gather}
                    \label{pde:hyperbolic_canonical_form}
                    \frac{\partial^2\zeta}{\partial\xi\partial\eta} = H(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
                \end{gather}
                where $H := \dfrac{F}{2B(\xi_x,\xi_y,\eta_x,\eta_y)}$.
            \item\textbf{Parabolic PDE} ($\Delta=0$): As in the hyperbolic case, the change of variables $\xi=\varphi(x,y)$ is performed. However, there is only one root of the defining equation, so the second variable can be chosen freely under the constraint that it should be independent of $\varphi(x,y)$. From the condition $\Delta=0$, it is also possible to derive the conditions $B(\xi_x,\xi_y\eta_x\eta_y)=0$ and $A(\eta_x,\eta_y)\neq0$. This gives the parabolic canonical form:
                \begin{gather}
                    \label{pde:parabolic_canonical_form}
                    \mpderiv{2}{\zeta}{\eta} = G(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
                \end{gather}
                where $G := \dfrac{F}{A(\eta_x,\eta_y)}$.
            \item\textbf{Elliptic PDE} ($\Delta<0$): In this case, there are two complex roots. Writing $\xi = \alpha + i\beta$ and $\eta = \alpha - i\beta$ gives the following (real) equation:
            \begin{gather}
                \frac{\partial^2\zeta}{\partial\xi\partial\eta} = \frac{1}{4}\left(\mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta}\right)\,.
            \end{gather}
            Substituting this in the PDE (together with $A(a,b)=0$), results in the following elliptic canonical form:
                \begin{gather}
                    \label{pde:elliptic_canonical_form}
                    \mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta} = K(\alpha,\beta,\zeta,\zeta_\alpha,\zeta_\beta)\,,
                \end{gather}
            where $K = \frac{4F}{B(\xi_x,\xi_y,\eta_x,\eta_y)}$.
        \end{itemize}
    }

    \begin{theorem}[Maximum principle]\index{maximum!principle}\label{pde:maximum_principle}
        Consider a PDE of the parabolic or elliptic type. The maximum of the solution is to be found on the boundary of the domain.
    \end{theorem}

\section{General methods}
\subsection{Separation of variables}\label{section:separation_of_variables}

    \begin{remark*}[Computational cost]
        Solutions obtained by separation of variables are generalized Fourier series, which tend to converge rather slowly. For numerical purposes, other techniques are recommended. However, the series solutions often give a good insight into the properties of the solutions.
    \end{remark*}

    \begin{method}[Separation of variables]
        Consider a PDE on $\mathbb{R}^n$. A useful method to find solutions is to assume a solution of the form
        \begin{gather}
            f(x) = \prod_{i=1}^nf_i(x_i)\,.
        \end{gather}
        By substituting this form in the PDE and using some (basic) algebra, it is sometimes possible to reduce the PDE to a system of $n$ ODEs.
    \end{method}

    \begin{example}[Heat equation]\index{heat equation}\index{separation!constant}
        Consider the heat equation (\cref{pde:heat_equation}):
        \begin{gather}
            \pderiv{u}{t} - \mpderiv{2}{u}{x} = 0\,.
        \end{gather}
        Substituting a solution of the form $u(x,t) = X(x)T(t)$, gives \[X(x)\deriv{T(t)}{t} - T(t)\mderiv{2}{X(x)}{x} = 0\,.\] This can be rewritten as (the arguments are dropped for convenience) \[\frac{1}{T}\deriv{T}{t} = \frac{1}{X}\mderiv{2}{X}{x}\,.\] Because both sides are independent, they must be equal to a constant $\lambda$. (This constant is called the \textbf{constant of separation}) This results in the following system of ordinary differential equations:
        \begin{gather}
            \begin{cases}
                X''(x) &= \lambda X(x)\,,\\
                T'(t) &= \lambda T(t)\,.
            \end{cases}
        \end{gather}
        Solutions are given by
        \begin{gather}
            \begin{cases}
                X(x) &= c_1e^{\sqrt{\lambda}x}+c_2e^{-\sqrt{\lambda}x}\,,\\
                T(t) &= c_3e^{\lambda t}+c_4e^{-\lambda t}\,.
            \end{cases}
        \end{gather}
    \end{example}

\section{Boundary conditions}\index{boundary!condition}

    Besides the boundary conditions from \cref{chapter:ODE}, PDEs allow for new types of boundary conditions.

    \newdef{Neumann boundary condition}{\label{ode:neumann_conditions}
        A condition of the form
        \begin{gather}
            \pderiv{f}{\hat{n}}(x) = f(x)
        \end{gather}
        for all $x\in\partial\Omega$, where $\Omega$ is the domain on which the problem is defined.
    }

    \newdef{Inhomogeneous boundary condition}{\label{pde:inhomogeneous_boundary_condition}
        \begin{gather}
            \alpha f(a,t) + \beta\pderiv{f}{x}(a,t) = h(t)\,,
        \end{gather}
        where $f$ is defined on $[a,b]\times\mathbb{R}$. When $h$ is identically zero, the boundary condition becomes \textbf{homogeneous}.
    }

    \newmethod{Steady-state solution}{\index{steady-state!method for boundary conditions}
        Assume that the function $h$ is constant. In this case, it is useful to rewrite the solution as
        \begin{gather}
            f(x,t) = v(x) + w(x,t)\,.
        \end{gather}
        The `time-independent' function $v$ is called the steady-state solution and the function $w$ represents the deviation from this steady-state scenario.

        In case the PDE is linear, the partial solutions $v$ and $w$ are required to individually satisfy the equation. Furthermore, if the function $v$ is also required to satisfy the given inhomogeneous boundary conditions, $w$ becomes a solution of a homogeneous PDE with homogeneous boundary conditions.
    }

    \begin{method}
        If the function $h$ is not a constant, a different method can be used. Decompose the solution as $f(x,t) = v(x,t) + w(x,t)$, where $v$ is now only required to be some function that satisfies the boundary conditions (and not the PDE)\footnote{As there are infinitely many possible functions that satisfy the boundary conditions, the best choice for $v$ is the one that makes the equation for $w$ as simple as possible.}. This will lead to $w$ satisfying the homogeneous boundary conditions as in the previous method. After substituting the function $v$ in the PDE, a differential equation for $w(x,t)$ is obtained (which, now, can be inhomogeneous).
    \end{method}

    A third, sometimes useful method is the following.
    \begin{method}
        If the problem consists of three homogeneous and one inhomogeneous boundary conditions, the problem can be solved by first using the homogeneous conditions to restrict the values of the constant of separation and then obtain a series expansion. Afterwards, the obtained series can be matched to the inhomogeneous condition to obtain the final remaining coefficients.

        If there is more than one inhomogeneous boundary condition, this method can be extended. Let there be $k\in\mathbb{N}_0$ boundary conditions. Rewrite the general solution as $f(x,t) = \sum_{i=1}^kv_i(x,t)$, where $v_i$ satisfies the $i^{\text{th}}$ inhomogeneous condition and the homogeneous versions of the other conditions. This way, the general solution still satisfies all conditions and the first part of the method can be applied to all functions $v_i$ to obtain a series expansion.
    \end{method}

    \begin{method}[Inhomogeneous PDE]\index{PDE!inhomogeneous}
        Inhomogeneous second-order PDEs of the form
        \begin{gather}
            Df(x,t) = \varphi(x,t)\,,
        \end{gather}
        given a set of homogeneous boundary conditions and initial value conditions
        \begin{gather}
            f(x,0)=\psi(x)\,,
        \end{gather}
        can be solved using the following method (where all involved functions are assumed to admit a generalized Fourier expansion):
        \begin{enumerate}
            \item Solve the homogeneous version of the PDE. This will result in a series expansion
            \begin{gather}
                \sum_iw_i(t)e_i(x)\,,
            \end{gather}
            where the $e_i$ are a complete set of eigenfunctions in the variable $x$. This solution should satisfy the (homogeneous\footnote{Inhomogeneous boundary conditions can be turned into homogeneous ones by the previous two methods.}) boundary conditions.
            \item Expand the source function $\varphi$ in the same eigenbasis as $f$. The coefficients $\varphi_i$ can be found using the orthogonality relations of the functions $e_n$.
            \item Inserting these expansions in the original PDE and rewriting the equation, will lead to a series of the form
            \begin{gather}
                \sum_i\bigl(\widetilde{D}w_i(t)\bigr)e_i(x) = 0\,,
            \end{gather}
            where $\widetilde{D}$ is a linear, first-order differential operator. Because all terms are independent, this gives $n$ first order ODEs for the functions $w_i$. These can generally be solved using \cref{ode:first_order_general_solution}.
            \item Initial value conditions for the functions $w_i$ are applied by setting $t=0$ in the series expansion of $f$ and equating it with the series expansion of $\psi$. This results in conditions $w_i(0)=\Psi_i$.
            \item The resulting ODEs, together with the found boundary conditions $w_i(0)=\Psi_i$, will give the general solutions for $w_i$.
            \item Inserting these solutions in the series expansion of $f$ will give the general solution of the inhomogeneous PDE.
        \end{enumerate}
    \end{method}
    \remark{The requirement that all involved functions admit a generalized Fourier expansion is restrictive. Not all inhomogeneous PDEs are solvable by this method.}

\subsection{Dirichlet problem}\index{Dirichlet!problem}

    The (interior) Dirichlet problem consists of finding a solution to a PDE in a finite region, given the value of the function on the boundary of that region, i.e.~given boundary conditions of the form $f|_{\partial\Omega}=0$. The uniqueness of a solution follows from the Maximum Principle~\ref{pde:maximum_principle} if the PDE is elliptic (such as Laplace's equation).
    \begin{proof}
        Let $\phi,\psi$ be two solutions of the interior Dirichlet problem. Due to linearity, both $\psi-\phi$ and $\phi-\psi$ are solutions as well (without applying the boundary conditions). According to the maximum principle, these solutions achieve their maximum on the boundary of the domain. Furthermore, due to the Dirichlet boundary conditions, $\phi(x)=\psi(x)$ for all $x\in\partial\Omega$. Combining these two facts gives $\max(\psi-\phi) = \max(\phi-\psi) = 0$ or alternatively $\psi\leq\phi$ and $\phi\leq\psi$ on the whole domain. This implies that $\phi=\psi$ on the whole domain.
    \end{proof}

    \sremark{There is also an exterior Dirichlet problem, where one has to find the solution of the PDE, given the boundary conditions, outside of the boundary.}

    \newdef{Green's function}{\index{Green!function}\label{pde:green_function}
        A fundamental solution (\cref{distributions:fundamental_solution}) of a Dirichlet problem.
    }

\section{Symbols}

    \newdef{Symbol}{\index{symbol}
        Consider a general $k^{\text{th}}$-order differential operator (multi-indices $\alpha$ are used)
        \begin{gather}
            D := \sum_{|\alpha|\leq k}c_\alpha(x)D^\alpha\,.
        \end{gather}
        The symbol of this operator is defined by replacing the partial derivatives by indeterminates $\xi^\alpha$:
        \begin{gather}
            p(D,\xi) := \sum_{|\alpha|\leq k}c_\alpha(x)\xi^\alpha\,.
        \end{gather}
    }
    \newdef{Principal symbol}{\index{principal!symbol}\label{pde:principal_symbol}
        The principal symbol of a $k^{\text{th}}$-order differential operator $D$ is defined as the highest-degree component of $p(D,\xi)$:
        \begin{gather}
            \sigma_D(\xi) := \sum_{|\alpha|=k}c_\alpha(x)\xi^\alpha\,.
        \end{gather}
        For a system of PDEs, the functions $c_\alpha$ are replaced by functions $c_\alpha^j$ (which can be regarded as matrix-valued functions).
    }

    \begin{property}[Tensoriality]
        The principal symbol of a differential operator transforms as a tensor.
    \end{property}
    \newdef{Ellipticity}{\index{elliptic!operator}
        A system of PDEs $Df(x)=0$ is elliptic if and only if $\sigma_D$ is invertible. Note that this is only possible if the number of variables is smaller than the number of equations, hence if the system is at most determined.
    }

\section{Sobolev spaces}

    Using the theory of $L^p$-spaces and distributions (\namecrefs{chapter:measure}~\nameref{chapter:measure} and~\nameref{chapter:distributions}), one can define an important class of function spaces that are ubiquitous in the field of PDEs (and beyond).

    \newdef{Sobolev space}{\index{Sobolev!space}
        For all nonnegative integers $m,p\in\mathbb{N}$, with $p\geq1$, one defines the Sobolev space $W^{m,p}(U)$ as the space of functions in $L^p(U)$ for which the weak derivatives (\cref{distributions:weak_derivative}) up to order $m$ are also in $L^p(U)$. When $p=2$, i.e.~when restricted to square-integrable functions, the notation $H^m(U)$ is often used.

        This space can be turned into a normed space by equipping it with the following norm:
        \begin{gather}
            \label{pde:sobolev_norm}
            \|f\| := \left(\sum_{|\alpha|\leq m}\|f^{(\alpha)}\|_{L^p}\right)^{1/p}\,.
        \end{gather}
        Using the fact that the Fourier transform $\mathcal{F}$ defines an isometry on $L^2$, one can also define $H^m$ in a different way:
        \begin{gather}
            \label{pde:sobolev_fourier}
            H^m(\mathbb{R}^n) := \left\{f\in L^2(\mathbb{R}^n)\,\middle\vert\,(1+\|x\|^2)^{-m/2}\mathcal{F}\!f\in L^2(\mathbb{R}^2)\right\}\,.
        \end{gather}
    }

    Sobolev spaces inherit the following property from the $L^p$-spaces.
    \begin{property}[Completeness]
        Every Sobolev space is a Banach space. Moreover, one can show that the spaces $H^m$ are Hilbert spaces.
    \end{property}
    Sobolev spaces also satisfy the following density theorem.
    \begin{property}
        $\mathcal{D}(\mathbb{R}^n)$ is dense in $W^{m,p}(\mathbb{R}^n)$ for all $m\in\mathbb{N}$. However, only for $m=0$ can this be proven for all open subsets of $\mathbb{R}^n$.
    \end{property}

    \begin{property}[Sobolev embedding]\index{Sobolev!embedding}
        Consider two integers $m,n\in\mathbb{N}$. If $f\in H^m(\mathbb{R}^n)$ and $m>n/2$, then $f$ vanishes at infinity.
    \end{property}

    The Sobolev norm is not always easy to work with, especially in practical applications. Luckily, one can restrict to partial derivatives of order $m$.
    \begin{theorem}[Friedrich]\index{Friedrich}
        For all bounded $U$, the following norm on $H^m_0(U)$ is equivalent to the Sobolev norm~\eqref{pde:sobolev_norm}:
        \begin{gather}
            \langle f\mid g \rangle := \sum_{|\alpha|=m}\Int_U f^{(\alpha)}\overline{g^{(\alpha)}}\,dx\,.
        \end{gather}
    \end{theorem}

    \Cref{measure:Lp_duals} allows to define the dual Sobolev spaces.
    \begin{definition}
        The space $W^{-m,p}(U)\subset\mathcal{D}'(U)$ is defined as the dual of $\overline{W^{m,p}(U)}$. For $m=2$, one can again use a characterization similar to \cref{pde:sobolev_fourier}. It can be shown that all elements in $W^{-m,p}(U)$ are of the form
        \begin{gather}
            T = \sum_{|\alpha|\leq m}f^{(\alpha)}_\alpha\,,
        \end{gather}
        where $f_\alpha\in L^{p'}(U)$, with $p'$ the H\"older conjugate of $p$ (\cref{measure:holders_inequality}).
    \end{definition}
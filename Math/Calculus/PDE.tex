\chapter{Partial differential equations}\label{chapter:pde}

    For a rigorous treatment of partial differential equations, the language of distributions is required. For an introduction, see \cref{chapter:distributions}.

\section{Important examples}
\subsection{Laplace equation}

    \begin{example}[Laplace equation]\index{Laplace!equation}\index{harmonic!function}\index{harmonic!cylindrical}\label{pde:laplace_equation}
        \begin{gather}
            \Delta f = \sum_{i=1}^d\mpderiv{2}{f}{x_i}=0
        \end{gather}
        Functions satisyfing this equation are said to be \textbf{harmonic}.
    \end{example}

    When solving the Laplace equation in cylindrical coordinates, one obtains Bessel's ODE~\eqref{ode:bessel_ode} with integer $n$. The solutions are the Bessel functions~\eqref{ode:bessel_function} and~\eqref{ode:neumann_function}. For this reason, Bessel functions for integer $n$ are also called \textbf{cylinder functions} or \textbf{cylindrical harmonics}.

\subsection{Helmholtz equation}

    \begin{example}[Helmholtz equation]\index{Helmholtz!equation}\index{Bessel!function}\label{pde:helmholtz_equation}
        \begin{gather}
            \Delta f = -k^2f
        \end{gather}
    \end{example}

    When solving the Helmholtz equation in spherical coordinates, one obtains a variant of the Bessel ODE~\eqref{ode:bessel_ode} for the radial part:
    \begin{gather}
        r^2y''(r) + 2ry'(r) + \bigl(r^2 - n(n+1)\bigr)y(r) = 0\,,
    \end{gather}
    where $n$ is an integer. The solutions, called the \textbf{spherical Bessel functions}, are related to the cylindrical Bessel functions in the following way (similarly for the Neumann functions):
    \begin{gather}
        j_n(r) = \sqrt{\frac{\pi}{2x}}J_{n + \frac{1}{2}}(r)\,.
    \end{gather}

    \todo{ADD (Poisson, heat, ...)}

\section{General linear equations}

    \newdef{Characteristic curve}{\index{characteristic!curve}
        A curve along which the highest-order partial derivatives are discontinuous. Equivalently, a curve along which the partial differential equation reduces to an ordinary differential equation (see the next sections).
    }

    \todo{COMPLETE or RESTRUCTURE}

\section{First-order PDE}

    \newformula{First-order quasilinear PDE}{\label{pde:first_order_pde}
        \begin{gather}
            P(x,y,u)\pderiv{u}{x} + Q(x,y,u)\pderiv{u}{y} = R(x,y,u)
        \end{gather}
    }

    \newformula{Characteristic curve}{\index{Pfaff!system}\index{integral!curve}
        Consider a first-order quasilinear PDE as above. The idea of the method of the characteristics is to obtain a new coordinate system $(s,t)$ such that the PDE becomes an ODE in the coordinate $s$ and, accordingly, that the solution has an arbitrary dependence on $t$. The curves of constant $t$ are called the \textbf{characteristics} of the PDE.

        To this end, the coefficients of the PDE are interpreted as the coefficients of a vector field $X\equiv(P,Q,R):\mathbb{R}^3\rightarrow\mathbb{R}^3$. By \cref{vector:normal_vector}, it can be shown that this vector field is tangent to the surface $u(x,y)$. The characteristic curves then correspond to the \textit{integral curves} of $X$ (see \cref{bundle:integral_curve}), which foliate $u$. Along a general curve, one obtains two linear equations for the partial derivatives $u_{,x},u_{,y}$, expressed as the following \textit{Pfaffian system}\footnote{Formalized in \cref{bundle:pfaff}.}:
        \begin{gather}
            \begin{cases}
                P\pderiv{u}{x}+Q\pderiv{u}{y} = R\,,\\
                \pderiv{u}{x}\dr x+\pderiv{u}{y}\dr y = \dr u\,.
            \end{cases}
        \end{gather}
        For characteristics, the derivatives with respect to the new coordinate $s$, should correspond with the coefficients of $X$, i.e.~the coefficients of the PDE. It follows that the above system should degenerate. Algebraically, this gives the following condition:
        \begin{gather}
            \det
            \begin{pmatrix}
                P&Q\\
                \dr x&\dr y
            \end{pmatrix}
            =0\,.
        \end{gather}
        By Cramer's rule~\ref{linalgebra:cramers_rule}, the existence of a (nonunique) solution also requires that
        \begin{gather}
            \det
            \begin{pmatrix}
                P&R\\
                \dr x&\dr u
            \end{pmatrix}
            =0\,.
        \end{gather}
        The characteristic curves are thus defined by
        \begin{gather}
            \frac{\dr x}{P} = \frac{\dr y}{Q}
        \end{gather}
        and along these curves the condition
        \begin{gather}
            \frac{\dr x}{P} = \frac{\dr u}{R}
        \end{gather}
        should hold as a consistency condition. The solution of the ODEs for the integral curves are given by the Picard--Lindel\"of theorem~\ref{ode:picard_lindelof}, provided that initial conditions are known.
    }

    \begin{formula}[Lagrange--Charpit equations]\index{Lagrange--Charpit equations}
        The general solution of \cref{pde:first_order_pde} is implicitly given by $F(\xi,\eta)=0$, where $F(\xi,\eta)$ is an arbitrary differentiable function and $\xi(x,y,u) = c_1,\eta(x,y,u) = c_2$ are solutions of the Lagrange--Charpit equations
        \begin{gather}
            \frac{dx}{P} = \frac{dy}{Q} = \frac{du}{R}\,,
        \end{gather}
        where $c_1,c_2$ are constants fixed by the boundary conditions.
    \end{formula}
    \remark{Looking at the defining equations of the characteristic curve, it is clear that these fix the general solution of the PDE.}

\section{Second-order PDE}\label{section:characteristics}

    \newformula{Second order quasilinear PDE}{\label{pde:general_2order_pde}
        Consider the following pseudolinear differential equation for a function $u:\mathbb{R}^2\rightarrow\mathbb{R}$:
        \begin{gather}
            R(x,y)u_{xx} + S(x,y)u_{xy} + T(x,y)u_{yy} = W(x,y,u,p,q)\,,
        \end{gather}
        where $p:=u_x$ and $q:=u_y$.
    }

    \newformula{Characteristic equation}{\index{characteristic}
        Similar to the case of first-order PDEs, characteristic curves are characterized by the following condition:
        \begin{gather}
            \det
            \begin{pmatrix}
                R&S&T\\
                \dr x&\dr y&0\\
                0&\dr x&\dr y\\
            \end{pmatrix}
            =0\,.
        \end{gather}
        This is equivalent to the following equation:
        \begin{gather}
            \label{pde:defining_equation_characteristic_curves}
            R\left(\deriv{y}{x}\right)^2 - S\left(\deriv{y}{x}\right) + T = 0\,.
        \end{gather}
        Accordingly, this equation is often called the \textbf{characteristic equation} of the PDE in \cref{pde:general_2order_pde}. The PDE has been reduced to an ODE (as before, Cramer's rule gives rise to additional ODEs).
    }

    \newdef{Types of characteristics}{\index{elliptic!PDE}\index{hyperbolic equation}
        \Cref{pde:defining_equation_characteristic_curves} is quadratic in $\deriv{y}{x}$. If this equation has two distinct real roots, the PDE is said to be \textbf{hyperbolic}. If the equation has only one root, the PDE is said to be \textbf{parabolic}. In the remaining case, where the equation has two distinct complex roots, the PDE is said to be \textbf{elliptic}.
    }

    \newformula{Canonical form}{\index{characteristic}
        Consider the general change of variables \[\xi = \xi(x,y)\qquad\eta = \eta(x,y)\qquad\zeta\equiv u.\] After this transformation, the PDE in \cref{pde:general_2order_pde} becomes
        \begin{gather}
            A(\xi_x,\xi_y)\mpderiv{2}{\zeta}{\xi} + B(\xi_x,\xi_y,\eta_x,\eta_y)\frac{\partial^2\zeta}{\partial\xi\partial\eta} + A(\eta_x,\eta_y)\mpderiv{2}{\zeta}{\eta} = F(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
        \end{gather}
        where:
        \begin{itemize}
            \item $A(a,b) = Ra^2 + Sab + Tb^2$, and
            \item $B(a,b,c,d) = 2Rac + S(bc+ad) + 2Tbd$.
        \end{itemize}
        The discriminant $\Delta$ of the quadratic equation~\eqref{pde:defining_equation_characteristic_curves} allows to rephrase the classification of characteristics in terms of canonical forms. The fact that this classification is well-defined follows from the result that the discriminant of \cref{pde:defining_equation_characteristic_curves} is, up to the square of the Jacobian of $(x,y)\longrightarrow(\xi,\eta)$, equal to $B(\xi_x,\xi_y,\eta_x,\eta_y)^2-4A(\xi_x,\xi_y)A(\eta_x,\eta_y)$.
        \begin{itemize}
            \item\textbf{Hyperbolic PDE} ($\Delta>0$): The sign of the discriminant implies that the quadratic equation $A=0$ has two real solutions $f_1(x,y)$ and $f_2(x,y)$. By choosing the transformation $\xi=f_1(x,y)$ and $\eta=f_2(x,y)$, the coefficients $A(a,b)$ are made to vanish and, hence, the canonical hyperbolic form is obtained:
                \begin{gather}
                    \label{pde:hyperbolic_canonical_form}
                    \frac{\partial^2\zeta}{\partial\xi\partial\eta} = H(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
                \end{gather}
                where $H := \dfrac{F}{2B(\xi_x,\xi_y\eta_x\eta_y)}$.
            \item\textbf{Parabolic PDE} ($\Delta=0$): As in the hyperbolic case the change of variables $\xi=f(x,y)$ is performed. However, there is only one root of the defining equation, so the second variable can be chosen freely under the constraint that it should be independent of $f_1(x,y)$. From the condition $\Delta=0$ it is also possible to derive the conditions that $B(\xi_x,\xi_y\eta_x\eta_y)=0$ and $A(\eta_x,\eta_y)\neq0$. This gives the parabolic canonical form:
                \begin{gather}
                    \label{pde:parabolic_canonical_form}
                    \mpderiv{2}{\zeta}{\eta} = G(\xi,\eta,\zeta,\zeta_\xi,\zeta_\eta)\,,
                \end{gather}
                where $G := \dfrac{F}{A(\eta_x,\eta_y)}$.
            \item\textbf{Elliptic PDE} ($\Delta<0$): In this case there are two complex roots. Writing $\xi = \alpha + i\beta$ and $\eta = \alpha - i\beta$ gives the following (real) equation: \[\frac{\partial^2\zeta}{\partial\xi\partial\eta} = \frac{1}{4}\left(\mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta}\right).\] Substituting this in the PDE (together with $A(a,b)=0$) results in the following elliptic canonical form:
                \begin{gather}
                    \label{pde:elliptic_canonical_form}
                    \mpderiv{2}{\zeta}{\alpha} + \mpderiv{2}{\zeta}{\beta} = K(\alpha,\beta,\zeta,\zeta_\alpha,\zeta_\beta)\,.
                \end{gather}
        \end{itemize}
    }

    \begin{theorem}[Maximum principle]\index{maximum!principle}\label{pde:maximum_principle}
        Consider a PDE of the parabolic or elliptic type. The maximum of the solution is to be found on the boundary of the domain.
    \end{theorem}

\section{Separation of variables}\label{section:separation_of_variables}

    \begin{remark*}
        Solutions obtained by separation of variables are generalized Fourier series, which tend to converge rather slowly. For numerical purposes, other techniques are recommended. However, the series solutions often give a good insight in the properties of the solutions.
    \end{remark*}

\subsection{Cartesian coordinates}

    \begin{method}[Separation of variables]
        Let $D$ be the operator associated with a partial differential equation such that $Du(x)=0$, where $x:=(x_1,\ldots,x_n)$ denotes the set of variables. A useful method to find solutions is to assume a solution of the form
        \begin{gather}
            u(x) = \prod_{i=1}^nu_i(x_i)\,.
        \end{gather}
        By substituting this form in the PDE and using some (basic) algebra it is sometimes possible to reduce the partial differential equation to a system of $n$ ordinary differential equations.
    \end{method}

    \begin{example}
        Consider the following PDE:
        \begin{gather}
            \pderiv{u}{t} - a\mpderiv{2}{u}{x} = 0\,.
        \end{gather}
        Substituting a solution of the form $u(x,t) = X(x)T(t)$ gives \[X(x)\deriv{T(t)}{t} - aT(t)\mderiv{2}{X(x)}{x} = 0\,,\] which can be rewritten as (the arguments are dropped for convenience) \[\frac{1}{aT}\deriv{T}{t} = \frac{1}{X}\mderiv{2}{X}{x}\,.\] Because both sides are independent, they must be equal to a constant $\lambda$. This results in the following system of ordinary differential equations:
        \begin{gather}
            \begin{cases}
                X''(x) &= \lambda X(x)\,,\\
                T'(t) &= a\lambda T(t)\,.
            \end{cases}
        \end{gather}
    \end{example}

\section{Boundary conditions}

    \newformula{Inhomogeneous boundary condition}{\label{pde:inhomogeneous_boundary_condition}
        \begin{gather}
            \alpha u(a,t) + \beta\pderiv{u}{x}(a,t) = h(t)
        \end{gather}
        When $h$ is identically zero, the boundary condition becomes \textbf{homogeneous}.
    }

    \newmethod{Steady-state solution}{\index{steady-state!method for boundary conditions}
        Assume that the function $h$ is constant. In this case, it is useful to rewrite the solution as
        \begin{gather}
            u(x,t) = v(x) + w(x,t)\,.
        \end{gather}
        The `time-independent' function $v$ is called the steady-state solution and the function $w$ represents the deviation of this steady-state scenario.

        Because the PDE is linear, the partial solutions $v$ and $w$ are required to individually satisfy the equation. Furthermore, the function $v$ is also required too satisfy the given inhomogeneous boundary conditions. This results in $w$ being the solution of a homogeneous PDE with homogeneous boundary conditions. This can be seen in the following proof.
        \begin{proof}
            Assume a boundary condition of the form \[\alpha u(a,t) + \beta\pderiv{u}{x}(a,t) = u_0\,.\] Due to the requirements of a steady-state solution, one also has \[\alpha v(a) + \beta\pderiv{v}{x}(a) = u_0\,.\] Combining these two conditions gives \[\alpha\left[v(a) + w(a,t)\right] + \beta\left[\pderiv{v}{x}(a) + \pderiv{w}{x}(a,t)\right] = \alpha v(a) + \beta\pderiv{v}{x}(a)\,.\] Using the conditions, this can be rewritten as \[\alpha w(a,t) + \beta\pderiv{w}{x}(a,t) = 0\,.\] The steady-state deviation $w(x,t)$ thus satisfies the homogeneous boundary conditions.
        \end{proof}

        \todo{CHECK (is this proof relevant)}
    }

    \begin{method}
        If the function $h$ is not a constant, a different method can be used. Rewrite the solution as $u(x,t) = v(x,t) + w(x,t)$, where $v$ is only required to be some function that satisfies the boundary conditions (and not the PDE)\footnote{As there are infinitely many possible functions that satisfy the boundary conditions, the best choice for $v$ is the one that makes the equation for $w$ as simple as possible.}. This will lead to $w$ satisfying the homogeneous boundary conditions as in the previous method. After substituting the function $v$ in the PDE, a differential equation for $w(x,t)$ is obtained (it can be inhomogeneous).
    \end{method}

    A third, sometimes useful method is the following.
    \begin{method}
        If the problem consists of three homogeneous and one inhomogeneous boundary conditions, the problem can be solved by first using the homogeneous conditions to restrict the values of the separation constant and then obtain a series expansion. Afterwards, the obtained series can be matched to the inhomogeneous condition to obtain the final remaining coefficients.

        If there is more than one inhomogeneous boundary condition, the method can be extended. Let there be $k$ boundary conditions. Rewrite the general solution as $u(x,t) = \sum_{i=1}^kv_i(x,t)$, where $v_i$ satisfies the $i^{\text{th}}$ inhomogeneous condition and the homogeneous versions of the other conditions. This way, the general solution still satisfies all conditions and the first part of the method can be applied to all functions $v_i$ to obtain a series expansion.
    \end{method}

    \begin{method}[Inhomogeneous PDE]\index{PDE!inhomogeneous}
        A possible way to solve inhomogeneous second order partial differential equations of the form
        \begin{gather}
            Du(x,t) = f(x,t)\,,
        \end{gather}
        given a set of homogeneous boundary conditions and initial value conditions $w(x,0)=\psi(x)$, is the following method (where all involved functions are assumed to admit a generalized Fourier expansion):
        \begin{enumerate}
            \item Solve the homogeneous version of the PDE. This will result in a series expansion
            \begin{gather}
                \sum_iw_i(t)e_i(x)\,,
            \end{gather}
            where the $e_i$ are a complete set of eigenfunctions in the variable $x$. This solution should satisfy the (homogeneous\footnote{Inhomogeneous boundary conditions can be turned into homogeneous ones by the previous two methods.}) boundary conditions.
            \item Expand the function $f$ in the same way as $u$. The coefficients $f_n$ can be found using the orthogonality relations of the functions $e_n$.
            \item Inserting these expansions in the original PDE and rewriting the equation will lead to a summation of the form
            \begin{gather}
                \sum_i\bigl(\widetilde{D}w_i(t)\bigr)e_i(x) = 0\,,
            \end{gather}
            where $\widetilde{D}$ is a linear first-order differential operator. Because all terms are independent, this gives $n$ first order ODEs for the functions $w_i$. These can generally be solved by using \cref{ode:first_order_general_solution}.
            \item Initial value conditions for the functions $w_i$ are applied by setting $t=0$ in the series expansion of $u$ and equating it with the series expansion of $\psi$. This results in conditions $w_i(0) = \Psi_i$.
            \item The obtained ODEs, together with the found boundary conditions $w_i(0) = \Psi_i$, will give the general solutions for $w_i$.
            \item Inserting these solutions in the series expansion of $u$ will give the general solution of the inhomogeneous PDE.
        \end{enumerate}
    \end{method}
    \remark{The requirement that all involved functions admit a generalized Fourier expansion is restrictive. Not all inhomogeneous PDEs are solvable by this method.}

\subsection{Dirichlet problem}\index{Dirichlet!problem}

    The (interior) Dirichlet problem is the problem of finding a solution to a PDE in a finite region, given the value of the function on the boundary of the region, i.e.~given boundary conditions of the form $u|_{\partial\Omega}=0$. The uniqueness of a solution can be proven with the Maximum Principle~\ref{pde:maximum_principle} if the PDE is of the elliptic kind (such as the Laplace equation).
    \begin{proof}
        Let $\phi,\psi$ be two solutions of the interior Dirichlet problem. Due to linearity both $\psi-\phi$ and $\phi-\psi$ are solutions too (without applying the boundary conditions). According to the maximum principle, these solutions achieve their maximum on the boundary of the domain. Furthermore, due to the Dirichlet boundary conditions, $\phi(x)=\psi(x)$ for all $x\in\partial\Omega$. Combining these two facts gives $\max(\psi-\phi) = \max(\phi-\psi) = 0$ or alternatively $\psi\leq\phi$ and $\phi\leq\psi$ on the whole domain. This implies that $\phi=\psi$ on the whole domain.
    \end{proof}

    \sremark{There is also an exterior Dirichlet problem, where one has to find the solution of the PDE, given the boundary conditions, outside of the boundary.}

    \newdef{Green's function}{\index{Green!function}\label{pde:green_function}
        A fundamental solution (\cref{distributions:fundamental_solution}) of a Dirichlet problem.
    }

\section{General}\label{section:pde_general}
\subsection{Symbols}

    \newdef{Symbol}{\index{symbol}
        Consider a general $k^{\text{th}}$-order differential operator (multi-indices $\alpha$ are used)
        \begin{gather}
            D := \sum_{|\alpha|\leq k}c_\alpha(x)D^\alpha\,.
        \end{gather}
        The symbol of this operator is defined by replacing the partial derivatives by indeterminates $\xi^i$:
        \begin{gather}
            p(D,\xi) := \sum_{|\alpha|\leq k}c_\alpha(x)\xi^\alpha\,.
        \end{gather}
    }
    \newdef{Principal symbol}{\index{principal!symbol}\label{pde:principal_symbol}
        The principal symbol of a $k^{\text{th}}$-order differential operator $D$ is defined as the highest-degree component of $p(D,\xi)$:
        \begin{gather}
            \sigma_D(\xi) := \sum_{|\alpha|=k}c_\alpha(x)\xi^\alpha\,.
        \end{gather}
        For a system of partial differential equations, the functions $c_\alpha$ are replaced by matrix-valued functions $\bigl(c_i^j\bigr)_\alpha$.
    }

    \begin{property}
        The principal symbol of a differential operator transforms as a tensor.
    \end{property}
    \begin{definition}[Ellipticity]\index{elliptic!operator}
        A system of PDEs \[Df(x) = 0\] is elliptic if and only if $\sigma_D$ is invertible. Note that this is only possible if the number of variables is smaller than the number of equations, hence if the system is at most determined.
    \end{definition}

\section{Sobolev spaces}

    Using the theory of $L^p$-spaces and distributions (\cref{chapter:measure} and \cref{chapter:distributions}), one can define an important class of function spaces that are ubiquitous in the field of PDEs (and beyond).

    \newdef{Sobolev space}{\index{Sobolev!space}
        \nomenclature[S_Sob]{$W^{m,p}(U)$}{the Sobolov space in $L^p$ of order $m$}
        For all nonnegative integers $m,p\in\mathbb{N}$, with $p\geq1$, one defines the Sobolev space $W^{m,p}(U)$ as the space of functions in $L^p(U)$ for which the weak derivatives (\cref{distributions:weak_derivative}) up to order $m$ are also in $L^p(U)$. When $p=2$, i.e.~when restricted to square-integrable functions, the notation $H^m(U)$ is often used.

        This space can be turned into a normed space by equipping it with the following norm:
        \begin{gather}
            \|f\| := \left(\sum_{|\alpha|\leq m}\|f^{(\alpha)}\|_{L^p}\right)^{1/p}\,.
        \end{gather}
        Using the fact that the Fourier transform $\mathcal{F}$ defines an isometry on $L^2$, one can also define $H^m$ in a different way:
        \begin{gather}
            \label{pde:sobolev_fourier}
            H^m(\mathbb{R}^n) := \left\{f\in L^2(\mathbb{R}^n)\,\middle\vert\,(1+\|x\|^2)^{-m/2}\mathcal{F}f\in L^2(\mathbb{R}^2)\right\}\,.
        \end{gather}
    }

    Sobolev spaces inherit the following property from the $L^p$-spaces.
    \begin{property}[Completeness]
        Every Sobolev space is a Banach space. Moreover, one can show that the spaces $H^m$ are Hilbert spaces.
    \end{property}
    Sobolev spaces also satisfy the following density theorem.
    \begin{property}
        $\mathcal{D}(\mathbb{R}^n)$ is dense in $W^{m,p}(\mathbb{R}^n)$ for all $m\in\mathbb{N}$. However, only for $m=0$ can this be proven for open subsets of $\mathbb{R}^n$.
    \end{property}

    \begin{property}[Sobolev embedding]
        Consider two integers $m,n\in\mathbb{N}$. If $f\in H^m(\mathbb{R}^n)$ and $m>n/2$, then $f$ vanishes at infinity.
    \end{property}

    The Sobolev norm is not always easy to work with, especially in practical applications. Luckily there exists a lemma showing that one can equivalently restrict to partial derivatives of order $m$.
    \begin{theorem}[Friedrich]\index{Friedrich}
        For all bounded $U$, one can introduce an equivalent norm on $H^m_0(U)$ as follows:
        \begin{gather}
            \langle f\mid g \rangle := \sum_{|\alpha|=m}\bigintsss_U f^{(\alpha)}\overline{g^{(\alpha)}}\,dx\,.
        \end{gather}
    \end{theorem}

    \Cref{measure:Lp_duals} allows to define the dual Sobolev spaces.
    \begin{definition}
        The space $W^{-m,p}(U)\subset\mathcal{D}'(U)$ is defined as the dual of $\overline{W^{m,p}(U)}$. For $m=2$, one can again use a characterization similar to \cref{pde:sobolev_fourier}. It can be shown that all elements in $W^{-m,p}(U)$ are of the form
        \begin{gather}
            T = \sum_{|\alpha|\leq m}f^{(\alpha)}_\alpha\,,
        \end{gather}
        where $f_\alpha\in L^{p'}(U)$ with $p'$ the H\"older conjugate of $p$.
    \end{definition}

    \todo{COMPLETE (continue in AMP1)}
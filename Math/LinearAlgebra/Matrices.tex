\section{Matrices}

    \begin{notation}\label{linalgebra:matrix_set}
        The vector space of all $m\times n$-matrices defined over the field $\mathfrak{K}$ is denoted by $M_{m,n}(\mathfrak{K})$. If $m=n$, the space is denoted by $M_n(\mathfrak{K})$ or $M(n,\mathfrak{K})$.
    \end{notation}

    \begin{property}[Dimension]\index{dimension!of matrix}\label{linalgebra:dimension_of_matrix_space}
        The dimension of $M_{m,n}(\mathfrak{K})$ is $mn$.
    \end{property}

    \newdef{General linear group}{\index{general linear group}\label{linalgebra:GL_matrices}
        The set of invertible matrices is called the general linear group and is denoted by $\GL_n(\mathfrak{K})$ or $\GL(n,\mathfrak{K})$.
    }
    \begin{property}
        For all $A\in\GL_n(\mathfrak{K})$, one has:
        \begin{itemize}
            \item $A^T\in\GL_n(\mathfrak{K})$, and
            \item $\left(A^T\right)^{-1}=\left(A^{-1}\right)^T$.
        \end{itemize}
    \end{property}

    \newdef{Trace}{\index{trace}\label{linalgebra:trace}
        Let $A\equiv(a_{ij})\in M_n(\mathfrak{K})$. The trace of $A$ is defined as follows:
        \begin{gather}
            \tr(A) := \sum_{i=1}^na_{ii}\,.
        \end{gather}
    }
    \begin{property}\label{linalgebra:trace_commutative}
        Let $A,B\in M_n(\mathfrak{K})$. The trace satisfies the following properties:
        \begin{itemize}
            \item $\tr:M_n(\mathfrak{K})\rightarrow k$ is a linear map,
            \item $\tr(AB) = \tr(BA)$, and
            \item $\tr(A^T) = \tr(A)$.
        \end{itemize}
    \end{property}

    \newformula{Hilbert--Schmidt norm}{\index{Frobenius!norm|see{Hilbert--Schmidt}}\index{Hilbert--Schmidt!norm}\label{linalgebra:hilbert_schmidt_norm}
        The Hilbert--Schmidt (or \textbf{Frobenius}) norm is defined by the following formula:
        \begin{gather}
            \|A\|^2_{HS} := \sum_{i,j}|A_{ij}|^2 = \tr\left(A^\dag A\right)\,.
        \end{gather}
        If one identifies $M_{n}(\mathbb{C})$ with $\mathbb{C}^{2n}$, this norm equals the standard Hermitian norm.
    }

    \newformula{Hadamard product}{\index{Hadamard!product}\label{linalgebra:hadamard_product}
        The Hadamard product of two matrices is defined as the entrywise product:
        \begin{gather}
            (A\circ B)_{ij} := A_{ij}B_{ij}\,.
        \end{gather}
    }

    \begin{property}\label{linalgebra:dim_columns_rows}
        Let $A\in M_{m,n}(\mathfrak{K})$. Denote the set of columns as $\{A_1,A_2,\ldots,A_n\}$ and the set of rows as $\{R_1,R_2,\ldots,R_m\}$. The set of columns is a subspace of $\mathfrak{K}^m$ and the set of rows is a subspace of $\mathfrak{K}^n$. Their spans satisfy the following property:
        \begin{gather}
            \dim\bigl(\mathrm{span}(A_1,\ldots,A_n)\bigr) = \dim\bigl(\mathrm{span}(R_1,\ldots,R_m)\bigr)\,.
        \end{gather}
    \end{property}
    \newdef{Rank}{\index{rank!of a matrix}\label{linalgebra:matrix_rank}
        Using the invariance relation from the previous property, one can define the rank of a matrix $A\in M_{m,n}(\mathfrak{K})$ as follows:
        \begin{gather}
            \rk(A) := \dim\bigl(\mathrm{span}(A_1,\ldots,A_n)\bigr) = \dim\bigl(\mathrm{span}(R_1,\ldots,R_m)\bigr)\,.
        \end{gather}
    }

    \begin{property}\label{linalgebra:rank_properties}
        Let $A\in M_{m,n}(\mathfrak{K}),B\in\GL_n(\mathfrak{K}),C\in M_{n,r}(\mathfrak{K})$ and $D\in M_{r,n}(\mathfrak{K})$. The ranks of these matrices satisfy the following properties:
        \begin{itemize}
            \item $\rk(AC)\leq\rk(A)$,
            \item $\rk(AC)\leq\rk(C)$,
            \item $\rk(BC)=\rk(C)$, and
            \item $\rk(DB)=\rk(D)$.
        \end{itemize}
    \end{property}
    \begin{property}\label{linalgebra:matrix_left_multiplication}
        Let $A\in M_{m,n}(\mathfrak{K})$. The linear map
        \begin{gather}
            L_A:\mathfrak{K}^n\rightarrow \mathfrak{K}^m:v\mapsto Av
        \end{gather}
        satisfies $\im(L_A)=\mathrm{span}(A_1,\ldots,A_n)$.
    \end{property}

\subsection{System of equations}\label{section:system_of_equations}

    \begin{property}\label{linalgebra:matrix_and_equations}
        Let $Ax=b$ with $A\in M_{m,n}(\mathfrak{K}),x\in\mathfrak{K}^n$ and $b\in\mathfrak{K}^m$ be a system of $m\in\mathbb{N}$ equations in $n\in\mathbb{N}$ variables and let $L_A$ be the linear map as defined in \cref{linalgebra:matrix_left_multiplication}. The following properties hold:
        \begin{itemize}
            \item The system is inconsistent if and only if $b\not\in\im(L_A)$.
            \item If the system is not inconsistent, the solution set is an affine space. If $x_0\in\mathfrak{K}^n$ is a solution, the solution set is given by: $x_0+\ker(L_A)$.
            \item If the system is homogeneous, i.e.~$b=0$, the solution set is equal to $\ker(L_A)$.
        \end{itemize}
    \end{property}
    \begin{property}[Uniqueness]\label{linalgebra:rank_unique_solution}
        Let $Ax=b$ with $A\in M_n(\mathfrak{K})$ be a system of $n\in\mathbb{N}$ equations in $n$ variables. If $\rk(A)=n$, the system has a unique solution.
    \end{property}

    \newformula{Cramer's rule}{\index{Cramer's rule}\label{linalgebra:cramers_rule}
        Let $Ax=b$ be a system of linear equations where the matrix $A$ has a nonzero determinant. There exists a unique solution:
        \begin{gather}
            x_i = \frac{\det(A_i)}{\det(A)}\,,
        \end{gather}
        where $A_i$ is the matrix obtained by replacing the $i^{\text{th}}$ column of $A$ by the column vector $b$.
    }

\subsection{Coordinates and matrix representations}

    \newdef{Coordinate vector}{\index{coordinate}\label{linalgebra:coordinate_vector}
        Let $\mathcal{B}=\{b_1,\ldots,b_n\}$ be a basis of $V$ and consider the vector $v=\sum_{i=1}^n\lambda_ib_i$. The coordinate vector of $v$ with respect to $\mathcal{B}$ is defined as the column vector $(\lambda_1,\ldots,\lambda_n)^T$. The scalars $\lambda_i$ are called the \textbf{coordinates} of $v$ with respect to $\mathcal{B}$.
    }
    \newdef{Coordinate isomorphism}{\label{linalgebra:coordinate_isomorphism}
        With the previous definition in mind, one can define the coordinate isomorphism induced by $\mathcal{B}$ as follows:
        \begin{gather}
            \beta:V\rightarrow\mathfrak{K}^n:\sum_{i=1}^n\lambda_ib_i\mapsto(\lambda_1,\ldots,\lambda_n)^T\,.
        \end{gather}
    }

    \begin{construct}[Matrix representation]\label{linalgebra:matrix_representation}
        Let $V,W$ be $m$- and $n$-dimensional vector spaces with bases $\mathcal{B}=\{b_1,\ldots,b_m\},\mathcal{C}=\{c_1,\ldots,c_n\}$ and consider a linear map $f:V\rightarrow W$. The matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$ is defined as the matrix $A_{f,\mathcal{B},\mathcal{C}}$ that satisfies the following condition for all vectors $v\in V$. Let $(\lambda_1,\ldots,\lambda_n)^T$ be the coordinate vector of $v$ with respect to $\mathcal{B}$ and let $(\mu_1,\ldots,\mu_m)^T$ be the coordinate vector of $f(v)$ with respect to $\mathcal{C}$, then
        \begin{gather}
            \label{linalgebra:matrix_representation_equation}
            \begin{pmatrix}
                \mu_1\\\vdots\\\mu_m
            \end{pmatrix}
            = A_{f,\mathcal{B},\mathcal{C}}
            \begin{pmatrix}
                \lambda_1\\\vdots\\\lambda_n
            \end{pmatrix}\,.
        \end{gather}
        This matrix can be constructed as follows. For every $j\in\{1,\ldots,m\}$, write $f(b_j)=\sum_{i=1}^na_{ij}c_i$. The matrix $A_{f,\mathcal{B},\mathcal{C}}\equiv(a_{ij})\in M_{n,m}(\mathfrak{K})$ is called the matrix representation of $f$. The $j^{\text{th}}$ column of $A_{f,\mathcal{B},\mathcal{C}}$ coincides with the coordinate vector of $f(b_j)$ with respect to $\mathcal{C}$.
    \end{construct}

    The following property shows that the matrix algebra $M_{m,n}(\mathfrak{K})$ is isomorphic to the algebra\footnote{The multiplication is given by the composition of linear maps.} of linear maps $\mathcal{L}(\mathfrak{K}^n,\mathfrak{K}^m)$, thereby explaining why the same notation for the space of invertible matrices (\cref{linalgebra:GL_matrices}) and the space of automorphisms (\cref{linalgebra:automorphism}) was used.
    \begin{property}[Matrices and linear maps]\label{linalgebra:map_matrix_relation}
        For every matrix $A\in M_{m,n}(\mathfrak{K})$, there exists a linear map $f:\mathfrak{K}^n\rightarrow \mathfrak{K}^m$ such that $A_{f,\mathcal{B},\mathcal{C}}=A$. Conversely, for every linear map $f:\mathfrak{K}^m\rightarrow \mathfrak{K}^n$ there exists a matrix $A\in M_{n,m}(\mathfrak{K})$ such that $f=L_A$ (given by the previous construction).
    \end{property}
    \begin{result}\label{linalgebra:matrix_invertible_map}
        Let $f\in\End(V)$ and let $A_f$ be the corresponding matrix representation. The linear map $f$ is invertible if and only if $A_f$ is invertible. Furthermore, if $A_f$ is invertible,
        \begin{gather}
            \left(A_f\right)^{-1} = A_{f^{-1}}\,.
        \end{gather}
        In other words, the linear isomorphism $\End(V)\rightarrow M_n(\mathfrak{K})$ descends to a group isomorphism
        \begin{gather}
            \GL(V)\rightarrow\GL_n(\mathfrak{K}):f\mapsto A_f\,,
        \end{gather}
        where $n=\dim(V)$.
    \end{result}

    \begin{formula}[Linear forms]
        Let $V\cong\mathfrak{K}^n$ and consider a linear form $f\in V^*$. \Cref{linalgebra:matrix_representation_equation} can be rewritten as
        \begin{gather}
            f\left((\lambda_1,\ldots,\lambda_n)^T\right) = (f(e_1), \ldots, f(e_n))(\lambda_1,\ldots,\lambda_n)^T = \sum_{i=1}^nf(e_i)\lambda_i\,,
        \end{gather}
        where $\{e_i\}_{i\in I}$ is the standard basis of $k^n$. In terms of the standard dual basis $\{\varepsilon_1,\ldots,\varepsilon_n\}$, this becomes:
        \begin{gather}
            \label{linalgebra:map_in_function_of_dual_basis}
            f = \sum_{i=1}^nf(e_i)\varepsilon_i\,.
        \end{gather}
    \end{formula}

    \begin{property}[Transpose]
        Let $f:V\rightarrow W$ be a linear map and let $f^*:W^*\rightarrow V^*$ be the corresponding dual map. If $A_f$ is the matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$, the transpose $A_f^T$ is the matrix representation of $f^*$ with respect to the dual basis of $\mathcal{C}$ and the dual basis of $\mathcal{B}$.
    \end{property}
    \begin{result}\index{adjoint!Hermitian}
        The Hermitian adjoint of a linear map (\cref{linalgebra:adjoint_operator}) induces the (Hermitian) adjoint of matrices $A\in\mathbb{C}^{m\times n}$. It is given by
        \begin{gather}
            A^\dag = \overline{A}^T\,,
        \end{gather}
        where $\overline{A}$ denotes the complex conjugate of $A$.
    \end{result}

\subsection{Coordinate transformations}

    \newdef{Transition matrix}{\label{linalgebra:transition_matrix}
        Let $\mathcal{B}=\{b_1,\ldots,b_n\}$ and $\mathcal{B}'=\{b_1',\ldots,b_n'\}$ be two bases of $V$. By definition, every element of $\mathcal{B}'$ can be written as a linear combination of elements in $\mathcal{B}$:
        \begin{gather}
            b_j' = q_{1j}b_1 + \cdots + q_{nj}b_n\,.
        \end{gather}
        The matrix $Q\equiv(q_{ij})\in M_n(\mathfrak{K})$ is called the transition matrix from $\mathcal{B}$ to $\mathcal{B}'$.
    }

    \begin{property}\label{linalgebra:transition_matrix_properties}
        Let $\mathcal{B},\mathcal{B}'$ be two bases of $V$ and let $Q$ be the transition matrix from $\mathcal{B}$ to $\mathcal{B}'$. The following statements hold:
        \begin{itemize}
            \item $Q\in\GL_n(\mathfrak{K})$ and $Q^{-1}$ is the transition matrix from $\mathcal{B}'$ to $\mathcal{B}$.
            \item Let $\mathcal{C}$ be an arbitrary basis of $V$ with $\gamma$ the corresponding coordinate isomorphism and define the following matrices:
            \begin{gather}
                B:=(\gamma(b_1),\ldots,\gamma(b_n)) \quad\text{and}\quad B':=(\gamma(b_1'),\ldots,\gamma(b_n'))\,.
            \end{gather}
            In terms of these matrices, one finds that $BQ = B'$.
            \item Consider $v\in V$. Let $(\lambda_1,\ldots,\lambda_n)^T$ be the coordinate vector with respect to $\mathcal{B}$ and let $(\lambda_1',\ldots,\lambda_n')^T$ be the coordinate vector with respect to $\mathcal{B}'$, then
            \begin{gather}
                Q
                \begin{pmatrix}
                    \lambda_1'\\\vdots\\\lambda_n'
                \end{pmatrix}
                =
                \begin{pmatrix}
                    \lambda_1\\\vdots\\\lambda_n
                \end{pmatrix}
                \quad\text{and}\quad
                \begin{pmatrix}
                    \lambda_1'\\\vdots\\\lambda_n'
                \end{pmatrix}
                = Q^{-1}
                \begin{pmatrix}
                    \lambda_1\\\vdots\\\lambda_n
                \end{pmatrix}\,.
            \end{gather}
        \end{itemize}
    \end{property}
    \begin{result}[Basis change]\label{linalgebra:transition_matrix_representation}
        Let $V,W$ be two finite-dimensional vector spaces. Consider two bases $\mathcal{B},\mathcal{B}'$ of $V$ and two bases $\mathcal{C},\mathcal{C}'$ of $W$. Let $Q,P$ be the transition matrices from $\mathcal{B}$ to $\mathcal{B}'$ and from $\mathcal{C}$ to $\mathcal{C}'$, respectively. The matrix representations $A=A_{f,\mathcal{B},\mathcal{C}}$ and $A'=A_{f,\mathcal{B}',\mathcal{C}'}$ of a linear map $f:V\rightarrow W$ are related in the following way:
        \begin{gather}
            A' = P^{-1}AQ\,.
        \end{gather}
    \end{result}

    \begin{remark}
        From the definition of the transition matrix and the above property, it follows that the basis vectors and coordinate representations transform by $Q$ and $Q^{-1}$ respectively. That they transform in an inverse manner makes sense, since a vector should be independent from its coordinate representation:
        \begin{gather}
            v'=\sum_{i=1}^n\lambda_i'e_i'=\sum_{i,j,k=1}^nQ^{-1}_{ji}\lambda_jQ_{ik}e_k = \sum_{i,j,k=1}^n\delta_{jk}\lambda_je_k = v\,.
        \end{gather}
    \end{remark}
    This remark gives a new way to define a vector $v\in V$.
    \newadef{Vector}{\index{vector}\label{linalgebra:vector_alternative}
        Consider an $n$-dimensional vector space $V$. One can define an equivalence relation on the set $\mathfrak{K}^n\times FV$, where $FV$ denotes the set of all bases of $V$, by saying that the pairs $(c,\mathfrak{b})$ and $(c',\mathfrak{b}')$ are equivalent if and only if there exists a matrix $A\in\GL_n(\mathfrak{K})$ such that $c'=Ac$ and $\mathfrak{b}=A\mathfrak{b}'$. A vector $v\in V$ is then defined as an equivalence class of such pairs.
    }

    \newdef{Matrix conjugation}{\index{conjugacy class}\index{matrix!conjugation}\label{linalgebra:conjugacy_class}
        Let $A\in M_n(\mathfrak{K})$. The set
        \begin{gather}
            \bigl\{Q^{-1}AQ\bigm\vert Q\in\GL_n(\mathfrak{K})\bigr\}
        \end{gather}
        is called the conjugacy class of $A$ in accordance with group theory (\cref{group:normal_subgroup}). Another term for conjugation is \textbf{similarity transformation}.
    }
    \begin{remark}
        If $A$ is a matrix representation of a linear operator $f$, the conjugacy class of $A$ consists of all matrix representations of $f$.
    \end{remark}

    \begin{property}[Trace]\label{linalgebra:trace_invariance}
        \Cref{linalgebra:trace_commutative} implies that the trace of a matrix is invariant under conjugation:
        \begin{gather}
            \tr(Q^{-1}AQ) = \tr(A)\,.
        \end{gather}
    \end{property}

    \newdef{Matrix congruence}{\index{congruence}\label{linalgebra:matrix_congruence}
        Let $A,B\in M_n(\mathfrak{K})$. The matrices are said to be congruent if there exists a matrix $P$ such that
        \begin{gather}
            A = P^TBP\,.
        \end{gather}
    }
    \begin{property}
        Every matrix congruent to a symmetric matrix is also symmetric.
    \end{property}

    \begin{property}[Orthogonality of basis changes]\label{linalgebra:orthogonal_transition_matrix}
        Let $V$ be an inner product space and let $\mathcal{B},\mathcal{B}'$ be two orthonormal bases of $V$ with transition matrix $Q$. $Q$ is \textit{orthogonal} (see \cref{linalgebra:orthogonal_group}):
        \begin{gather}
            Q^TQ = \mathbbm{1}_n\,.
        \end{gather}
    \end{property}

\subsection{Determinant}

    \newdef{Minor}{\index{minor}
        The $(i,j)$-th minor of $A$ is defined as $\det(A_{ij})$ where $A_{ij}\in M_{n-1}(\mathfrak{K})$ is the matrix obtained by removing the $i^{\text{th}}$ row and the $j^{\text{th}}$ column from $A$.
    }
    \newdef{Cofactor}{\index{co-!factor}
        The cofactor $\alpha_{ij}$ of the matrix element $a_{ij}$ is defined as $(-1)^{i+j}\det(A_{ij})$.
    }
    \newdef{Adjugate matrix}{\index{adjugate matrix}\label{linalgebra:adjugate_matrix}
        The adjugate matrix of $A\in M_n(\mathfrak{K})$ is defined as follows:
        \begin{gather}
            \mathrm{adj}(A):=
            \begin{pmatrix}
                \alpha_{11}&\alpha_{21}&\dotsm&\alpha_{n1}\\
                \alpha_{12}&\alpha_{22}&\dotsm&\alpha_{n2}\\
                \vdots&\vdots&\ddots&\vdots\\
                \alpha_{1n}&\alpha_{2n}&\dotsm&\alpha_{nn}\\
            \end{pmatrix}\,,
        \end{gather}
        or, in terms of the cofactors: $\mathrm{adj}(A) = (\alpha_{ij})^T$, where the transpose is taken after the elements have been replaced by their cofactor.
    }

    \begin{formula}[Laplace]\index{Laplace!determinant formula}\label{linalgebra:laplace_formula}
        The determinant of a matrix $A\equiv(a_{ij})\in M_n(\mathfrak{K})$ can be evaluated as follows for any $l\leq n$:
        \begin{gather}
            \det(A) = \sum_{i=1}^n(-1)^{i+l}a_{il}\det(A_{il})\,.
        \end{gather}
    \end{formula}
    \begin{property}\label{linalgebra:determinant_properties}
        Let $A,B\in M_n(\mathfrak{K})$ and denote the columns of $A$ by $A_1,\ldots,A_n$. The determinant has the following properties:
        \begin{itemize}
            \item $\det(AB) = \det(A)\det(B)$,
            \item $\det(A^T) = \det(A)$,
            \item $\det(A_1,\dotso, A_i+\lambda A_i',\dotso,A_n) = \det(A_1,\dotso,A_i,\dotso,A_n) + \lambda\det(A_1,\dotso,A_i',\dotso,A_n)$ for all $A_i,A_i'\in M_{n,1}(\mathfrak{K})$, and
            \item $\det(A_{\sigma(1)},\dotso,A_{\sigma(n)}) = \sgn(\sigma)\det(A_1,\dotso,A_n)$.
        \end{itemize}
        Items 2, 3 and 4 further imply that a matrix with two identical rows or columns has a vanishing determinant.
    \end{property}

    \begin{property}\label{linalgebra:theorem:rank_det_equivalence}
        Let $A\in M_n(\mathfrak{K})$. The following statements are equivalent:
        \begin{itemize}
            \item $\det(A)\neq 0$,
            \item $\rk(A) = n$, or
            \item $A\in\GL_n(\mathfrak{K})$.
        \end{itemize}
    \end{property}
    \begin{property}\label{linalgebra:adjugate_matrix_determinant}
        For all $A\in M_n(\mathfrak{K})$, one finds that
        \begin{gather}
            A\,\mathrm{adj}(A) = \mathrm{adj}(A)A = \det(A)I_n\,.
        \end{gather}
    \end{property}
    \begin{result}\label{linalgebra:determinant_inverse}
        For all $A\in\GL_n(\mathfrak{K})$, one finds that
        \begin{gather}
            A^{-1} = \det(A)^{-1}\,\mathrm{adj}(A)\,.
        \end{gather}
    \end{result}

    \begin{adefinition}[Minor]\index{minor}
        Let $A\in M_{m,n}(\mathfrak{K})$ and choose $l\leq\min(m,n)$. A $l\times l$-minor of $A$ is the determinant of a $l\times l$-partial matrix obtained by removing $m-l$ rows and $n-l$ columns from $A$.
    \end{adefinition}
    \begin{property}
        Let $A\in M_{m,n}(\mathfrak{K})$ and choose $l\leq\min(m,n)$. Then $\rk(A)\geq l$ if and only if $A$ contains a nonzero $l\times l$-minor.
    \end{property}

    \begin{property}[Invariance of determinant]
        Let $f\in\End(V)$. The determinant of the matrix representation of $f$ is invariant under basis transformations.
    \end{property}
    \newdef{Determinant of a linear map}{\index{determinant}\label{linalgebra:operator_determinant}
        The previous property allows for an unambiguous definition of the determinant of $f\in\End(V)$:
        \begin{gather}
            \det(f) := \det(A)
        \end{gather}
        for any matrix representation $A$ of $f$.
    }

\subsection{Characteristic polynomial}

    \newdef{Characteristic polynomial}{\index{characteristic!polynomial}\index{characteristic!equation}\label{linalgebra:characteristic_polynomial}
        Consider a linear map $f\in\End(V)$ and denote its matrix representation by $A_f$. The function
        \begin{gather}
            \chi_f(x) := \det(x\mathbbm{1}_n-A_f)\in\mathfrak{K}[x]
        \end{gather}
        is a monic polynomial of degree $n\in\mathbb{N}$ in the variable $x$. The following equation is called the \textbf{characteristic equation} or \textbf{secular equation} of $f$:
        \begin{gather}
            \label{linalgebra:characteristic_equation}
            \chi_f(x) = 0\,.
        \end{gather}
    }

    \begin{formula}\label{linalgebra:parts_of_characteristic_polynomial}
        Consider a matrix $A\equiv(a_{ij})\in M_n(\mathfrak{K})$ with characteristic polynomial \[\chi_A(x) = x^n + c_{n-1}x^{n-1} + \dotso + c_1x + c_0\,.\] The first and last of the coefficients $c_i$ have a simple expression:
        \begin{gather}
            \begin{cases}
                c_0 = (-1)^n\det(A),\\
                c_{n-1} = -\tr(A)\,.
            \end{cases}
        \end{gather}
    \end{formula}

    \begin{theorem}[Cayley--Hamilton]\index{Cayley--Hamilton}\label{linalgebra:cayley_hamilton}
        Consider a linear map $f\in\End(V)$ with characteristic polynomial $\chi_f$.
        \begin{gather}
            \chi_f(f) = f^n + \sum_{i=1}^{n-1}c_if^i=0\,.
        \end{gather}
    \end{theorem}
    \begin{result}
        From \cref{algebra:minimal_polynomial_divisor} and the Cayley--Hamilton theorem, it follows that the minimal polynomial $\mu_f$ is a divisor of the characteristic polynomial $\chi_f$.
    \end{result}

\subsection{Matrix groups}\label{section:linear_groups}

    \newdef{Elementary matrix}{\index{elementary matrix}\label{linalgebra:elementary_matrix}
        An elementary matrix is a matrix of the form
        \[
            \begin{pmatrix}
                1&0&\dotsm&0\\
                0&1&a&0\\
                \vdots&\vdots&\ddots&\vdots\\
                0&0&\cdots&1
            \end{pmatrix}
            \,,
            \begin{pmatrix}
                1&0&\dotsm&0\\
                0&1&\dotsm&0\\
                \vdots&b&\ddots&\vdots\\
                0&0&\cdots&1
            \end{pmatrix}
            \,,\dotso
        \]
        i.e.~it is equal to the sum of an identity matrix and a multiple of a matrix unit $U_{ij}$. The elementary matrix with the scalar $c$ at position $(i,j)$ is denoted by $E_{ij}(c)$.

        A second type of elementary matrix is one of the form
        \[
            \begin{pmatrix}
                1&0&0&\dotsm&0\\
                0&0&0&a&0\\
                0&0&1&\cdots&0\\
                \vdots&a&\vdots&\ddots&\vdots\\
                0&0&0&\cdots&1
            \end{pmatrix}\,.
        \]
        These matrices are sometimes denoted by $T_{i,j}$.
    }
    \begin{property}[Invertibility]
        Elementary matrices have determinant 1 and, accordingly, are elements of $\GL_n(\mathfrak{K})$.
    \end{property}
    \begin{property}
        Multiplication by an elementary matrix has the following properties:
        \begin{itemize}
            \item Left multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $i^{\text{th}}$ row of the matrix with the $i^{\text{th}}$ row plus $c$ times the $j^{\text{th}}$ row.
            \item Right multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $j^{\text{th}}$ column of the matrix with the $j^{\text{th}}$ column plus $c$ times the $i^{\text{th}}$ column.
            \item Left multiplication by an elementary matrix $T_{i,j}$ interchanges the $i^{\text{th}}$ and $j^{\text{th}}$ rows.
        \end{itemize}
    \end{property}

    \begin{property}\label{linalgebra:elementary_matrices}
        Every invertible matrix can be written as a product of elementary matrices.
    \end{property}

    \newdef{Special linear group}{\label{linalgebra:special_linear_group}
        The subgroup of $\GL_n(\mathfrak{K})$ consisting of all matrices with determinant 1:
        \begin{gather}
            \mathrm{SL}_n(\mathfrak{K}) := \{A\in\GL_n(\mathfrak{K})\mid\det(A) = 1\}\,.
        \end{gather}
    }

    \newdef{Orthogonal group}{\index{orthogonal!group}\label{linalgebra:orthogonal_group}
        The orthogonal and special orthogonal group are defined as follows:
        \begin{gather}
            \begin{aligned}
                \mathrm{O}(n,k) &:= \{A\in\GL_n(\mathfrak{K})\mid AA^T = A^TA = \mathbbm{1}_n\}\,,\\
                \mathrm{SO}(n,k) &:= \mathrm{O}_n(\mathfrak{K})\cap\mathrm{SL}_n(\mathfrak{K})\,.
            \end{aligned}
        \end{gather}
    }

    \newdef{Unitary group}{\label{linalgebra:unitary_group}
        Consider a field $\mathfrak{K}$ equipped with an involution $\sigma:\lambda\mapsto\overline{\lambda}$. The unitary and special unitary group are defined as follows:
        \begin{gather}
            \begin{aligned}
                \mathrm{U}_n(k,\sigma) &:= \{A\in\GL_n(\mathfrak{K})\mid A\sigma(A)^T = \sigma(A)^TA = \mathbbm{1}_n\}\,,\\
                \mathrm{SU}_n(k,\sigma) &:= \mathrm{U}_n(\mathfrak{K})\cap\mathrm{SL}_n(\mathfrak{K})\,.
            \end{aligned}
        \end{gather}
        In practice, $\mathfrak{K}$ is often $\mathbb{C}$ with complex conjugation as the involution. For this reason the notation $A^\dag:=\sigma(A)^T$ is common. Moreover, in the case $\mathfrak{K}=\mathbb{C}$, the notation is further simplified to $\mathrm{U}(n)$ and $\mathrm{SU}(n)$.
    }

    \newdef{Unitary equivalence}{\index{equivalence!unitary}
        Let $A,B\in M_n(\mathfrak{K})$ over a field $\mathfrak{K}$ with an involution. The matrices are said to be unitarily equivalent if there exists a unitary matrix $U$ such that \[A = U^\dag BU\,.\]
    }
    \begin{property}\label{linalgebra:con_equivalence}
        For orthogonal matrices, conjugacy (\cref{linalgebra:conjugacy_class}) and congruency (\cref{linalgebra:matrix_congruence}) coincide. More generally, for unitary matrices, conjugacy and unitary equivalence coincide.
    \end{property}

    \newdef{Symplectic group}{\index{symplectic!group}\label{linalgebra:symplectic_group}
        Consider a vector space $V$ with an antisymmetric nonsingular matrix $\Omega$. The symplectic group $\mathrm{Sp}(V,\Omega)$ is defined as follows:
        \begin{gather}
            \mathrm{Sp}(V,\Omega) := \{A\in\GL(V)\mid A^T\Omega A = \Omega\}\,.
        \end{gather}
        Over the real or complex numbers one can define the canonical \textbf{symplectic} matrix
        \begin{gather}
            \Omega_{st} :=
            \begin{pmatrix}
                0&-\mathbbm{1}\\
                \mathbbm{1}&0
            \end{pmatrix}\,.
        \end{gather}
        The groups of matrices that preserve this matrix are denoted by $\mathrm{Sp}(n,\mathbb{R})$ and $\mathrm{Sp}(n,\mathbb{C})$.
    }
    \begin{remark}
        Symplectic groups can only be defined on even-dimensional spaces because antisymmetric matrices can only be nonsingular if the dimension $n$ is even.
    \end{remark}
    \newdef{Compact symplectic group}{\index{quaternions}
        The compact symplectic group is defined as follows (although the notation is confusing, it is standard):
        \begin{gather}
            \mathrm{Sp}(n) := \mathrm{Sp}(2n,\mathbb{C})\cap\mathrm{U}(2n)\,.
        \end{gather}
        This group is in fact isomorphic to the \textit{quaternionic} unitary group in $n$ quaternionic dimensions.
    }
    \begin{property}
        \[\mathrm{Sp}(1)\cong\mathrm{SU}(2)\]
    \end{property}

\subsection{Matrix decompositions}

    \begin{method}[QR Decomposition]\index{QR!decompositon}
        Every square complex matrix $M$ can be decomposed as
        \begin{gather}
            M = QR
        \end{gather}
        with $Q$ unitary and $R$ upper triangular. The easiest way to achieve this decomposition is by applying the Gram--Schmidt orthonormalization process:
        \begin{quote}
            Let $\{v_i\}_{i\leq n}$ be a basis for the column space of $M$. By applying the Gram--Schmidt process to this basis one obtains a new orthonormal basis $\{e_i\}_{i\leq n}$. The matrix $M$ can then be written as a product $QR$ of the following matrices:
            \begin{itemize}
                \item an upper-triangular matrix $R$ with entries $R_{ij} = \langle e_i\mid\mathrm{col}_j(M) \rangle$, where $\mathrm{col}_j(M)$ denotes the $j^{\text{th}}$ column of $M$.
                \item a unitary matrix $Q=(e_1,\ldots,e_n)$ constructed by setting the $i^{\text{th}}$ column equal to the $i^{\text{th}}$ basis vector $e_i$.
            \end{itemize}
        \end{quote}
    \end{method}
    \begin{property}
        If $M$ is invertible and if the diagonal elements of $R$ are required to have positive norm, the QR-decomposition is unique.
    \end{property}

    \todo{COMPLETE (Cholesky, polar, ...)}
\chapter{Vector \& Tensor Calculus}

    References for this chapter are~\citet{jeevanjee_introduction_2015,choquet-bruhat_analysis_1991}. For a more geometric approach to some of the concepts and results in this chapter, see \cref{chapter:curves_surfaces}, \cref{chapter:vector_bundles} and \cref{section:integration_manifolds} (e.g.~\cref{bundle:vector_calculus}).

    \begin{remark*}\index{vector field}
        In this chapter, a \textbf{vector field} will mean a vector-valued function $\vector{A}:\mathbb{R}^n\rightarrow\mathbb{R}^n$ with smooth projections.
    \end{remark*}

    \minitoc

\section{Nabla-operator}\label{section:nabla}

    \newdef{Gradient}{\index{gradient}\label{vector:gradient}
        Let $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$ be a smooth function.
        \begin{gather}
            \nabla\varphi := \left(\pderiv{\varphi}{x^1},\ldots,\pderiv{\varphi}{x^n}\right)
        \end{gather}
    }
    \begin{property}\label{vector:normal_vector}
        The gradient of a smooth real-valued function is perpendicular to its level sets (\cref{set:level_set}).
    \end{property}

    \newdef{Directional derivative}{\index{directional derivative}\label{vector:directional_derivative}
        Consider a smooth function $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$ and let $\hat{a}$ be a unit vector. The directional derivative $\nabla_{\hat{a}}\varphi$ is defined as the change of the function $\varphi$ in the direction of $\hat{a}$:
        \begin{gather}
            \nabla_{\hat{a}}\varphi := (\hat{a}\cdot\nabla)\varphi\,.
        \end{gather}
    }
    \begin{example}
        Let $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$ be a smooth function and let $\deriv{\vector{r}}{s}$ denote the tangent vector to a curve $\vector{r}(s)$ with \textit{natural parameter} (see \cref{diff:natural_parameter}). The variation of $\varphi$ along $\vector{r}(s)$ is given by
        \begin{gather}
            \pderiv{\varphi}{s} = \deriv{\vector{r}}{s}\cdot\nabla\varphi\,.
        \end{gather}
    \end{example}

    \newdef{Conservative vector field}{\index{conservative!vector field}
        A vector field that can be expressed as the gradient of a scalar function.
    }

    \newdef{Gradient of a tensor}{\index{gradient}
        Let $T$ be a tensor field on $\mathbb{R}^n$ and let $\vector{e}_i$ be the coordinate basis. The gradient of $T$ is defined as follows:
        \begin{gather}
            \nabla T := \sum_{i=1}^n\pderiv{T}{x^i}\otimes\vector{e}_i\,.
        \end{gather}
    }

    \newdef{Divergence}{\index{divergence}\label{vector:divergence}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^n$.
        \begin{gather}
            \nabla\cdot\vector{A} := \sum_{i=1}^n\pderiv{A_i}{x^i}
        \end{gather}
    }
    \newdef{Solenoidal vector field}{\index{solenoidal}
        A vector field $\vector{A}$ that satisfies
        \begin{gather}
            \nabla\cdot\vector{A}=0\,.
        \end{gather}
        Such a vector field is also said to be \textbf{divergence-free} because of \cref{vector:divergence_of_rotor} below.
    }

    \newdef{Rotor / curl}{\index{curl}\index{rotor|see{curl}}\label{vector:rotor}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \nabla\times\vector{A} := \left(\pderiv{A_z}{y} - \pderiv{A_y}{z}, \pderiv{A_x}{z} - \pderiv{A_z}{x}, \pderiv{A_y}{x} - \pderiv{A_x}{y}\right)
        \end{gather}
    }

    \newdef{Irrotational vector field}{\index{irrotational}
        A vector field $\vector{A}$ that satisfies
        \begin{gather}
            \nabla\times\vector{A} = 0\,.
        \end{gather}
    }

    \newdef{Laplacian}{\index{Laplace!operator}
        Let $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$ be a smooth function.
        \begin{gather}
            \label{vector:laplacian}
            \Delta\varphi := \nabla^2\varphi = \mpderiv{2}{\varphi}{x} + \mpderiv{2}{\varphi}{y} + \mpderiv{2}{\varphi}{z}
        \end{gather}
        For a vector field $\vector{A}$ on $\mathbb{R}^3$ one can define a \textbf{vector Laplacian}:
        \begin{align}
            \label{vector:vector_laplacian}
            \Delta\vector{A}&:=\nabla^2\vector{A} = \nabla\left(\nabla\cdot\vector{A}\right) - \nabla\times \left(\nabla\times\vector{A}\right)\\
            &\phantom{:}= \left(\Delta A_x,\Delta A_y,\Delta A_z\right)\,.
        \end{align}
    }

    \begin{property}[Mixed properties]\label{vector:mixed_properties}
        The differential operators introduced above satisfy the following identities:
        \begin{align}
            \label{vector:rotor_of_gradient}
            \nabla\times\left(\nabla\varphi\right) &= 0\,,\\
            \label{vector:divergence_of_rotor}
            \nabla\cdot\left(\nabla\times\vector{A}\right) &= 0\,.
        \end{align}
    \end{property}
    \begin{result}
        All conservative vector fields (on $\mathbb{R}^3$) are irrotational. However, the converse is only true if the domain is simply-connected (\cref{topology:simply_connected}). All of this is formalized by \textit{Poincar\'e lemma}~\ref{bundle:poincare}.
    \end{result}

    \newformula{Helmholtz decomposition}{\index{Helmholtz!decomposition}\label{vector:helmholtz_decomposition}
        If $\vector{A}$ is a vector field on $\mathbb{R}^3$ that decays faster than $1/r$ when $r\longrightarrow+\infty$, it can be written as
        \begin{gather}
            \vector{A} = \nabla\times\vector{B} + \nabla\varphi
        \end{gather}
        for some smooth vector field $\vector{B}$ and smooth function $\varphi$.
    }

    The differential operators introduced above can also be generalized to curvilinear coordinates. To this end one needs the \textit{scale factors} as formally defined in \cref{diff:scale_factor}. For the remainder of this section the Einstein summation convention will not be used to make everything as explicit as possible.
    \newformula{Unit vectors}{
        \begin{gather}
            \pderiv{\vector{r}}{q^i} = h_i\hat{e}_i
        \end{gather}
    }
    \newformula{Gradient}{\index{gradient}
        \begin{gather}
            \nabla\varphi = \sum_{i=1}^n\frac{1}{h_i}\pderiv{\varphi}{q^i}\hat{e}_i
        \end{gather}
    }
    \newformula{Divergence}{\index{divergence}
        \begin{gather}
            \nabla\cdot\vector{A} = \frac{1}{\prod_{i=1}^nh_i}\sum_{i=1}^n\left(\pderiv{A_i\prod_{j\neq i}h_j}{q^i}\right)
        \end{gather}
    }
    \newformula{Rotor}{\index{rotor}
        \begin{gather}
            \left(\nabla\times\vector{A}\right)_i = \sum_{j,k=1}^3\frac{\varepsilon_{ijk}}{h_jh_k}\left(\pderiv{}{q^j}(A_kh_k) - \pderiv{}{q^k}(A_jh_j)\right)\,,
        \end{gather}
        where $\varepsilon_{ijk}$ is the 3-dimensional \textit{Levi-Civita symbol} (see \cref{vector:levi_civita_symbol}).
    }

    \newformula{Laplacian in curvilinear coordinates}{\label{vector:laplace_operator}
        In general the Laplace operator is defined as
        \begin{gather}
            \Delta f := \nabla\cdot\nabla f\,.
        \end{gather}
        The Laplacian can also be expressed in different coordinate systems:
        \begin{itemize}
            \item Cylindrical coordinates $(\rho,\phi,z)$:
            \begin{gather}
                \label{vector:cylindrical}
                \frac{1}{\rho}\pderiv{}{\rho}\left(\rho\pderiv{}{\rho}\right) + \frac{1}{\rho^2}\mpderiv{2}{}{\phi} + \mpderiv{2}{}{z}\,.
            \end{gather}
            \item Spherical coordinates $(r,\phi,\theta)$:
            \begin{gather}
                \label{vector:spherical}
                \frac{1}{r^2}\left[\pderiv{}{r}\left(r^2\pderiv{}{r}\right) + \frac{1}{\sin^2\theta}\mpderiv{2}{}{\phi} + \frac{1}{\sin\theta}\pderiv{}{\theta}\left(\sin\theta\pderiv{}{\theta}\right)\right]\,.
            \end{gather}
        \end{itemize}
    }

\section{Integration}
\subsection{Line integrals}\index{line!integral}\index{path!integral|see{line integral}}

    \newformula{Line integral of a continuous function}{\label{vector:line_integral_scalar}
        Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a continuous function and let $\Gamma$ be a piecewise smooth curve $\vector{\varphi}:[a,b]\rightarrow\mathbb{R}^n$. The line integral of $f$ along $\Gamma$ is defined as follows:
        \begin{gather}
            \Int_\Gamma f\,ds := \Int_a^bf(\vector{\varphi}(t))\,\|\vector{\varphi}'(t)\|\,dt\,.
        \end{gather}
    }
    \newformula{Line integral of a continuous vector field}{\label{vector:line_integral_vector}
        Let $\vector{F}$ be a continuous vector field on $\mathbb{R}^n$ and let $\Gamma$ be a piecewise smooth curve $\vector{\varphi}:[a,b]\rightarrow\mathbb{R}^n$. The line integral of $\vector{F}$ along $\Gamma$ is defined as follows:
        \begin{gather}
            \Int_\Gamma\vector{F}\cdot d\vector{s} := \Int_a^b\vector{F}(\vector{\varphi}(t))\cdot\vector{\varphi}'(t)\,dt\,.
        \end{gather}
    }

    \begin{property}[Conservative vector fields]
        A vector field is conservative if and only if its line integral is path-independent, i.e.~if it only depends on the values at the end points. This is a corollary of \textit{Stokes's theorem}~\ref{bundle:stokes_theorem}.
    \end{property}

\subsection[Integral theorems]{Integral theorems\footnotemark}
    \footnotetext{These theorems follow from a more general theorem by Stokes (see \cref{bundle:stokes_theorem}).}

    \begin{theorem}[Fundamental theorem of calculus for line integrals]\index{fundamental theorem!for line integrals}\label{vector:fundamental_theorem}
        Let $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^n$ be a piecewise smooth curve defined on the interval $[a,b]$.
        \begin{gather}
            \Int_\Gamma\nabla f\cdot d\vector{r} = \varphi(\Gamma(b)) - \varphi(\Gamma(a))
        \end{gather}
    \end{theorem}

    \begin{theorem}[Kelvin--Stokes theorem]\index{Kelvin--Stokes}\label{vector:kelvin_stokes_theorem}
        Let $\vector{A}$ be a vector field defined on $\mathbb{R}^3$ and let $S$ be a smooth surface with boundary $\partial S$.
        \begin{gather}
            \Oint_{\partial S}\vector{A}\cdot d\vector{l} = \Iint_S \left(\nabla\times\vector{A}\right)\cdot d\vector{S}
        \end{gather}
    \end{theorem}

    \begin{theorem}[Divergence theorem\footnotemark]\index{divergence!theorem}\index{Gauss--Ostrogradsky}\label{vector:divergence_theorem}
        \footnotetext{Also known as \textbf{Gauss's theorem} or the \textbf{Gauss--Ostrogradsky theorem}.}
        Let $\vector{A}$ be a vector field defined on $\mathbb{R}^n$.
        \begin{gather}
            \Oiint_{\partial V}\vector{A}\cdot d\vector{S} = \Iiint_V \left(\nabla\cdot\vector{A}\right)\,dV
        \end{gather}
    \end{theorem}
    \begin{result}[Green's identity]\index{Green!identity}\label{vector:green_indentity}
        Let $\phi,\psi$ be smooth real-valued functions defined on $\mathbb{R}^3$.
        \begin{gather}
            \Oiint_{\partial V}\left(\psi\nabla\phi - \phi\nabla\psi\right)\cdot d\vector{S} = \Iiint_V \left(\psi\nabla^2\phi - \phi\nabla^2\psi\right)\,dV
        \end{gather}
    \end{result}

\section{Tensors}\label{section:tensors}
\subsection{Tensor product}\index{tensor product!of vector spaces}

    There are two possible (equivalent) ways to introduce the concept of a `tensor' on finite-dimensional vector spaces. One is to interpret tensors as multilinear maps, while the other is to work in a local fashion and express work with the expansion coefficients with respect to a chosen basis.

    \newdef{Tensor product space}{\index{outer!product}\label{vector:tensor_product}
        The tensor product of two finite-dimensional vector spaces $V$ and $W$ is defined as\footnote{`\textit{isomorphic to}' would be better terminology. See the universal property below (\cref{vector:universal_property}).} the set of bilinear maps on the Cartesian product $V^*\times W^*$. Let $v,w$ be vectors in respectively $V$ and $W$ and let $g,h$ be vectors in the corresponding dual spaces. The tensor product of $v$ and $w$ is then defined as follows:
        \begin{gather}
            (v\otimes w)(g,h) := v(g)w(h)\,.
        \end{gather}
        In this incarnation the tensor product is sometimes known as the \textbf{outer product}. Outer products are also frequently called \textbf{pure} or \textbf{simple tensors}.
    }
    \newdef{Tensor component}{
        Let $\mathbf{T}$ be a tensor that takes $r$ vectors and $s$ covectors as input and returns a scalar (element of the underlying field). The components of $\mathbf{T}$ with respect to a frame $\{e_i\}_{i\leq n}$ and a coframe $\{e^i\}_{i\leq n}$ are defined as $T_{i\ldots j}^{\ \ \ \ k\ldots  l} := \mathbf{T}(e_i,\ldots,e_j,e^k,\ldots,e^l)$.
    }

    The above definition can be restated as a universal property (this is also the right way to generalize tensors to infinite-dimensional spaces and avoid the awkward definition involving dual spaces).
    \begin{uproperty}\index{universal!property}\label{vector:universal_property}
        Let $Z$ be a vector space. For every bilinear map $T:V\times W\rightarrow Z$, there exists a unique linear map $f:V\otimes W\rightarrow Z$ such that $T = f\circ\otimes$.
    \end{uproperty}
    \begin{result}
        The tensor product is unique up to linear isomorphisms. This results in the commutativity of the tensor product:
        \begin{gather}
           \label{vector:commutativity}
            V\otimes W \cong W\otimes V\,.
        \end{gather}
    \end{result}

    \begin{notation}[Tensor power]\index{tensor!power}\index{tensor!type}\label{vector:type}
        \begin{gather}
            V^{\otimes n} := \underbrace{V\otimes\cdots\otimes V}_{n\text{ copies}}
        \end{gather}
        More generally, the tensor product of $r$ copies of $V$ and $s$ copies of $V^*$ is denoted by
        \begin{gather}
            \mathcal{T}^r_s(V) = V^{\otimes r}\otimes V^{*\otimes s}\,.
        \end{gather}
        Tensors in this space are said to be of \textbf{type} $(r,s)$.
    \end{notation}
    \newdef{Scalar}{\index{scalar}
        The elements of the underlying field. These are, by definition, the $(0,0)$-tensors.
    }

    \newdef{Tensor algebra}{\index{tensor!algebra}\label{vector:tensor_algebra}
        The tensor algebra over a vector space $V$ is defined as follows:
        \begin{gather}
            T(V) := \bigoplus_{k=0}^{+\infty}V^{\otimes k}\,.
        \end{gather}
    }

    The following remark is strongly related to \cref{linalgebra:dual_space_dimension}.
    \begin{remark}
        For finite-dimensional vector spaces, $\mathcal{T}^1_1V$ is isomorphic to $\End(V)$ and $\mathcal{T}^1_0V$ is isomorphic to $V$ itself. However, when including infinite-dimensional spaces, $\mathcal{T}^1_1V$ is only isomorphic to the endomorphism space $\End(V^*)$ of the dual. This isomorphism is given by the map $\hat{T}:V^*\rightarrow V^*:\omega\mapsto T(-,\omega)$ for every $T\in\mathcal{T}^1_1V$. Dually, in this general setting, the spaces $T^0_1V$ and $V^*$ are also isomorphic.
    \end{remark}

    The tensor product space can also be algebraically defined as follows.
    \begin{adefinition}[Tensor product]
        Consider two vector spaces $V,W$ over a field $K$. First, construct the free vector space $F(V\times W)$ over $K$. Then, construct the subspace $N$ of $F(V\times W)$ spanned by elements of the form
        \begin{itemize}
            \item $(v+v',w) - (v,w) - (v',w)$,
            \item $(v,w+w') - (v,w) - (v,w')$,
            \item $(\lambda v,w) - \lambda(v,w)$, or
            \item $(v,\mu w) - \mu(v,w)$,
        \end{itemize}
        where $v\in V,w\in W$ and $\lambda,\mu\in K$. The tensor product $V\otimes W$ is defined as the quotient $F(V\times W)/N$. It can be shown that this construction is associative, i.e.~$U\otimes(V\otimes W)\cong(U\otimes V)\otimes W$, and as such these brackets will be omitted in all expressions.

        Now, consider the case where $W=V^*$. In this case, the basis of the tensor product $\mathcal{T}^r_s(V)$ will be denoted by
        \[\underbrace{e_i\otimes\cdots\otimes e_j}_{r\text{ basis vector}}\ \ \otimes\underbrace{\varepsilon^k\otimes\cdots\otimes \varepsilon^l_{\textcolor{white}{a}}}_{s\text{ dual basis vectors}}\]
        and the expansion coefficients will be denoted by $T_{i\ldots j}^{\ \ \ \ k\ldots l}$.
    \end{adefinition}

    \newprop{Dimension}{
        From the previous construction, it follows that the dimension of $\mathcal{T}^r_s(V)$ is equal to $rs$.
    }

    For completeness the proof that the values of the tensor operating on $r$ basis vectors and $s$ basis covectors are equal to the corresponding expansion coefficients is given.
    \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
        \begin{proof}
            Consider a general tensor $\mathbf{T} = T_{i\ldots j}^{\ \ \ \ k\ldots l}e_k\otimes\cdots\otimes e_l\otimes\varepsilon^i\otimes\cdots\otimes\varepsilon^j$. Combining \cref{vector:tensor_product} and the pairing of dual vectors~\eqref{linalgebra:dual_basis_2} gives
            \begin{align*}
                \mathbf{T}(\varepsilon^m,\ldots,\varepsilon^n,e_a,\ldots,e_b) &= T_{i\ldots j}^{\ \ \ \ k\ldots l}e_k(\varepsilon^m)\ldots e_l(\varepsilon^n)\varepsilon^i(e_a)\ldots\varepsilon^j(e_b)\\
                &= T_{i\ldots j}^{\ \ \ \ k\ldots l}\delta_k^m\ldots\delta_l^n\delta_a^i\ldots\delta_b^j\\
                &= T_{a\ldots b}^{\ \ \ \ m\ldots n}\,.
            \end{align*}$ $
        \end{proof}
    \end{mdframed}

\subsection{Transformation rules}

    In this section, the behaviour of tensors under basis transformations of the form
    \begin{gather}
        e_i'=A^i_{\ j}\,e_j
    \end{gather}
    is considered.

    \newdef{Contravariant}{\index{contra-!variant}\label{vector:contravariant}
        A tensor component that transforms by the following rule is said to be contravariant:
        \begin{gather}
            v^i = A^i_{\ j}\,v'^j\,.
        \end{gather}
    }
    \newdef{Covariant}{\index{co-!variant}\label{vector:covariant}
        A tensor component that transforms by the following rule is said to be covariant:
        \begin{gather}
            p'_i = A^j_{\ i}\,p_j\,.
        \end{gather}
    }
    \begin{example}[Mixed tensor]
        As an example the transformation rule of a mixed third-order tensor $T\in\mathcal{T}^1_2$ is given:
        \begin{gather}
            T_{\ ij}^k = A^k_{\ w}(A^{-1})^u_{\ i}(A^{-1})^v_{\ j}T_{\ \ uv}'^w\,.
        \end{gather}
    \end{example}

    The following rule is a useful substitute for the `illegal' division of tensors.
    \begin{method}[Quotient rule]
        Consider an equation such as $Q_i^{\ j}A_{jl}^{\ \ k}=B_{il}^{\ \ k}$, where $A$ and $B$ are two known tensors. The quotient rule asserts the following: ``\textit{If the equation holds under all transformations, then $Q$ is a tensor.}'' Note that this rule does not necessarily hold when $B=0$ because transformation rules are not well-defined for the zero tensor.
    \end{method}

\subsection{Tensor operations}

    \newdef{Contraction}{\index{contraction}\label{vector:contraction}
        Let $A$ be a tensor of type $(m,n)$. Taking a subscript and superscript to be equal and summing over all possible values of this index gives a new tensor of type $(m-1,n-1)$. This operation is called the contraction of $A$. It is induced by the evaluation map/pairing (\cref{linalgebra:natural_pairing}).
    }
    \begin{example}
        Let $A$ be a tensor of type $(1,2)$. The contraction over the first and third index can be written using the Einstein conventionas follows:
        \begin{gather}
            A_{\ ij}^j = \sum_{j=1}^nA_{\ ij}^j\,.
        \end{gather}
    \end{example}

    \newdef{Direct product}{\index{direct product}
        The tensor constructed by the componentwise multiplication of two tensors.
    }
    \begin{example}
        Let $A_{\ k}^i$ and $B_{\ lm}^j$ represent two tensors. Their direct product is equal to \[C_{\ k\ lm}^{i\ j} = A_{\ k}^iB_{\ lm}^j\,.\]
    \end{example}
    \begin{example}[Kronecker product]\index{Kronecker!product}\label{vector:operator_product}
        It is also possible to combine operators acting on different vector spaces to make them act on the tensor product space:
        \begin{gather}
            (A\otimes B)(v\otimes w) := Av\otimes Bw\,.
        \end{gather}
        This should not be confused with the Hadamard product (\cref{linalgebra:hadamard_product}).
    \end{example}
    \begin{notation}[Abuse of notation]\label{vector:tensor_abuse}
        Consider an operator $A$ acting on a vector space $V_1$. This operator can be extended to any tensor product space $V_1\otimes V_2$ as $A\otimes\mathbbm{1}$. However, it is often still denoted by $A$.
    \end{notation}

    \begin{notation}[Symmetric part]\index{symmetric!part}
        Consider a second-order tensor $T$ (here taken to be of covariant type for notational simplicity). The symmetric and antisymmetric part of $T$ are often denoted by
        \begin{gather}
            T_{(ij)} := \frac{1}{2}\left(T_{ij} + T_{ji}\right)
        \end{gather}
        and
        \begin{gather}
            T_{[ij]} := \frac{1}{2}\left(T_{ij} - T_{ji}\right)\,.
        \end{gather}
        This notation is easily generalized to other types of tensors.
    \end{notation}

    \begin{formula}[Gradient of tensor products]\index{gradient!of outer product}
        The gradient of an outer product is defined through the Leibniz rule:
        \begin{gather}
            \nabla\cdot(v\otimes w) := (\nabla\cdot v)w+(v\cdot\nabla)w\,.
        \end{gather}
    \end{formula}

    \newdef{Complexification}{\index{complexification}\label{vector:complexification}
        Let $V$ be a real vector space. The complexification of $V$ is defined as follows:
        \begin{gather}
            V^{\mathbb{C}} := V\otimes\mathbb{C}\,.
        \end{gather}
        This space can still be considered a real vector space, but it can also be turned into a complex vector space by generalizing the scalar product:
        \begin{gather}
            \alpha(v\otimes\beta) := v\otimes(\alpha\beta)
        \end{gather}
        for all $\alpha\in\mathbb{C}$.
    }
    \begin{property}\label{vector:complexification_decomposition}
        By (multi)linearity, every element $v_{\mathbb{C}}\in V^{\mathbb{C}}$ can be written as \[v_{\mathbb{C}} = (v_1\otimes1) + i(v_2\otimes 1)\,.\] Therefore, the complexification can be (formally) decomposed as
        \begin{gather}
            V^{\mathbb{C}}\cong V\oplus iV\,.
        \end{gather}
    \end{property}

\section{Exterior algebra}
\subsection{Antisymmetric tensors}

    \newdef{Antisymmetric tensor}{\index{antisymmetry}
        A tensor that changes sign under the interchange of any two indices.
    }
    \begin{notation}[Symmetric tensors]
        The space of symmetric $(n,0)$-tensors is denoted by $S^n(V)$.
    \end{notation}
    \begin{notation}[Antisymmetric tensors]\label{vector:antysimmetric_space}
        The space of antisymmetric $(n,0)$-tensors is denoted by $\Lambda^n(V)$.
    \end{notation}

    \begin{property}
        Let $n=\dim(V)$. The space $\Lambda^r(V)$ equals the zero space for all $r\geq n$.
    \end{property}

\subsection{Determinant}

    \newdef{Form}{\index{form}
        An $n$-form on a vector space $V$ is a totally antisymmetric element of $\mathcal{T}^0_nV$, where $n\leq\dim(V)$.
    }
    \newdef{Volume form}{\index{volume!form}
        A form of rank $\dim(V)$ is also called a \textbf{top form} or \textbf{volume form}.
    }

    \newdef{Determinant}{\index{determinant}
        Consider a finite-dimensional vector space $V$ with basis $\{e_i\}_{i\leq n}$. Let $\varphi$ be a tensor in $\mathcal{T}^1_1V\cong\End(V)$ and let $\omega$ be a volume form on $V$. The determinant of $\varphi$ is defined as follows:
        \begin{gather}
            \det(\varphi) := \frac{\omega\bigl(\varphi(e_1),\ldots,\varphi(e_n)\bigr)}{\omega(e_1,\ldots,e_n)}\,.
        \end{gather}
        This definition is well-defined, i.e.~it is independent of the choice of volume form and basis. Furthermore, it coincides with \cref{linalgebra:operator_determinant}.

        One should note that the determinant is only well-defined for $(1,1)$-tensors. Although other types of tensors can also be represented as matrices, for these the above formula would not be independent of a choice of basis anymore. A more general concept can be defined using the language of \textit{fibre bundles} (see \cref{chapter:principal_bundles}).
    }

\subsection{Levi-Civita symbol}

    \newdef{Levi-Civita symbol}{\index{Levi-Civita!symbol}\label{vector:levi_civita_symbol}
        In $n\in\mathbb{N}$ dimensions, the Levi-Civita symbol is defined as follows:
        \begin{gather}
            \varepsilon_{i_1\ldots i_n} =
            \begin{cases}
                1&\cif(i_1\ldots i_n)\text{ is an even permutation of }(1\ldots n)\,,\\
                -1&\cif(i_1\ldots i_n)\text{ is an odd permutation of }(1\ldots n)\,,\\
                0&\cif\text{any of the indices occurs more than once.}
            \end{cases}
        \end{gather}
    }
    \begin{remark}[Pseudotensor]\label{vector:levi_civita_pseudotensor}
        The Levi-Civita symbol is not a tensor, it is a \textit{pseudotensor}. This means that the sign changes under reflections or, more generally, any transformation with negative determinant. (Given a metric tensor $g$, one can turn it into a proper tensor by multiplying it by $\sqrt{g}$.)
    \end{remark}

    \begin{formula}[Contraction]
        \begin{gather}
            \varepsilon_{i_1,\ldots,i_k}\varepsilon^{j_1,\ldots,j_k}=\delta_{i_1,\ldots,i_k}^{j_1,\ldots,j_k}
        \end{gather}
        and
        \begin{gather}
            \varepsilon_{i_1,\ldots,i_k,i_{k+1},\ldots,i_l}\varepsilon^{i_1,\ldots,i_k,j_{k+1},\ldots,j_l}=k!\delta_{i_{k+1},\ldots,i_l}^{j_{k+1},\ldots,j_l}
        \end{gather}
    \end{formula}

    \newdef{Cross product}{\index{cross!product}\label{vector:cross_product}
        Using the Levi-Civita symbol, one can define the $i^{\text{th}}$ component of the cross product as follows:
        \begin{gather}
            (v\times w)_i = \sum_{j,k=1}^3\varepsilon_{ijk}v_jw_k\,.
        \end{gather}
        The previous remark implies that the cross product is in fact not a vector, instead it is a pseudovector.
    }
    \begin{remark}[Hurwitz theorem]\index{Hurwitz}
        The cross product actually exists in four cases: $\mathbb{R}^0$, $\mathbb{R}^1$, $\mathbb{R}^3$ and $\mathbb{R}^7$. In general, it is characterized by the following conditions:
        \begin{enumerate}
            \item\textbf{Bilinearity}: $(\lambda v)\times(\kappa w) = \lambda\kappa(v\times w)$.
            \item\textbf{Orthogonality}: $v\cdot(v\times w)=0=w\cdot(v\times w)$.
            \item\textbf{Magnitude}: $\|v\times w\|^2 = \|v\|^2\|w\|^2 - (v\cdot w)^2$.
        \end{enumerate}
        These conditions imply that on $\mathbb{R}^1$ the cross product is identically zero. However, on $\mathbb{R}^3$ and $\mathbb{R}^7$ one obtains an anticommutative bilinear operation. On $\mathbb{R}^3$, it is unique, while on $\mathbb{R}^7$ different choices exist.

        This construction is related to the Hurwitz classification theorem~\ref{linalgebra:hurwitz}, since one can construct the cross products on $\mathbb{R}^n$ by embedding it as the imaginary part of the (real, normed) division algebra of dimension $n+1$. The cross product is then obtained from the algebra product after discarding the real component. For example, for $\mathbb{R}^1$ embedded in  $\mathbb{C}$, one obtains a product of two purely imaginary numbers, which is real. Discarding this component gives exactly zero, as mentioned above.
    \end{remark}

    \begin{property}[Exceptional Lie group $G_2$]\index{$G_2$}\index{triple product}
        Using the vector product, one can define an associative 3-form, the \textbf{triple product}:
        \begin{gather}
            u\otimes v\otimes w\mapsto u\cdot(v\times w)\,.
        \end{gather}
        The group of linear isomorphisms that preserve the triple product on $\mathbb{R}^7$ is denoted by $G_2$. By the relation between vector products and division algebras, this group is also the automorphism group of the octonions.
    \end{property}

\subsection{Wedge product}\label{section:wedge_product}

    \newdef{Antisymmetrization}{\label{vector:antisymmetrization}
        Let $S_k$ denote the permutation group on $k\in\mathbb{N}$ elements (\cref{group:permutation_group}). The antisymmetrization operator is defined as follows:
        \begin{gather}
            \Alt(e_1\otimes\cdots\otimes e_k) := \sum_{\sigma\in S_k} \sgn(\sigma)e_{\sigma(1)}\otimes\cdots\otimes e_{\sigma(k)}\,.
        \end{gather}
        Note that many authors introduce a factor $1/k!$. This convention is not adopted here to keep the subsequent constructions clean. If the factor is included, \cref{vector:general_wedge_product} below should be modified.
    }

    \newdef{Wedge product}{\index{wedge!product}\label{vector:wedge_product}
        Let $\{e_i\}_{i\leq \dim(V)}$ be a basis for $V$. The wedge product of basisvectors is defined as follows:
        \begin{gather}
            e_1 \wedge\ldots\wedge e_k := \Alt(e_1\otimes\cdots\otimes e_k)\,.
        \end{gather}
        From this definition, it immediately follows that the wedge product is (totally) antisymmetric.
    }

    \begin{construct}
        Let $\{e_i\}_{i\leq \dim(V)}$ be a basis for $V$. The above definition implies that a basis for $\Lambda^r(V)$ is given by
        \begin{gather}
            \{e_{i_1}\wedge\ldots\wedge e_{i_r}\mid\forall k\leq r: 1\leq i_k \leq \dim(V)\}\,.
        \end{gather}
        Accordingly, the dimension of this space is given by
        \begin{gather}
            \label{vector:wedge_dimension}
            \dim\Lambda^r(V) = \binom{n}{r}\,.
        \end{gather}
        For $r=0$, this construction would be vacuous, so one just defines $\Lambda^0(V) := \mathbb{R}$.
    \end{construct}

    \begin{formula}\label{vector:general_wedge_product}
        Let $v\in\Lambda^r(V)$ and $w\in\Lambda^m(V)$. The wedge product \ref{vector:wedge_product} can be generalized as follows:
        \begin{gather}
            v\wedge w := \frac{1}{r!m!}\Alt(v\otimes w)\,.
        \end{gather}
    \end{formula}

    \newdef{Blade}{\index{blade}\index{pure!vector}
        Elements of $\Lambda^k(V)$ that can be written as the wedge product of $k$ vectors are known as $k$-blades or \textbf{pure $k$-vectors}.
    }

    \begin{formula}[Cross product]\label{vector:wedge_to_cross}
        In dimension 3, there exists an important isomorphism $J:\Lambda^2(\mathbb{R}^3)\rightarrow\mathbb{R}^3$:
        \begin{gather}
            J(\lambda)^i = \frac{1}{2}\varepsilon^i_{\ jk}\lambda^{jk}\,,
        \end{gather}
        where $\lambda\in\Lambda^2(\mathbb{R}^3)$. See also the Hodge $\ast$-operator further below (\cref{vector:hodge_star}).

        Looking at \cref{vector:cross_product} of the cross product, one can see that $v\times w$ is actually the same as $J(v\wedge w)$. One can thus use the wedge product to generalize the cross product to arbitrary dimensions.
    \end{formula}

\subsection{Exterior algebra}

    \newdef{Exterior power}{\index{exterior!power}\index{form}
        In the theory of tensor calculus, the space $\Lambda^k(V)$ is often called the $k^{\text{th}}$ exterior power of $V$. As mentioned before, its elements are called (exterior) $k$-forms.
    }
    \newdef{Exterior algebra}{\index{exterior!algebra}\index{Grassmann!algebra}\label{vector:exterior_algebra}
        One can define a \textit{graded vector space} (see \cref{hda:graded_vector_space}) as follows:
        \begin{gather}
            \Lambda^\bullet(V) := \bigoplus_{k\geq0}\Lambda^k(V)\,.
        \end{gather}
        This graded vector space can be turned into a graded algebra by taking the wedge product as the multiplication:
        \begin{gather}
            \wedge:\Lambda^k(V)\times\Lambda^l(V)\rightarrow \Lambda^{k+l}(V)\,.
        \end{gather}
        This algebra is called the exterior algebra or \textbf{Grassmann algebra} over $V$. Elements of the space $\otimes_{k\in2\mathbb{N}}\Lambda^k(V)$ are said to be \textbf{Grassmann-even} and elements of $\otimes_{k\in2\mathbb{N}+1}\Lambda^k(V)$ are said to be \textbf{Grassmann-odd}.
    }

    \begin{adefinition}\label{vector:adef_exterior_algebra}
        Let $T(V)$ be the tensor algebra (\cref{vector:tensor_algebra}) over the vector space $V$, i.e.
        \begin{gather}
            T(V) = \bigoplus_{k\geq0}V^{\otimes k}\,.
        \end{gather}
        The exterior algebra $\Lambda^\bullet(V)$ over $V$ is defined as the quotient of $T(V)$ by the two-sided ideal $I$ generated by the elements $\{v\otimes v\mid v\in V\}$.
        \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
            \begin{proof}[Proof of equivalence]
                Consider the equality
                \begin{gather*}
                    (u+v)\otimes(u+v) - u\otimes u - v\otimes v = u\otimes v + v\otimes u
                \end{gather*}
                The left-hand side is an element of the ideal $I$ generated by $\{v\otimes v\mid v\in V\}$. Using the ideal generated by elements of the form of the right-hand side gives the usual definition of the exterior algebra based on the wedge product as defined in \cref{vector:wedge_product} because it imposes the relation $u\wedge v = -v\wedge u$.

                However, one should pay attention to one little detail. As mentioned in \cref{vector:adef_exterior_algebra} the general definition uses the ideal $I$ to construct the quotient space. The other construction is only equivalent when working over a field with characteristic different from 2. This follows from the fact that one has to divide by 2 when trying to obtain the ideal $I$ from the right-hand side when setting $u=v$.
            \end{proof}
        \end{mdframed}
    \end{adefinition}

    \begin{property}[Graded-commutativity]
        The exterior algebra is both a unital associative algebra (with identity $1\in K$) and a coalgebra. Furthermore, it is also commutative in the graded sense (see \cref{hda:graded_commutative}).
    \end{property}

    \begin{property}[Nilpotency]
        Graded-commutativity implies that the wedge product of any odd exterior form with itself is identically 0. The wedge product of an even exterior form with itself vanishes if and only if the form can be decomposed as a product of one-forms, i.e.~if it is pure.
    \end{property}

\subsection{Hodge star}

    \Cref{vector:wedge_dimension} says that the spaces $\Lambda^k(V)$ and $\Lambda^{n-k}(V)$ have the same dimension and, hence, that there exists a linear isomorphism between them. Such an isomorphism is given by the Hodge star operator if one restricts to vector spaces equipped with a nondegenerate Hermitian form (\cref{linalgebra:NDH_form}).

    When equipped with an inner product and, hence, an orthonormal basis $\{e_i\}_{i\leq\dim(V)}$, every finite-dimensional vector space admits a canonical volume form given by
    \begin{gather}
        \vol = e_1\wedge\ldots\wedge e_n\,.
    \end{gather}
    This convention will also be adopted in the remainder of this section.

    \newdef{Orientation}{\index{orientation}\label{vector:orientation}
        Let $\vol(V)$ be the volume form on the vector space $V$ as defined above. From the definition of a volume form it follows that every other $\dim(V)$-form is a scalar multiple of $\vol(V)$. Denote this number by $r$. This also implies that a choice of volume form induces an equivalence relation on top-dimensional forms, where equivalence class under this relation are called orientations, as follows: If $r>0$, the orientation is said to be \textbf{positive} and, if $r<0$, the orientation is said to be \textbf{negative}.
    }

    \begin{formula}[Inner product]\index{inner!product}\label{vector:wedge_inner_product}
        Let $V$ be equipped with an inner product $\langle\cdot\mid\cdot\rangle$. One can extend this to an inner product on $\Lambda^k(V)$ by first defining it on blades and extending it by linearity:
        \begin{gather}
            \langle v_1\wedge\ldots\wedge v_k\mid w_1\wedge\ldots\wedge w_k \rangle_k := \det(\langle v_i\mid w_j\rangle)\,.
        \end{gather}
        For an orthogonal basis this formula factorizes as follows:
        \begin{gather}
            \langle v_1\wedge\ldots\wedge v_k\mid w_1\wedge\ldots\wedge w_k \rangle_k = \langle v_1\mid w_1 \rangle\cdots\langle v_k\mid w_k \rangle\,.
        \end{gather}
    \end{formula}
    \newdef{Hodge star}{\index{Hodge!star}\label{vector:hodge_star}
        The Hodge star $\ast:\Lambda^k(V)\rightarrow\Lambda^{n-k}(V)$ is defined as the unique isomorphism such that for all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$, the following equality holds:
        \begin{gather}
            \omega\wedge\rho = \langle\ast\omega\mid\rho\rangle_{n-k}\vol(V)\,,
        \end{gather}
        where $\langle\cdot\mid\cdot\rangle_{n-k}$ is the inner product \eqref{vector:wedge_inner_product} on $\Lambda^{n-k}(V)$. The element $\ast\omega$ is often called the \textbf{(Hodge) dual} of $\omega$.
        \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
            \begin{proof}
                Fix an element $\omega\in\Lambda^k(V)$. For every element $\rho\in\Lambda^{n-k}(V)$ one can see that $\omega\wedge\rho$ is an element of $\Lambda^n(V)$ and as such it is a scalar multiple of $\vol(V)$. This implies that it can be written as
                \begin{gather*}
                    c_\omega(\rho)\vol(V)\,.
                \end{gather*}
                The map $c_\omega:\Lambda^{n-k}(V)\rightarrow\mathbb{R}:\rho\mapsto c_\omega(\rho)$ is a bounded (and thus continuous) linear map, so \textit{Riesz's representation theorem}~\ref{functional:riesz} can be applied to identify $c_\omega$ with a unique element $\ast\omega\in\Lambda^{n-k}(V)$ such that \[c_\omega(\rho) = \langle\ast\omega\mid\rho\rangle_{n-k}\,.\]
            \end{proof}
        \end{mdframed}
    }

    \begin{formula}
        Let $\{e_i\}_{i\leq n}$ be a positively oriented orthonormal basis for $V$. An explicit formula for the Hodge star is given by the following construction. Let $\{i_1,\ldots,i_k\}$ and $\{j_1,\ldots,j_{n-k}\}$ be two ordered complementary index sets and consider an element $\omega\equiv e_{i_1}\wedge\ldots\wedge e_{i_k}\in\Lambda^k(V)$.
        \begin{gather}
            \label{vector:explicit_hodge_star}
            \ast\omega =\sgn(\tau)\prod_{m = 1}^{n-k}\langle e_{j_m}\mid e_{j_m} \rangle e_{j_1}\wedge\ldots\wedge e_{j_{n-k}}\,,
        \end{gather}
        where $\tau$ is the permutation that maps $e_{i_1}\wedge\ldots\wedge e_{i_k}\wedge e_{j_1}\wedge\ldots\wedge e_{j_{n-k}}$ to $\vol(V)$.
    \end{formula}

    Using this formula, one can easily prove the following important property.
    \begin{property}
        Consider an inner product space $V$. The Hodge dual is involutive up to a factor:
        \begin{gather}
            \ast\ast\omega = (-1)^{k(n-k)}\omega\,,
        \end{gather}
        where $\omega\in\Lambda^k(V)$.
    \end{property}

    Taking the definition of the Hodge star operator together with the above property leads to the following formula (which is often found in the literature as the definition of the Hodge dual).
    \begin{formula}
        For all $\omega,\rho\in\Lambda^k(V)$, the Hodge star operator satisfies the following formula:
        \begin{gather}
            \omega\wedge\ast\rho = \langle\omega\mid\rho\rangle\vol(V)\,.
        \end{gather}
    \end{formula}

    \begin{result}\label{vector:hodge_star_vectorcalculus}
        Consider three vectors $u,v,w\in\mathbb{R}^3$.
        \begin{align}
            \ast(v\wedge w) &= v\times w \label{vector:cross_by_hodge_star}\\
            \ast(v\times w) &= v\wedge w\\
            \ast(u\wedge v\wedge w) &= u\cdot(v\times w)
        \end{align}
    \end{result}
    \begin{remark}
        \Cref{vector:wedge_to_cross} is an explicit evaluation of the first equation~\eqref{vector:cross_by_hodge_star}.
        \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
            \begin{proof}
                The signs $\sgn(\sigma)$ in the definition of the wedge product can be written using the Levi-Civita symbol $\varepsilon_{ijk}$ as defined in \cref{vector:levi_civita_symbol}. The factor $\nicefrac{1}{2}$ is introduced to correct for the double counting due to the contraction over both the indices $j$ and $k$.
            \end{proof}
        \end{mdframed}
    \end{remark}

    \newdef{Selfdual form}{\index{dual!selfdual}\label{vector:self_dual_form}
        Let $V$ be a 4-dimensional inner product space and consider an element $\omega\in\Lambda^2(V)$. Then $\omega$ is said to be selfdual if
        \begin{gather}
            \ast\omega = \omega\,.
        \end{gather}
        Every element $\rho\in\Lambda^2(V)$ can be uniquely decomposed as the sum of a selfdual and an antiselfdual two-form:
        \begin{gather}
            \rho \equiv \rho^++\rho^- = \frac{1}{2}\bigl((\rho + \ast\rho) + (\rho - \ast\rho)\bigr)\,.
        \end{gather}
    }

\section{Graded vector spaces}\label{section:graded_spaces}

    \newdef{Graded vector space}{\index{degree!of vector}\index{graded!vector space}\label{hda:graded_vector_space}
        A vector space $V$ that can be decomposed as
        \begin{gather}
            V = \bigoplus_{i\in I}V_i
        \end{gather}
        for a collection of vector spaces $\{V_i\}_{i\in I}$, where $I$ can be both finite or countable. The index $i$ is often called the \textbf{degree} of the subspace $V_i$ in $V$. One writes $\deg(v)=i$ if $v\in V_i$.
    }
    \newdef{Finite type}{\index{finite!type}
        A graded vector space is said to be of finite type if it is finite-dimensional in each degree.
    }

    \begin{example}[Super-vector space]\index{super-!vector space}\label{hda:superspace}
        A $\mathbb{Z}_2$-graded vector space.
    \end{example}

    \newdef{Graded algebra}{\index{graded!algebra}
        A $\mathbb{Z}$-graded vector space $V$ with the additional structure of an algebra $(V,\star)$ such that $V_k\star V_l\subseteq V_{k+l}$ for all $k,l\in\mathbb{Z}$.\footnote{The grading can be relaxed to any commutative monoid.}
    }
    \newdef{Graded-commutative algebra}{\index{graded!commutativity}\label{hda:graded_commutative}
        A graded algebra $(V,\star)$ such that
        \begin{gather}
            v\star w = (-1)^{\deg(v)\deg(w)}w\star v
        \end{gather}
        holds for all homogeneous elements $v,w\in V$.
    }

    \begin{example}[Superalgebra]\index{super-!algebra}\label{hda:superalgebra}
        A $\mathbb{Z}_2$-graded algebra
        \begin{gather}
            A = A_0\oplus A_1\,,
        \end{gather}
        such that for all $i,j\in\{0,1\}$:
        \begin{gather}
            A_i\star A_j \subseteq A_{i+j\bmod2}\,.
        \end{gather}
    \end{example}

    \newdef{Parity and suspension}{\index{parity!functor}\index{suspension}\label{hda:suspension}
        Consider the category $\mathbf{sVect}$ of super-vector spaces. One can define the \textbf{parity functor} $\func{\Pi}{sVect}{sVect}$ as the functor that interchanges even and odd subspaces:
        \begin{gather}
            \begin{aligned}
                (\Pi V)_0 &:= V_1\,,\\
                (\Pi V)_1 &:= V_0\,.
            \end{aligned}
        \end{gather}
        A more general construction exists in $\mathbb{Z}\mathbf{Vect}$. For every graded vector space $V$, the $k$-\textbf{shifted} vector space or $k$-\textbf{suspension} $V[k]$ is defined as follows (some authors use the opposite convention):
        \begin{gather}
            V[k]_i := V_{i-k}\,.
        \end{gather}
    }

    \begin{example}[Free GCA]\index{word!length}\label{hda:symmetric_algebra}
        Let $V$ be a graded vector space. The free GCA $\Sym^\bullet V$ on $V$ is defined as the quotient of the tensor algebra $T(V)$ by the relations
        \begin{gather}
            x\otimes y-(-1)^{\deg(x)\deg(y)}y\otimes x=0
        \end{gather}
        ranging over all homogeneous elements $x,y\in V$. This algebra can equivalently be obtained as the tensor product
        \begin{gather}
            \Sym^\bullet V = \Sym(V_{\text{even}})\otimes\Alt(V_{\text{odd}})\,,
        \end{gather}
        where $\Sym$ and $\Lambda$ denote the symmetric and exterior algebras of ordinary vector spaces. It it not hard to see that this definition combines the definitions of $\Sym$ and $\Alt$. A similar definition gives a graded alternating algebra:
        \begin{gather}
            \Alt^\bullet V = T(V)/\bigl(x\otimes y+(-1)^{\deg(x)\deg(y)}y\otimes x\bigr)\,.
        \end{gather}
        Note that both of these algebras actually carry a bigrading, the total degree coming from $V$ and the \textbf{word length}:
        \begin{align}
            \deg(v_1\cdots v_n) &:= \deg(v_1)+\cdots+\deg(v_n)\,,\\
            \mathrm{wl}(v_1\cdots v_n) &:= n\,.
        \end{align}
        In general, only the word length is made explicit when writing down the space, i.e.~$v\in\Alt^{\mathrm{wl}(v)}V$.
    \end{example}
    \begin{remark}[Different conventions and d\'ecalage]\index{d\'ecalage}\index{Koszul!sign}\label{hda:decalage}
        Some authors use the notation $\Lambda^\bullet V$ for the free graded-commutative algebra on $V$. However, this might be confused with the notation for the Grassmann (exterior) algebra of an ordinary vector space.\footnote{This inconvenient change of conventions can be found everywhere in the literature, so one should pay close attention to the conventions that are used.} In fact, there is a good reason why these notations are used in a seemingly interchangeable way for graded vector spaces. The suspension functor $V\rightarrow V[1]$ gives a way to relate the Grassmann algebra over an ordinary vector space $V$ to the free GCA on the shifted space $V[1]$, i.e.~$\Alt^\bullet V\cong\Sym^\bullet V[1]$. However, at this point, the \textbf{d\'ecalage isomorphism}
        \begin{gather}
            \mathrm{dec}_k:\Lambda^kV[k]\cong\Sym^kV[1]
        \end{gather}
        is only a linear isomorphism. There are two ways to see that it can be extended to an algebra isomorphism.

        The first one defines the suspension functor as an intertwiner between the symmetrization and antisymmetrization operations in the definition of $\Sym$ and $\Alt$. Define the symmetric and antisymmetric Koszul signs of a permutation $\sigma\in S_n$ as follows:
        \begin{align}
            \varepsilon(\sigma;v_1,\ldots,v_n) &:= (-1)^{\#\text{ odd-odd neighbour transpositions in }\sigma}\,,\\
            \chi(\sigma;v_1,\ldots,v_n) &:= \sgn(\sigma)\varepsilon(\sigma;v_1,\ldots,v_n)\,.
        \end{align}
        D\'ecalage then says that the suspension functor should satisfy
        \begin{gather}
            \varepsilon(\sigma;v_1,\ldots,v_n)\sigma\circ[1]^{\otimes n} = [1]^{\otimes n}\circ\chi(\sigma;v_1,\ldots,v_n)\sigma\,.
        \end{gather}
        Since both $\Sym$ and $\Alt$ can be defined in terms of the projectors
        \begin{gather}
            \pi_{\Sym}:=\sum_{\sigma\in S_n}\varepsilon(\sigma)\sigma\qquad\text{and} \qquad \pi_{\Alt}:=\sum_{\sigma\in S_n}\chi(\sigma)\sigma\,,
        \end{gather}
        d\'ecalage interchanges symmetric and antisymmetric tensors. The most common choice is the following one:
        \begin{gather}
            [1]:V^{\otimes n}\rightarrow V[1]^{\otimes n}:v_1\otimes\cdots\otimes v_n\mapsto(-1)^{\sum_{i=1}^n(n-i)\deg(v_i)}v_1[1]\otimes\cdots\otimes v_n[1]\,.
        \end{gather}
        This choice is induced by the following definition of the suspension functor (one could also choose the convention where $V$ is tensored on the right):
        \begin{gather}
            [1]:\mathbb{Z}\mathbf{Vect}_k\rightarrow\mathbb{Z}\mathbf{Vect}_k:V\mapsto k[1]\otimes V\,.
        \end{gather}
        This definition also directly induces an algebra isomorphism in the following way. Consider two homogeneous elements $v,w\in V$. In $\Sym^2V$, their product satisfies
        \begin{gather}
            vw = (-1)^{\deg(v)\deg(w)}wv\,.
        \end{gather}
        After applying the suspension functor, the product on the left-hand side becomes:
        \begin{gather}
            v[1]w[1]=(\underline{1}\otimes v)(\underline{1}\otimes w) \cong (-1)^{\deg(w)}\underline{\underline{1}}\otimes(vw)=(-1)^{\deg(w)}vw[2]\,.
        \end{gather}
        To calculate the suspension of the right-hand side, the braiding in $\mathbb{Z}\mathbf{Vect}_k$ is used:
        \begin{align*}
            v[1]w[1]=(v\otimes\underline{1})(w\otimes\underline{1})\mapsto&(-1)^{\deg(v)+\deg(w)+\deg(v)\deg(w)+1}(w\otimes\underline{1})(v\otimes\underline{1})\\
            =&(-1)^{\deg(w)+\deg(v)\deg(w)+1}(wv)\otimes\underline{\underline{1}}\\
            =&(-1)^{\deg(w)+\deg(v)\deg(w)+1}wv[2]\,.
        \end{align*}
        The difference in signs is $(-1)^{\deg(v)\deg(w)+1}$. If either $v$ or $w$ is even, this final sign is $-1$ or, equivalently, the product is antisymmetric, while if both $v$ and $w$ are odd, the product is symmetric. This is exactly the opposite situation of that in $\Sym^2V$. The most thorough review of these issues was found in~\citet{miti_homotopy_2021}.
    \end{remark}

\subsection{Supermatrices}

    For this section, the requirement that all algebraic structures are defined over a field $k$ is relaxed to working over a supercommutative ring. This means that the objects will be (graded) modules instead of proper vector spaces.

    \newdef{Supermatrix}{\index{parity!of matrix}
        Every linear transformation between super vector spaces $(V_0,V_1)$ and $(W_0,W_1)$ can be decomposed as the sum of 4 linear transformations between the even/odd subspaces:
        \begin{itemize}
            \item $A:V_0\rightarrow W_0$,
            \item $B:V_1\rightarrow W_0$,
            \item $C:V_0\rightarrow W_1$, and
            \item $D:V_1\rightarrow W_1$.
        \end{itemize}
        If these components are represented as matrices, the full transformation can be represented as a block matrix \[X=\begin{pmatrix}A&B\\C&D\end{pmatrix}\,.\]

        These matrices can be classified according to their \textbf{parity}. Not all supermatrices preserve the grading or, equivalently, not all linear transformations of super vector spaces are morphisms of super vector spaces. The ones that are, are said to have even parity and they are of the form \[X=\begin{pmatrix}\mathrm{even}&\mathrm{odd}\\\mathrm{odd}&\mathrm{even}\end{pmatrix}\,,\] where even/odd means that the entries in these blocks have even/odd parity as elements of the underlying (graded) ring. It should be clear that these matrices indeed preserve the grading, since acting with an odd scalar on an odd vector gives an even vector (and similarly for the other combinations). The matrices that do not preserve the grading are said to have odd parity and are of the form \[X=\begin{pmatrix}\mathrm{odd}&\mathrm{even}\\\mathrm{even}&\mathrm{odd}\end{pmatrix}\,.\]
    }

    \newdef{Supertrace}{\index{super-!trace}
        The supertrace of a supermatrix generalizes the trace of an ordinary matrix. Given the block matrix form from the previous definition, the supertrace is defined as follows:
        \begin{gather}
            \mathrm{str}(X) := \tr(A)-\tr(D)\,.
        \end{gather}
    }
    \begin{property}
        As was the case for the ordinary trace, the supertrace is invariant under basis transformations. Furthermore, the cyclicity property also still holds after a slight modification to make it compatible with the grading:
        \begin{gather}
            \mathrm{str}(XY) = (-1)^{\deg(X)\deg(Y)}\mathrm{str}(YX)\,.
        \end{gather}
    \end{property}

    \newdef{Berezinian}{\index{Berezinian}\index{Schur!complement}\label{hda:berezinian}
        The Berezinian or \textbf{superdeterminant} generalizes the determinant of an ordinary matrix. It is (uniquely) defined through the following two conditions:
        \begin{enumerate}
            \item $\mathrm{Ber}(XY) = \mathrm{Ber}(X)\mathrm{Ber}(Y)$, and
            \item $\mathrm{Ber}(e^X) = e^{\mathrm{str}(X)}$.
        \end{enumerate}
        An explicit formula is given by
        \begin{gather}
            \mathrm{Ber}(X) = \det(A - BD^{-1}C)\det(D)^{-1} = \det(A)\det(D-CA^{-1}B)^{-1}\,,
        \end{gather}
        where the last expression involves the \textit{Schur complement} of $A$ relative to $X$. It should be noted that the Berezinian is only well-defined for invertible even matrices.
    }

\subsection{Berezin calculus}\label{section:berezin}

    This section is an application of the concept of exterior algebras (\cref{vector:exterior_algebra}). Grassmann numbers/variables are used in quantum field theory when performing calculations in e.g.~the fermionic sector or \textit{Faddeev--Popov quantization}.

    \newdef{Grassmann numbers}{\index{Grassmann!number}\label{hda:grassmann_number}
        Let $V$ be a vector space spanned by a set of elements $\theta_i$. The Grassmann algebra with Grassmann variables $\theta_i$ is the exterior algebra over $V$. In this setting the wedge symbol of Grassmann variables is often omitted when writing the product:\[\theta_i\wedge\theta_j \equiv \theta_i\theta_j\,.\]
    }
    \begin{remark}
        From the (anti)commutativity, it follows that one can regard the Grassmann variables as being nonzero square roots of zero.
    \end{remark}

    \begin{notation}[Parity]\index{parity}
        In the case of superalgebras and, in particular, that of Grassmann numbers, the degree of an element is often called the (Grassmann) parity of the element. It is also often denoted by $\varepsilon(x)$ or $\varepsilon_x$ instead of $\deg(x)$. In this text this convention is only adopted for graded algebras where there is both a supergrading and a (co)homological $\mathbb{Z}$-grading.
    \end{notation}

    \begin{property}[Polynomials]
        Consider a one-dimensional Grassmann algebra (with generator $\theta$). When constructing the polynomial ring $\mathbb{C}[\theta]$ generated by $\theta$, it can be seen that, due to the anticommutativity, $\mathbb{C}[\theta]$ is spanned only by $1$ and $\theta$. All higher-degree terms vanish because $\theta^2 = 0$. This implies that the most general polynomial over a one-dimensional Grassmann algebra is of the form
        \begin{gather}
            p(\theta) = a + b\theta\,.
        \end{gather}
    \end{property}

    \newdef{DeWitt convention}{\index{DeWitt!convention}
        One can equip the exterior algebra $\Lambda$ with Grassmann variables $\theta_i$ with an involution:
        \begin{gather}
            (\theta_i\theta_j\ldots\theta_k)^* := \theta_k\ldots\theta_j\theta_i\,.
        \end{gather}
        Elements $z\in\Lambda$ that satisfy $z^*=z$ are said to be \textbf{(super)real} and elements that satisfy $z^*=-z$ are said to be \textbf{(super)imaginary}. This convention is called the DeWitt convention.
    }

    To keep the discussion about Grassmann variables self-contained, the calculus of Grassmann variables is introduced here.
    \newdef{Derivative of Grassmann variables}{
        Consider the polynomial algebra $\mathbb{C}[\theta_1,\ldots,\theta_n]$ on $n$ Grassmann variables (more general functions would be defined through a series expansion, but given that $\theta^2=0$, these always reduce to a simple polynomial). Differentiation on this ring is defined through the following relations:
        \begin{gather}
            \pderiv{}{\theta_j}\theta_i = \delta_i^j\qquad\qquad\qquad\theta_i\pderiv{}{\theta_j}+\pderiv{}{\theta_j}\theta_i = 0\,.
        \end{gather}
        The second relation implies that the partial derivatives are also Grassmann-odd. The odd parity in fact allows to introduce two distinct differentiation operations. One is the left derivative, this is the one that was just introduced. The other is the right derivative that acts as
        \begin{gather}
            \theta_i\pderiv{^R}{\theta^j} = \delta_i^j\,.
        \end{gather}
        The left and right derivatives are also sometimes denoted by \[\overset{\rightarrow}{\pderiv{}{\theta^i}} \qquad\qquad\text{and}\qquad\qquad \overset{\leftarrow}{\pderiv{}{\theta^i}}\,,\] respectively.
    }

    Next, one also needs some kind of integration theory. Instead of working with a definition \`a la Riemann, the integral will be defined purely axiomatically.
    \newdef{Berezin integral: axiomatic}{\index{Berezin!integral}
        Consider a function $f$ of $n$ Grassmann variables $\{\theta_i\}_{i\leq n}$. The Berezin integral $\int_B$ is defined by the following conditions:
        \begin{enumerate}
            \item The map $f\mapsto\int_Bf(\theta)\,d\theta$ is linear.
            \item The result $\int_Bf(\theta)\,d\theta$ is independent of the variable(s) $\theta$, i.e.~it is a number.
            \item The result is invariant under a translation of the integration variable.
        \end{enumerate}
    }
    \begin{remark}
        Multiple integrals can be defined by adding the Fubini theorem as an additional axiom.
    \end{remark}

    It can be shown that this definition is equivalent to the following one.
    \newadef{Berezin integral: analytic}{
        First consider functions in one Grassmann variable, i.e.~$f(\theta)=a+b\theta$. The Berezin integral is then defined as follows:\footnote{Technically the axioms only imply this formula up to some multiplicative constant. The original convention by \textit{Berezin} will be adopted, i.e.~this constant is chosen to be $1$.}
        \begin{gather}
            \Int_B(a+b\theta)\,d\theta := b\,.
        \end{gather}
        This means that the integral is equal to the coefficient of the highest-degree term. As a simple generalization, define
        \begin{gather}
            \Int_Bf(\theta_1,\ldots,\theta_n)\,d\theta := \text{coefficient of }\theta_1\cdots\theta_n\,.
        \end{gather}
        Some authors reverse the order of the variables in the above definition. Depending on the number of variables, this might introduce an additional minus sign.
    }
    \begin{remark}
        It is interesting to see that the (one-dimensional) Berezin integral is equal to the (Grassmann) derivative. This is completely different from the usual integral in calculus. It also gives some intuition for the distinct transformation behaviour of the Berezin integral as explained in the following property.
    \end{remark}

    \begin{formula}[Change of variables]
        Consider a general Berezin integral $\int_Bf(\theta)\,d\theta$. Now, suppose that a transformation $\theta\rightarrow\xi(\theta)$ is applied to the Grassmann variables. If $J$ is the Jacobian matrix associated to this transformation, the integral transforms according to the following formula:
        \begin{gather}
            \Int_Bf(\xi)\,d\xi = \Int_Bf(\theta)(\det J)^{-1}\,d\theta\,.
        \end{gather}
    \end{formula}

    Berezin calculus can easily be unified with ordinary calculus by using the fact that ordinary coordinates (even parity) commute with Grassmann numbers (odd parity). A mixed derivative (resp. integral) can always be factorized as the composition of a Berezin derivative (resp. integral) and an ordinary one. The transformation behaviour is then generalized to this case by using the Berezinian.
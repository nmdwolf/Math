\section{Vector spaces}\label{section:vector_spaces}

    \newdef{$\mathfrak{K}$-vector space}{\index{vector!space}\label{linalgebra:vector_space}
        Let $\mathfrak{K}$ be a field. A $\mathfrak{K}$-vector space $V$ is a set equipped with two operations, \textbf{(vector) addition} $V\times V\rightarrow V$ and \textbf{scalar multiplication} $\mathfrak{K}\times V\rightarrow V$, that satisfy the following axioms:
        \begin{enumerate}
            \item $V$ forms an Abelian group under vector addition.
            \item Scalar multiplication is associative: $\lambda(\mu v) = (\lambda\mu)v$ for all $\lambda,\mu\in\mathfrak{K}$ and $v\in V$.
            \item The identity of the field $\mathfrak{K}$ acts as a neutral element for scalar multiplication: $1_{\mathfrak{K}}v = v$ for all $v\in V$.
            \item Scalar multiplication is distributive with respect to vector addition: $\lambda(v+w) = \lambda v + \lambda w$ for all $\lambda\in\mathfrak{K}$ and $v,w\in V$.
            \item Vector addition is distributive with respect to scalar multiplication: $(\lambda+\kappa)v = \lambda v + \kappa w$ for all $\lambda,\kappa\in\mathfrak{K}$ and $v\in V$.
        \end{enumerate}
    }
    From here on, the underlying field $\mathfrak{K}$ will be left implicit unless the results depend on it.

    \remark{The above definition can be restated in abstract algebraic terms. A $\mathfrak{K}$-vector space is a module (\cref{algebra:module}) over $\mathfrak{K}$.}

\subsection{Linear independence}

    \newdef{Linear combination}{\index{basis!Hamel}\label{linalgebra:linear_combination}
        The vector $w$ is a linear combination of elements in the set $\{v_i\}_{i\leq n}\subset V$ if it can be written as
        \begin{gather}
            w = \sum_{i=1}^n\lambda_iv_i
        \end{gather}
        for some $\{\lambda_i\}_{i\leq n}\subset\mathfrak{K}$. One can generalize this to general subsets $S\subseteq V$, but the number of nonzero elements $\lambda_i$ is always required to be finite.\footnote{Generalizations are possible in the context of topological vector spaces (see \namecrefs{chapter:topology}~\nameref{chapter:topology} and~\nameref{chapter:functional}), where one can define the notion of convergence.} (See \cref{linalgebra:hamel_remark} about \textit{Hamel bases}.)
    }
    \newdef{Linear independence}{\label{linalgebra:linear_independence}
        A finite set $\{v_i\}_{i\leq n}$ is said to be linearly independent if the following relation holds:
        \begin{gather}
            \sum_{i=1}^n\lambda_i v_i = 0 \iff \forall i\leq n:\lambda_i = 0\,.
        \end{gather}
        A general set $S\subset V$ is said to be linearly independent if every finite subset of it is linearly independent.
    }

    \newdef{Span}{\index{span}
        A set of vectors $S\subseteq V$ is said to span $V$ if every vector $v\in V$ can be written as a linear combination of elements in $S$.
    }

    \newdef{Frame}{\index{frame}\label{linalgebra:frame}
        A $k$-frame is an ordered set of $k\in\mathbb{N}$ linearly independent vectors.
    }

\subsection{Bases}\index{basis}

    \newdef{Basis}{
        A subset $\mathcal{B}\subset V$ that is linearly independent and spans $V$.
    }
    \begin{property}
        Every spanning set contains a basis.
    \end{property}

    \begin{remark}[Hamel basis]\index{basis!Schauder}\index{basis!Hamel}\label{linalgebra:hamel_remark}
        In the previous definition, the concept of a Hamel basis was implicitly used. This concept is based on two conditions:
        \begin{enumerate}
            \item The basis is linearly independent.
            \item Every element in the vector space can be written as a linear combination of a \underline{finite} subset of the basis.
        \end{enumerate}
        For bases consisting of a finite number of vectors, one does not have to worry. However, for infinite bases, one has to keep this in mind. An alternative construction that allows for combinations of a countably infinite number of elements is given by that of a \textit{Schauder basis}.
    \end{remark}
    Nonetheless, it can be shown that every vector space admits a Hamel basis.
    \begin{construct}[\difficult{Hamel basis}]\index{basis!Hamel}\label{linalgebra:hamel_basis}
        Let $V$ be a vector space and consider the set of all linearly independent subsets of $V$. Under the relation of inclusion, this set becomes a partially ordered set (\cref{set:poset}). Zorn's lemma~\ref{set:zorns_lemma} then says that there exists at least one maximal linearly independent set.

        Now, one can show that this maximal subset $S$ is also a spanning set of $V$. Choose a vector $v\in V$ that is not already in $S$. From the maximality of $S$, it follows that $S\cup v$ is linearly dependent and, hence, there exists a finite sequence of scalars $(a^1,\ldots,a^n,b)$ and a finite sequence of elements $(e_1,\ldots,e_n)$ in $S$ such that:
        \begin{gather}
            \sum_{i=0}^n a^ie_i + bv = 0\,,
        \end{gather}
        where not all scalars are zero. This implies that $b\neq0$, because otherwise the set $\{e_i\}_{i\leq n}$ and, hence, also $S$ would be linearly dependent. It follows that $v$ can be written as\footnote{It is this step that requires $R$ to be a division ring in \cref{algebra:module_basis} because, otherwise, one would, in general, not be able to divide by $b\in R$.}
        \begin{gather}
            v = -\frac{1}{b}\sum_{i=0}^na^ie_i\,.
        \end{gather}
        Because $v$ was chosen randomly, one can conclude that $S$ is a spanning set for $V$.
    \end{construct}
    \begin{remark*}
        This construction assumes the axiom of choice in set theory, only ZF does not suffice. It can even be shown that the existence of a Hamel basis for every vector space is equivalent to the axiom of choice.
    \end{remark*}

    \begin{property}
        Every basis of a vector space has the same number of elements. For infinite-dimensional spaces, this means that all bases have the same \textit{cardinality}.
    \end{property}
    \newdef{Dimension}{\index{dimension!of vector space}\label{linalgebra:dimension}
        Let $V$ be a finite-dimensional vector space and let $\mathcal{B}$ be a basis for $V$ with $n$ elements. With the previous property in mind, the dimension of $V$ is defined as follows:
        \begin{gather}
            \dim(V) := n\,.
        \end{gather}
    }

    \newdef{Subspace}{\label{linalgebra:subspace}
        Let $V$ be a vector space. A subset $W$ of $V$ is called a subspace if $W$ is itself a vector space under (the restriction of) the operations of $V$:
        \begin{gather}
            W\leq V\iff\forall w_1,w_2\in W,\forall\lambda\in\mathfrak{K}:\lambda w_1 + w_2 \in W\,.
        \end{gather}
    }

\subsection{Sum and direct sum}

    \newdef{Sum}{\index{sum}
        Let $V$ be a vector space and consider a finite collection of subspaces $\{W_1,\ldots,W_n\}$. The sum of these subspaces is defined as follows:
        \begin{gather}
            W_1+\cdots+W_n := \left\{\sum_{i=1}^nw_i\,\middle\vert\,w_i\in W_i\right\}\,.
        \end{gather}
        For an infinite collection of subspaces, the linear combinations have to be finite.
    }
    \newdef{Direct sum}{\index{direct!sum}\label{linalgebra:direct_sum}
        If every element $v$ of the sum can be written as a unique linear combination, the sum is called a direct sum.
    }
    \newnot{Direct sum}{
        The direct sum of vector spaces is denoted by
        \begin{gather*}
            W_1\oplus\cdots\oplus W_k\equiv\bigoplus_{i=1}^kW_i\,.
        \end{gather*}
    }

    \begin{formula}
        Let $V$ be a finite-dimensional vector space and consider two subspaces $W_1,W_2\leq V$. The dimensions of these spaces can be related in the following way:
        \begin{gather}
            \dim(W_1+W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1\cap W_2)\,.
        \end{gather}
    \end{formula}
    \begin{property}
        Let $V$ be a vector space and assume that $V$ can be decomposed as $W=W_1\oplus W_2$. If $\mathcal{B}_1$ is a basis of $W_1$ and if $\mathcal{B}_2$ is a basis of $W_2$, then $\mathcal{B}_1\cup\mathcal{B}_2$ is a basis of $W$.
    \end{property}

    \newdef{Complement}{\index{complement!vector space}
        Let $V$ be a vector space and let $W$ be a subspace of $V$. A subspace $W'$ of $V$ is called a complement of $W$ if $V = W\oplus W'$.
    }
    \begin{property}[Existence of complements]\label{linalgebra:complement}
        Let $V$ be a vector space and let $U,W$ be two subspaces of $V$. If $V = U+W$, there exists a subspace $Y\leq U$ such that $V = Y\oplus W$. In particular, every subspace of $V$ has a complement in $V$.
    \end{property}

\section{Linear maps}

    \newdef{Linear map}{\index{linear!map}
        Let $V,W$ be $\mathfrak{K}$-vector spaces. A function $f:V\rightarrow W$ is said to be ($\mathfrak{K}$-)linear if $f(\lambda v+w)=\lambda f(v)+f(w)$ for all $\lambda\in\mathfrak{K}$ and $v,w\in V$.
    }

    \remark{Linear maps are also called \textbf{linear transformations} or \textbf{linear mappings}.}

\subsection{Homomorphisms}

    \newdef{Homomorphism space}{\index{morphism!of vector spaces}\label{linalgebra:hom_space}
        Let $V,W$ be two $\mathfrak{K}$-vector spaces. The set of all linear maps between $V$ and $W$ is called the homomorphism space from $V$ to $W$:
        \begin{gather}
            \hom_{\mathfrak{K}}(V,W) := \bigl\{f:V\rightarrow W\bigm\vert f\text{ is $\mathfrak{K}$-linear}\bigr\}\,.
        \end{gather}
        The collection of $\mathfrak{K}$-vector spaces and linear maps between them form a category $\symbfsf{Vect}_{\mathfrak{K}}$.
    }
    \begin{formula}\label{linalgebra:hom_dimension}
        Let $V,W$ be two finite-dimensional $\mathfrak{K}$-vector spaces.
        \begin{gather}
            \dim\bigl(\hom_{\mathfrak{K}}(V,W)\bigr) =\dim(V)\dim(W)
        \end{gather}
    \end{formula}

    \newdef{Endomorphism ring}{\index{endo-!morphism}
        The space $\hom_{\mathfrak{K}}(V,V)$ with composition of maps as multiplication forms a ring, the endomorphism ring. It is denoted by $\End_{\mathfrak{K}}(V)$ or $\End(V)$ when the underlying field is clear.
    }
    \begin{property}[Commutator]\index{commutator}
        The endomorphism ring $\End(V)$ can also be endowed with the structure of a \textit{Lie algebra} (see \cref{lie:end_as_lie_algebra}) by equipping it with the commutator
        \begin{gather}
            [A,B] := A\circ B - B\circ A\,.
        \end{gather}
    \end{property}

    \begin{property}
        Let $V$ be finite-dimensional vector space and let $f:V\rightarrow V$ be an endomorphism. The following statements are equivalent:
        \begin{itemize}
            \item $f$ is injective,
            \item $f$ is surjective, and
            \item $f$ is bijective.
        \end{itemize}
    \end{property}

    \newdef{Automorphism}{\index{automorphism}\index{general linear group}\label{linalgebra:automorphism}
        The automorphism group (\cref{algebra:automorphism}) of a vector space is called the general linear group\footnote{It is isomorphic to the general linear group of invertible \textit{matrices} (see \cref{linalgebra:GL_matrices}), hence the similar name and notation.} and is denoted by $\GL_{\mathfrak{K}}(V)$ or $\GL(V)$ when the underlying field is clear.
    }
    \remark{Sometimes automorphisms are also called \textbf{linear operators}. However, this terminology is also used for a general linear map in operator theory (see \labelref{chapter:operator_algebras}) and so this terminology is not adopted in this text.\index{operator}}

    \newdef{Kernel}{\index{kernel}
        Consider a linear map $f:V\rightarrow W$. The kernel of $f$ is defined as the following subspace of $V$:
        \begin{gather}
            \ker(f) := \{v\in V\mid f(v) = 0\}\,.
        \end{gather}
    }
    \begin{property}
        A linear map $f:V\rightarrow W$ is injective if and only if $\ker(f)=0$.
    \end{property}

    \newdef{Rank}{\index{rank!of a linear map}\label{linalgebra:image_rank}
        The dimension of the image of a linear map.
    }
    \newdef{Nullity}{\index{nullity}
        The dimension of the kernel of a linear map.
    }

    \begin{theorem}[Dimension theorem\footnotemark]\index{rank-nullity theorem}\label{linalgebra:dimension_theorem}
        \footnotetext{Also called the \textbf{rank-nullity theorem}.}
        Let $f:V\rightarrow W$ be a linear map.
        \begin{gather}
            \dim\bigl(\im(f)\bigr) + \dim\bigl(\ker(f)\bigr) = \dim(V)
        \end{gather}
    \end{theorem}
    \begin{result}\label{linalgebra:dimension_isomorphism}
        Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.
    \end{result}

    \begin{property}[Jordan--Chevalley decomposition]\index{Jordan--Chevalley decomposition}\index{semisimple!operator}\index{nilpotent}\label{linalgebra:jordan_chevalley}
        Every endomorphism $A$ can be decomposed as follows:
        \begin{gather}
            A = A_{ss} + A_n\,,
        \end{gather}
        where
        \begin{itemize}
            \item $A_{ss}$ is \textbf{semisimple}: for every invariant subspace of $A_{ss}$ there exists an invariant complementary subspace.
            \item $A_n$ is \textbf{nilpotent}: $\exists k\in\mathbb{N}:A_n^k = 0$.
        \end{itemize}
        Furthermore, this decomposition is unique and the endomorphisms $A_{ss},A_n$ can be written as polynomials in $A$.
    \end{property}

\subsection{Dual maps}

    \newdef{Dual space}{\index{dual!space}\index{linear!form}\index{functional}\label{linalgebra:dual_space}
        Let $V$ be a $\mathfrak{K}$-vector space. The (algebraic) dual $V^*$ of $V$ is defined as the following vector space:
        \begin{gather}
            V^*:=\hom_{\mathfrak{K}}(V,\mathfrak{K})=\bigl\{f:V\rightarrow \mathfrak{K}\bigm\vert f\text{ is $\mathfrak{K}$-linear}\bigr\}\,.
        \end{gather}
        The elements of $V^*$ are called \textbf{linear forms} or (linear) \textbf{functionals}.
    }
    \begin{property}[Dimension]\label{linalgebra:dual_space_dimension}
        From \cref{linalgebra:hom_dimension}, it follows that $\dim(V^*)=\dim(V)$ whenever $V$ is finite dimensional. If $V$ is infinite dimensional, this property is \underline{never} valid. In the infinite-dimensional case, $\mathrm{card}(V^*)>\mathrm{card}(V)$ always holds.
    \end{property}

    \newdef{Dual basis}{\index{basis!dual}\label{linalgebra:dual_basis}
        Let $\mathcal{B}=\{e_1,e_2,\ldots,e_n\}$ be a basis for a finite-dimensional vector space $V$. One can construct a basis $\mathcal{B}^*=\{\varepsilon_1,\varepsilon_2,\ldots,\varepsilon_n\}$ for $V^*$, called the dual basis of $\mathcal{B}$, as follows:
        \begin{gather}
            \varepsilon_i:\sum_{j=1}^na_ie_i\mapsto a_i\,.
        \end{gather}
        The relation between a basis and its associated dual basis can also be expressed as
        \begin{gather}
            \label{linalgebra:dual_basis_2}
            \varepsilon^i(e_j) = \delta^i_j\,.
        \end{gather}
    }
    \newdef{Natural pairing}{\index{natural!pairing}\label{linalgebra:natural_pairing}
        The definition of the dual basis extends to a natural pairing of $V$ and its dual $V^*$ in terms of the following bilinear map:
        \begin{gather}
            \langle v,v^* \rangle := v^*(v)\,.
        \end{gather}
        (See \cref{hda:dual} for a generalization of this map.)
    }

    \newdef{Dual map}{\index{dual!map}\index{transpose}\label{linalgebra:transpose}
        Let $f:V\rightarrow W$ be a linear map. The linear map
        \begin{gather}
            f^*:W^*\rightarrow V^*:\varphi\rightarrow\varphi\circ f
        \end{gather}
        is called the dual map or \textbf{transpose} of $f$. It is also often denoted by $f^T$.
    }

    \newdef{Dual pair}{\index{dual!pair}
        The concept of a dual space can be generalized. A dual pair (or \textbf{dual system}) is a triple $(V,W,\langle\cdot,\cdot\rangle)$, where $V,W$ are two $\mathfrak{K}$-vector spaces and $\langle\cdot,\cdot\rangle:V\times W\rightarrow\mathfrak{K}$ is a nondegenerate bilinear map, i.e.
        \begin{enumerate}
            \item If $\langle v,\cdot\rangle=0$ for some $v\in V$, then $v=0$.
            \item If $\langle\cdot,w\rangle=0$ for some $w\in W$, then $w=0$.
        \end{enumerate}
        Note that, given a dual pair $(V,W,\langle\cdot,\cdot\rangle)$, one obtains canonical maps $V\rightarrow W^*$ and $W\rightarrow V^*$.
    }

    \newdef{Orthogonal complement}{\index{complement!orthogonal}\label{linalgebra:dual_complement}
        Consider a dual pair $(V,W,\langle\cdot,\cdot\rangle)$. The orthogonal complement of a subset $S\subseteq V$ is defined as follows:
        \begin{gather*}
            S^\perp := \{w\in W\mid\forall s\in S:\langle s,w\rangle=0\}\,.
        \end{gather*}
    }
    \sremark{$S^\perp$ is pronounced as `S-perp'.}
\chapter{Optimization Problems and Data Analysis}

    The main reference for the sections on optimization problems is \cite{conjugategradient}. For the geometry of clustering methods, see \cite{clustering_bregman}. The main references for the section on \textit{conformal prediction} are \cite{cp, cp_all}. Although a part of this chapter is a continuation of the previous one, the focus here lies more on the computational aspect of the analysis of large data sets. For this reason the chapter starts with some sections on applied linear algebra (for a refresher see Chapter \ref{chapter:linear_algebra}).

\section{Optimization}
\subsection{Linear equations}

    \begin{method}[Normal equation]\index{normal!equation}
        Given the equation \[Ax=b\] as in Section \ref{section:system_of_equations}, one can try to numerically solve for $x$ by minimizing the $\ell^2$-norm $\|Ax-b\|^2$:
        \begin{gather}
            \hat{x}:=\arg\min_x(Ax-b)^T(Ax-b).
        \end{gather}
        This leads to the so-called normal equation\footnote{The name stems from the fact that the equation $A^TAx = A^Tb$ implies that the residual is orthogonal (normal) to the range of $A$.}
        \begin{gather}
            \label{data:normal_equation}
            A^TAx = A^Tb.
        \end{gather}
        This can be formally be solved by $x=(A^TA)^{-1}A^Tb$, where $(A^TA)^{-1}A^T$ is the pseudoinverse of $A$.
    \end{method}
    \remark{It is easy to see that the above linear problem is obtained when trying to extremize the quadratic form associated to a symmetric matrix.}

    \begin{method}[Tikhonov regularization]{\index{Tikhonov regularization}\index{ill-conditioned}
        Consider a linear (regression) problem \[Ax = b.\] The most straightforward way to solve for $x$ is the least squares method introduced in Chapter~\ref{chapter:statistics}, where the solution is (formally) given by the normal equation: $x=(A^TA)^{-1}A^Tb$. However, sometimes it might happen that $A$ is nearly singular (it is said to be \textbf{ill-conditioned}). In this case a regularization term can be added to the minimization problem:
        \begin{gather}
            \|Ax-b\|^2+\|\Gamma x\|^2,
        \end{gather}
        where $\Gamma$ is called the \textbf{Tikhonov matrix}. In the case that $\Gamma=\lambda\mathbbm{1}$, one speaks of \textbf{$\ell^2$-regularization}. This regularization technique benefits solutions with smaller norms.
    }
    \end{method}
    \begin{remark}\index{lasso}\index{ridge}
        The $\ell^2$-regularization can be generalized by replacing the 2-norm by any $p$-norm $\|\cdot<|_p$. For $p=1$ and $p=2$ the names \textbf{lasso} and \textbf{ridge} regression are often used. For general $p\geq0$ one sometimes speaks of \textbf{bridge} regression.

        The minimization procedures for $p\leq1$ have the important property that they not only shrink the coefficients, but even perform feature selection, i.e. some coefficients become identically zero. However, it can be shown that the optimization problem for $p<1$ is nonconvex and, hence, is harder to solve. In general it is found that lasso regression gives the best results.

        A benefit of $\ell^2$-regularization is that it can be derived from a Bayesian approach. By choosing a Gaussian prior $\mathcal{N}(0,\lambda^{-1})$, Bayesian inference immediately gives the $\ell^2$-regularized cost function as the posterior distribution. Accordingly the $\ell^2$-regularized linear regressor is equivalent to the maximum a posteriori estimator with Gaussian priors. One can obtain $\ell^p$-regularization in a similar way by replacing the Gaussian priors with generalized normal distributions (such as the Laplace distribution for $p=1$).
    \end{remark}

    \newdef{Multicollinearity}{\index{collinearity}
        Consider a finite set of random variables $\{X_i\}_{1\leq i\leq n}$. These random variables are said to be perfectly (multi)collinear if there exists an affine relation between them, i.e. there exist variables $\{\lambda_i\}_{0\leq i\leq n}$ such that
        \begin{gather}
            \lambda_0 + \lambda_1X_1 + \cdots + \lambda_nX_n = 0.
        \end{gather}
        The same concept can be applied to data samples. The data is said to be (multi)collinear if the above equation holds for all entries of the data set. However, in this case one also define ``near multicollinearity'' if the variables $X_i$ are related as above up to some error term $\varepsilon$. If the variance of $\varepsilon$ is small, the matrix $X^TX$ might have an ill-conditioned inverse which might render the algorithms unstable.
    }
    \newdef{Variance inflation factor}{\index{variance!inflation factor}
        \nomenclature[A_VIF]{VIF}{variance inflation factor}
        The VIF is an estimate for how much the variance of a coefficient is inflated by multicollinearity. The VIF of a coefficient $\beta_i$ is defined as follows:
        \begin{gather}
            \mathrm{VIF}_i := \frac{1}{1-R_i^2},
        \end{gather}
        where $R_i^2$ is the $R^2$-value obtained after regressing the predictor $\hat{X}_i$ on all other predictors. The rule of thumb is that $\mathrm{VIF}\geq10$ implies that a significant amount of multicollinearity is present in the model.
    }

\subsection{Descent methods}

    The gradient descent algorithm is first introduced in the case of quadratic forms:
    \begin{method}[Steepest descent]\index{residual}
        Consider the quadratic form \[f(x) = \frac{1}{2}x^TAx - b^Tx + c.\] Assume that $A$ is symmetric and positive-definite such that $Ax=b$ gives the minimum of $f$. Like most recursive algorithms, gradient descent starts from an arbitrary guess $x_0$. It then takes a step in the direction of steepest descent (or largest gradient), i.e. in the direction opposite to $f'(x_0) = Ax_0-b =: -r_0$:
        \begin{gather}
            x_{i+1} := x_i + \alpha r_i.
        \end{gather}
        The quantities $r_i$ are called the \textbf{residuals}. This procedure is repeated until convergence, i.e. until the residual vanishes up to a fixed numerical tolerance.

        A naive gradient descent method would require to fine-tune the step size $\alpha$. However, a more efficient method is given by the \textbf{line search algorithm}, where the value of $\alpha$ is optimized in every step as to minimize $f$ along the line defined by $r_i$. A standard calculus argument leads to the following form of the step size:
        \begin{gather}
            \alpha_i = \frac{r_i^Tr_i}{r_i^TAr_i}.
        \end{gather}
        This choice forces the descent direction to be orthogonal to the previous one since $\deriv{}{\alpha}f(x_i) = -f'(x_i)^Tf'(x_{i-1})$. As a consequence, this minimization scheme often results in a chaotic zigzag trajectory through the configuration space. The higher the \textbf{condition number} $\kappa=\frac{|\lambda_{\max}|}{|\lambda_{\min}|}$, the worse the zigzag motion will be. A very narrow valley (or some higher-dimensional analogue) will make the trajectory bounce back and forth between the walls, instead of moving towards the minimum.
    \end{method}

\subsection{Conjugate gradient}\index{conjugate!gradient}

    As noted in the previous section, a common problem with gradient descent is that the direction of steepest descent is often not the same as the direction pointing to the optimal solution and, hence, convergence might only occur after a long time.

    A simple solution can be obtained by considering multiple orthogonal directions and taking a suitable step once in every direction. This way one obtains an algorithm that converges in $n$ steps, where $n$ is the dimension of the coefficient matrix $A$. By requiring that the error at step $i+1$ is orthogonal to the direction $d_i$, it is assured that no direction is used twice. However, the main problem with this idea is that the exact error $e_i$ is not known and, hence, one cannot calculate the required steps.

    By modifying the orthogonality condition one can avoid this problem. This is the idea behind conjugate direction methods:
    \newdef{Conjugate vectors}{\index{Arnoldi method}\label{Arnoldi iteration}
        Consider a symmetric positive-definite matrix $A$. Any such matrix induces an inner product as follows:
        \begin{gather}
            \label{data:A_inner_product}
            \langle v|w \rangle_A := v^TAw.
        \end{gather}
        Two vectors $v,w$ are said to be ($A$-)conjugate if they are orthogonal with respect to $\langle\cdot|\cdot\rangle_A$. The general approach to obtain a basis of $A$-conjugate vectors is a modified version of the Gram-Schmidt procedure \ref{linalgebra:gram_schmidt} where the ordinary Euclidean inner product is replaced by \eqref{data:A_inner_product}. This modification is called the \textbf{Arnoldi method}.
    }

    By taking the input vectors of the Arnoldi method to be the residuals $r_i$, one obtains the \textbf{conjugate gradient} (CG) algorithm. It is interesting to note that the residuals themselves satisfy a recursion relation:
    \begin{gather}
        r_{i+1} = r_i - \alpha_iAd_i,
    \end{gather}
    where the step size $\alpha_i$ is defined similar to the step size for ordinary steepest descent:
    \begin{gather}
        \alpha_i = \frac{d_i^Tr_i}{d_i^TAd_i}.
    \end{gather}
    Since the directions are constructed using the residuals, they span the same subspace. By denoting the subspace spanned by the first $i$ directions by $\mathcal{D}_i$, the relation $r_{i+1}\in\mathcal{D}_i+Ad_i$ leads to the following expression because of the above recursion relation:
    \begin{gather}
        \mathcal{D}_i = \mathrm{span}\left\{r_0,Ar_0,\ldots,A^{i-1}r_0\right\}.
    \end{gather}
    Because of their prominence in the literature on numeric optimization techniques, these subspaces have earned their own name:
    \newdef{Krylov subspace}{\index{Krylov subspace}
        \nomenclature[S_KnAv]{$\mathcal{K}_n(A,v)$}{Krylov subspace of dimension $n$ generated by the matrix $A$ and the vector $v$}
        A vector space $\mathcal{K}$ of the form
        \begin{gather}
            \mathcal{K} := \mathrm{span}\left\{v,Av,\ldots,A^nv\right\}
        \end{gather}
        for some matrix $A$, vector $v$ and natural number $n\in\mathbb{N}$. Given such an $A$ and $v$, one often denotes the associated Krylov subspace of dimension $n$ by $\mathcal{K}_n(A,v)$.
    }

    The fact that the spaces $\mathcal{D}_i$ are Krylov spaces also has an import implication for the numerical complexity of the CG algorithm. The residual $r_{i+1}$ can be shown to be orthogonal to the space $\mathcal{D}_{i+1}$ (this is generally called the \textbf{Galerkin condition}\index{Galerkin condition}). But since $A\mathcal{D}_i\subset\mathcal{D}_{i+1}$, this also implies that $r_{i+1}$ is $A$-conjugate to $\mathcal{D}_i$. It follows that the only relevant contribution in the Arnoldi method is given by the last direction $d_i$. This reduces the complexity (both time-wise and memory-wise) per iteration from $O(n^2)$ to $O(n)$.

    The steps in the CG algorithm are summarized below:
    \begin{method}[Conjugate gradient]
        Let $x_0$ be the initial guess with the associated residual $r_0:=b-Ax_0$ acting as the first direction vector $d_0$. The following scheme gives an iterative $n$-step ($n$ being the dimension of the coefficient matrix $A$) algorithm to obtain the solution to $Ax=b$:
        \begin{align}
            \alpha_i &:= \frac{r_i^Tr_i}{d_i^TAd_i}\\
            x_{i+1} &:= x_i+\alpha_id_i\\
            r_{i+1} &:= r_i-\alpha_iAd_i\label{data:residual_recurrence}\\
            d_{i+1} &:= r_{i+1}+\frac{r_{i+1}^Tr_{i+1}}{r_i^Tr_i}d_i.\label{data:beta}
        \end{align}
    \end{method}

    \remark{In exact arithmetic the above optimization scheme would result in an exact solution after $n$ iterations (in fact the number of iterations is bounded by the number of distinct eigenvalues of $A$). However, in real life one is not working in exact arithmetic and one has to take into account the occurrence of floating-point errors. These not only ruin the accuracy of the residual recursion relation \eqref{data:residual_recurrence}, but more importantly\footnote{The residual problem can be solved by computing the residual ``exactly'', i.e. by the formula $r_i=b-Ax_i$, every $k$ iterations.} it might result in the search directions not being $A$-conjugate.}

    Now, what about general coefficient matrices $A$, for example those resulting in under- or overdetermined systems? For nonsymmetric or nondefinite square matrices one can still solve the normal equation \eqref{data:normal_equation} using the same methods, since $A^TA$ is both symmetric and positive-definite. For underdetermined systems an exact solution does not always exact, but the numerical methods will always be able to find a solution that minimizes the $\ell^2$-error. For overdetermined systems $A^TA$ will be nonsingular and the numerical methods can find an exact solution. However, the condition number of $A^TA$ is the square of that of $A$ and, hence, the algorithms will convergence much slower.

    A different approach exists where the CG algorithm is not applied to the matrix $A^TA$, but the individual matrices are used $A,A^T$ directly. This way not one Krylov space is generated, but two dual ``copies'' are constructed:
    \begin{align*}
        \mathcal{D}_i &:= \mathrm{span}\left\{r_0,Ar_0,\ldots,A^{i-1}r_0\right\}\\
        \widetilde{\mathcal{D}}_i &:= \mathrm{span}\left\{\widetilde{r}_0,A^T\widetilde{r}_0,\ldots,(A^T)^{i-1}\widetilde{r}_0\right\},
    \end{align*}
    where $\widetilde{r}_0$ does not have to be related to $r_0$. In this case there are two Galerkin conditions $r_i\perp\mathcal{D}_i$ and $\widetilde{r}_i\perp\widetilde{\mathcal{D}}_i$ (only the first one is relevant). The residuals form biorthogonal bases of the Krylov subspaces:
    \begin{gather}
        \langle r_i|r_j \rangle = \|r_i\|^2\delta_{ij}.
    \end{gather}
    As a consequence the search directions also form biconjugate bases:
    \begin{gather}
        \langle d_i|d_j \rangle_A = \|d_i\|_A^2\delta_{ij}.
    \end{gather}

\subsection{Nonlinear conjugate gradients}\index{conjugate!gradient}

    Of course, many real-world applications are determined by nonlinear equations and, hence, it would be pleasant if one could salvage some of the above ideas even when linear algebra is not the natural language. The main requirement would be that one can calculate the gradient of the function to be minimized.

    On the level of the implementation, the structure of the algorithm remains more or less the same. What does change is the form of the Arnoldi method, in particular, the prefactor in Equation \eqref{data:beta}. For linear CG there are multiple equivalent formulas, but for nonlinear CG these do not lead to the same algorithm. The two most common choices are given below.
    \begin{method}[Nonlinear CG]\index{Fletcher-Reeves formula}\index{Polak-Ribi\`ere formula}
        Since there is no linear equation related to the minimization problem, the residuals are always defined as $r_i:=-f'(x_i)$. The algorithm consists of the following iterations:
        \begin{align}
            \alpha_i &:= \arg\min_\alpha f(x_i+\alpha d_i)\label{data:argmin}\\
            x_{i+1} &:= x_i+\alpha_id_i\\
            r_{i+1} &:= -f'(x_i)\\
            d_{i+1} &:= r_{i+1}+\beta_{i+1}d_i,
        \end{align}
        where $\beta_{i+1}$ is computed by one of the following formulas:
        \begin{itemize}
            \item \textbf{Fletcher-Reeves formula}:
                \begin{gather}
                    \beta_{i+1} := \frac{r_{i+1}^Tr_{i+1}}{r_i^Tr_i}.
                \end{gather}
            \item \textbf{Polak-Ribi\`ere formula}:
                \begin{gather}
                    \label{data:polak_ribiere}
                    \beta_{i+1} := \max\left\{\frac{r_{i+1}^T(r_{i+1}-r_i)}{r_i^Tr_i}, 0\right\}.
                \end{gather}
        \end{itemize}
    \end{method}

    Some general remarks have to be made concerning the nonlinear CG algorithm:
    \begin{remark}
        As was already mentioned for the linear version, floating-point errors might lead to a loss of conjugacy. For the nonlinear extension this becomes worse. The more $f$ deviates from a quadratic function, the quicker conjugacy is lost (for quadratic formulas the Hessian is exactly the matrix $A$, but for higher-degree functions the Hessian varies from point to point). Another problem, one that did not occur for quadratic functions, is that nonlinear functions might have multiple local minima. The CG method does not care about local vs. global and, hence, it will not necessarily converge to the global minimum. A last remark concerns the fact that there is no theoretical guarantee that the method will converge in $n$ steps. Since the Gram-Schmidt procedure can only construct $n$ conjugate vectors, the simplest solution is to perform a restart of the algorithm every $n$ iterations.\footnote{The $\max$ operation in Equation \eqref{data:polak_ribiere} is already a form of restarting, due to the fact that the Polak-Ribi\`ere version of nonlinear CG sometimes results in cyclic behaviour.}
    \end{remark}

    For linear CG a simple formula for finding the optimal value of $\alpha_i$ was obtained. However, for nonlinear CG one cannot solve Equation \eqref{data:argmin} as easily. The main idea, i.e. that $f'$ should be orthogonal to the previous search direction remains, is still valid. Here, only the \textbf{Newton-Raphson approach} is considered:\footnote{Another common method is the \textit{secant method}.}\index{Newton-Raphson alogrithm}
    \begin{gather}
        \alpha_i = \frac{f'(x_i)^Td_i}{d_i^Tf''(x_i)d_i}.
    \end{gather}
    To obtain the optimal $\alpha$-value, one should iteratively apply the Newton-Raphson method in every CG iteration. If the action of the Hessian $f''$ on $d_i$ cannot be simplified, i.e. if the full Hessian has to be computed in every iteration, this can lead to considerable computational overhead. The general rule of thumb is to perform only a few Newton-Raphson iterations and obtain a less accurate but more efficient algorithm. To make sure that the search descent direction is indeed a direction of descent (and not one of ascent), one can check that $r^Td\geq0$ and restart the procedure if it is negative.

\subsection{Krylov methods}

    Generally one starts from an iterative fixed-point based technique to solve the linear equation $Ax=b$ as before, i.e. one iterates $x_{i+1} = b + (\mathbbm{1}-A)x_i$. Using the residuals $r_i = b - Ax_i$ this can be rewritten as
    \begin{gather}
        x_i = x_0 + \sum_{k=0}^{i-1}r_k = x_0 + \sum_{k=0}^{i-1}(\mathbbm{1}-A)^kr_0.
    \end{gather}
    It is clear that this results in $x_i-x_0\in\mathcal{K}_i(A,r_0)$. The main idea is then to find optimal degree-$k$ polynomials $P_k$ such that $x_i-x_0=\sum_{k=0}^{i-1}P_k(A)r_0$.

    \begin{method}[Jacobi method]\index{Jacobi!method}
        Consider a linear problem $Ax=b$ where $A$ has spectral radius less than $1$. First, decompose $A$ as the sum of a diagonal matrix $D$ and and a matrix $E$ with zero diagonal elements. If one assumes that $D$ is invertible, the following recursive scheme is obtained:
        \begin{gather}
            x_{i+1} := D^{-1}(b-Ex_i).
        \end{gather}
        A sufficient condition for convergence is strict diagonal dominance, i.e. $|D_{ii}|>\sum_{j\neq i}|E_{ij}|$.
    \end{method}

    ?? COMPLETE (e.g. Lanczos)??

\subsection{Constrained optimization}

    A common generalization of the above optimization problems is the addition of constraints involving equalities:
    \begin{gather}
        \label{data:constrained_problem}
        \arg\min_x f(x) \qquad\text{such that}\qquad g_i(x)=0\qquad\forall 1\leq i\leq n.
    \end{gather}
    The general approach to solving such constrained problems is by extending the optimization loss:
    \begin{method}[Lagrange multipliers]\index{Lagrange!multipliers}
        Given a constrained optimization problem of the form \eqref{data:constrained_problem}, one can construct the enhanced loss function
        \begin{gather}
            \mathcal{L}(x,\lambda_1,\ldots,\lambda_n) := f(x) + \sum_{i=1}^n\lambda_ig_i(x).
        \end{gather}
        The solution to the original problem is obtained by extremizing this loss with respect to $x$ and the Lagrange multipliers $\lambda_i$ (as usual this might fail globally for nonconvex problems):
        \begin{gather}
            \begin{cases}
                \pderiv{\mathcal{L}}{x} = 0\\\\
                \pderiv{\mathcal{L}}{\lambda_i} = 0&\forall1\leq i\leq n.
            \end{cases}
        \end{gather}
    \end{method}
    The situation becomes even more interesting when one also allows constraints involving inequalities:
    \begin{gather}
        \label{data:constrained_optimization}
        \arg\min_xf(x)\qquad\text{such that}\qquad
        \begin{cases}
            g_i(x)=0&\forall 1\leq i\leq m\\
            h_j(x)\leq0&\forall 1\leq j\leq n.
        \end{cases}
    \end{gather}
    Problems of this form are called \textbf{primal optimization problems}. By defining an enhanced loss using Lagrange multipliers as before
    \begin{gather}
        \mathcal{L}(x,\alpha,\beta) := f(x) + \sum_{i=1}^m\alpha_ig_i(x) + \sum_{i=1}^n\beta_ih_i(x),
    \end{gather}
    it is not hard to see that
    \begin{gather}
        \max_{\alpha,\beta;\beta_j\geq0}\mathcal{L}(x,\alpha,\beta) =
        \begin{cases}
            \infty&\text{if a constraint is violated}\\
            f(x)&\text{if all constraints are satisfied}.
        \end{cases}
    \end{gather}
    \newdef{Primal optimization problem}{
        Denote the maximum of $\mathcal{L}(x,\alpha,\beta)$ by $\theta_P(x)$.
        \begin{gather}
            p^* := \min_x\theta_P(x) = \min_x\max_{\alpha,\beta;\beta_i\geq0}\mathcal{L}(x,\alpha,\beta).
        \end{gather}
    }
    By interchanging the max and min operators in the primal formulation, another problem is obtained:
    \newdef{Dual optimization problem}{
        \begin{gather}
            d^* := \max_{\alpha,\beta;\beta_i\geq0}\theta_D(\alpha,\beta) = \max_{\alpha,\beta;\beta_i\geq0}\min_x\mathcal{L}(x,\alpha,\beta).
        \end{gather}
    }
    From basic calculus it is known that $\max\min\leq\min\max$ and, hence, that $d^*\leq p^*$. The difference $p^*-d^*$ is called the \textbf{duality gap}\index{duality gap} and, if $d^*=p^*$, one says that \textbf{strong duality} holds. The real question then becomes: ``\textit{When does strong duality hold?}''.
    \newdef{Slater conditions}{\index{Slater!conditions}
        Consider a convex optimization problem, i.e. a problem of the form \eqref{data:constrained_optimization} where $f$ is convex, the $g_i$ are convex and the $h_j$ are affine. This problem is said to satisfy the Slater condition(s) if there exists an $x$ that is strictly \textbf{feasible}, i.e. $h_j(x)<0$ for all $1\leq j\leq n$.
    }
    \begin{property}[Strong duality]\index{optimum}
        If a convex problem satisfies the Slater conditions, strong duality holds. The solutions $x$ and $(\alpha,\beta)$ that attain this duality are called primal optima and dual optima respectively.
    \end{property}

    The following property gives a set of sufficient conditions:
    \begin{property}[Karush-Kuhn-Tucker conditions]\index{Karush-Kuhn-Tucker conditions}
        \nomenclature[A_KKT]{KKT}{Karush-Kuhn-Tucker}
        If there exist $x,\alpha$ and $\beta$ such that strong duality holds, the following conditions are satisfied:
        \begin{align}
            \begin{cases}
                \pderiv{\mathcal{L}}{x} = 0\\\\
                \pderiv{\mathcal{L}}{\alpha_i} = 0
            \end{cases}
            \quad\forall1\leq i\leq m \qquad\text{and}\qquad
            \begin{cases}
                \beta_jh_j(x) = 0\\
                h_j(x)\leq0&\quad\forall1\leq j\leq n\\
                \beta_j\geq0.
            \end{cases}
        \end{align}
        Conversely, if there exists values $x,\alpha$ and $\beta$ that satisfy the KKT conditions, they give strongly dual solutions for the primal and dual problems.
    \end{property}
    \begin{remark}[Complementary slackness]\label{data:slackness}
        The third equation in the KKT conditions has an important implication. It says that if there is an index $j$ such that the constraint $h_j$ is not \textbf{active}, i.e. $h_j(x)<0$, the associated Lagrange multiplier is 0 and, conversely, if there is an index $j$ such that the Lagrange multiplier $\beta_j>0$, the constraint $h_j$ is active.
    \end{remark}

    \remark{It is not hard to see that the KKT conditions reduce to the conditions for Lagrange multipliers when all $h_j$ are identically 0. For this reason the quantities $\alpha$ and $\beta$ are called the \textbf{KKT multipliers}.}

\section{Classification problems}
\subsection{Clustering}\index{clustering}

    Probably the most well-known and simplest algorithm for clustering in the unsupervised setting is the $k$-means algorithm:
    \begin{method}[$k$-means algorithm]
        Assume that an unlabelled dataset $\mathcal{D}\subset\mathbb{R}^n$ is given. For every integer $k\in\mathbb{N}$, usually satisfying $k\ll|\mathcal{D}|$, and any choice of $k$ distinct \textbf{centroids} $\{c_i\in\mathbb{R}^n\}_{i\leq k}$, the $k$-means algorithm is defined through the following iterative scheme:
        \begin{enumerate}
            \item To every point $d\in\mathcal{D}$ assign a cluster $C_i$ based on the following criterion:
            \begin{gather}
                i = \arg\min_{j\leq k}\|d-c_j\|^2.
            \end{gather}
            \item Update the centroids $c_i$ to represent the center of mass of the associated cluster $C_i$:
            \begin{gather}
                c_i\longleftarrow\frac{1}{|C_i|}\sum_{d\in C_i}d.
            \end{gather}
        \end{enumerate}
        This algorithm optimizes the following global cost function with respect to the centroids $c_i$:
        \begin{gather}
            \mathcal{L}_{k\text{-means}}(c_1,\ldots,c_k) = \sum_{i=1}^k\sum_{d\in C_i}\|d - c_i\|^2.
        \end{gather}
    \end{method}
    Given the above idea, one could ask for a more general algorithm where clustering is performed with respect to a divergence function \ref{info:divergence}. In the case of Bregman divergences \ref{info:bregman_divergence} it can be shown that all one needs to do is replace the Euclidean distance by the divergence $D_f$:
    \begin{property}[Centroid position]
        Let $D_f$ be a Bregman divergence. The minimizer
        \begin{gather}
            \arg\min_\kappa\sum_{i=1}^kD_f(x_i\|\kappa)
        \end{gather}
        is given by the arithmetic average
        \begin{gather}
            \kappa = \frac{1}{k}\sum_{i=1}^kx_i.
        \end{gather}
        If instead of a cluster $C=\{x_i\in\mathbb{R}^n\}_{i\leq k}$, one is given a probability distribution $p$, one simply has to replace the arithmetic average by the expectation value with respect to $p$. It can be furthermore be shown that for any Bregman divergence the $k$-means algorithm always converges in a finite number of steps (however, the clustering is not necessarily optimal).
    \end{property}

    The cluster boundaries $H(c_1, c_2)=\{x\in\mathbb{R}^n\mid D_f(x\|c_1)=D_f(x\|c_2)\}$ admit a simple geometric construction:
    \begin{property}[Cluster boundaries]\index{Voronoi diagram}
        Let $D_f$ be a Bregman divergence and consider the $k$-means problem associated to $D_f$ for $k=2$ (higher-dimensional problems can be treated similarly). The boundary $H(c_1,c_2)$ is exactly the dual geodesic hypersurface orthogonal to the affine geodesic connecting $c_1$ and $c_2$. This partitioning of the data manifold is a generalization of \textit{Voronoi diagrams} to (Bregman) divergences.\footnote{See \cite{voronoi_bregman} for more information. This is also introduced in \cite{amari}, but there the author has confusingly interchanged the affine and dual coordinates.}
    \end{property}

\subsection{Nearest neighbour search}

    ?? COMPLETE ??

\section{Garden}

    ?? ADD (e.g. trees, forests)??

\section{Support-vector machines}
\subsection{Kernel methods}

    This section will introduce the mathematics of kernel methods. This mainly involves the language of Hilbert spaces (see Chapter \ref{chapter:functional} for a refresher).

    \newdef{Kernel\footnotemark}{\index{kernel|seealso{Mercer}}\index{Mercer!kernel}\label{data:kernel}
        \footnotetext{Also called a \textbf{Mercer kernel}. See Mercer's theorem below for more information.}
        A function $k:X\times X\rightarrow\mathbb{C}$ that is (conjugate) symmetric and for which the Gram-matrix $K_{ij}:=K(x_i,x_j)$ is positive-definite for all $n\in\mathbb{N}$ and $\{x_i\in X\}_{i\leq n}$.
    }
    \newdef{Reproducing kernel Hilbert space}{\index{Hilbert!reproducing kernel}
        \nomenclature[A_RKHS]{RKHS}{reproducing kernel Hilbert space}
        A Hilbert space $\mathcal{H}\subset\mathrm{Map}(X,\mathbb{C})$ for some set $X$ for which all evaluation functionals $\delta_x:f\mapsto f(x)$ are bounded (or continuous by Property \ref{functional:bounded_continuous}). Reproducing kernel Hilbert spaces are often abbreviated as \textbf{RKHS}s.
    }
    Using the Riesz representation theorem \ref{functional:riesz} one can express every evaluation functional $\delta_x$ on a RKHS $\mathcal{H}$ as a function $K_x\in\mathcal{H}$. This allows for the introduction of a kernel on $X$:
    \newdef{Reproducing kernel}{
        Let $\mathcal{H}$ be a RKHS on a set $X$. The (reproducing) kernel $k$ on $X$ is defined as follows:
        \begin{gather}
            k(x,y) := \delta_x(K_y)\overset{\text{Riesz}}{=} \langle K_x|K_y \rangle_\mathcal{H}.
        \end{gather}
        Because $k$ is given by a metric, it is not hard to see that the reproducing kernel is a Mercer kernel \ref{data:kernel}.
    }

    Starting from a kernel one can also characterize an RKHS as follows:
    \newadef{RKHS}{
        A Hilbert space $\mathcal{H}\subset\mathrm{Map}(X,\mathbb{C})$ of functions over a set $X$ such that there exists a kernel $k$ on $X$ with the following properties:
        \begin{enumerate}
            \item\textbf{Reproducing property}: For all $x\in X, f\in\mathcal{H}$ the evaluation functional $\delta_x$ satisfies $\delta_x(f) = \langle k(\cdot,x)|f \rangle_{\mathcal{H}}$.
            \item\textbf{Density}: The span of $\{k(\cdot,x)\mid x\in X\}$ is dense in $\mathcal{H}$.
        \end{enumerate}
        The density property is often replaced by the property that $k(\cdot, x)\in\mathcal{H}$ for all $x\in X$.
    }

    \begin{property}[Convergence]
        In an RKHS, convergence in norm implies pointwise convergence.
    \end{property}

    \begin{theorem}[Moore-Aronszajn]\index{Moore-Aronszajn}
        There exists a bijection between RKHSs and kernels.

        \begin{proof}
            One direction of the theorem is, as mentioned before, rather simple to see. The other direction is constructive:
            \begin{quote}
                 Given a kernel $k$, one defines for all $x\in X$ the function $K_x:=k(\cdot, x)$. The RKHS is then constructed as the Hilbert completion of $\mathrm{span}\{k_x\mid x\in X\}$, where the inner product is defined as follows
                \begin{gather}
                    \left\langle\left.\sum_{x\in X}a_xK_x\right|\sum_{y\in X}b_yK_y\right\rangle_\mathcal{H} := \sum_{x,y\in X}\overline{a_x}b_yk(x,y).
                \end{gather}
            \end{quote}
        \end{proof}
    \end{theorem}

    \begin{formula}
        Let $\mathcal{H}$ be an RKHS with kernel $k$. If $\{e_i\}_{i\leq\dim(\mathcal{H})}$ is an orthonormal basis for $\mathcal{H}$, then
        \begin{gather}
            k(x,y) = \sum_{i=1}^{\dim(\mathcal{H})}e_i(x)\overline{e_i(y)}.
        \end{gather}
    \end{formula}

    \remark{Note that one can use different conventions in the above definitions, e.g. the definition $k(x,y)=\langle K_y|K_x\rangle_\mathcal{H}$ is also valid.}

    \begin{theorem}[Mercer]
        Let $X$ be a finite measure space and consider a (conjugate) symmetric function $k\in L^2(X\times X,\mathbb{C})$. If $k$ satisfies the \textbf{Mercer condition}
        \begin{gather}
            \iint_{X\times X}k(x,y)\overline{f(x)}f(y)dxdy\geq0
        \end{gather}
        for all $f\in L^2(X,\mathbb{C})$, the Hilbert-Schmidt operator
        \begin{gather}
            T_k:L^2(X,\mathbb{C})\rightarrow L^2(X,\mathbb{C}):f\mapsto\int_Xk(\cdot,x)f(x)dx
        \end{gather}
        admits a countable orthonormal basis $\{e_i\}_{i\in\mathbb{N}}$ with nonnegative eigenvalues $\{\lambda_i\}_{i\in\mathbb{N}}$ such that
        \begin{gather}
            k(x,y) = \sum_{i=1}^\infty\lambda_ie_i(x)\overline{e_i(y)}.
        \end{gather}
    \end{theorem}
    \begin{theorem}[Bochner]\index{Bochner}
        A continuous function satisfies the Mercer condition if and only if it is a kernel.
    \end{theorem}

    \newadef{Kernel}{
        Consider a set $X$. A function $k:X\times X\rightarrow\mathbb{C}$ is called a kernel on $X$ if there exists a Hilbert space $\mathcal{H}$ together with a function $\phi:X\rightarrow\mathcal{H}$ such that
        \begin{gather}
            k(x,y) = \langle\phi(x)|\phi(y)\rangle_\mathcal{H}.
        \end{gather}
        When using Mercer's theorem, the feature maps are given by $\phi_i:x\mapsto\sqrt{\lambda_i}e_i(x)$.
    }

\subsection{Decision boundaries}

    Consider a linear model for a classification problem $y = w^Tx + b$. The object $x_i$ is said to belong to the positive (resp. negative) class if $y>0$ (resp. $y<0$). This is implemented by the sign activation function
    \begin{gather}
        \sgn(y) =
        \begin{cases}
            1&y>0\\
            -1&y<0
        \end{cases}
    \end{gather}
    to the linear model. The \textbf{decision boundary} $y=0$, where the decision becomes ambiguous, forms a hyperplane in the feature space. However, it should be clear that in generic situations there are multiple hyperplanes that can separate the two classes for a finite number of data points. The problem then becomes to obtain the hyperplane with the maximal separation, i.e. the hyperplane for which the distance to the nearest data point is maximal.

    The unit vector $\frac{w}{\|w\|}$ defines the normal to the hyperplane and, therefore, one can obtain the distance $d(x)$ from a data point $x$ to the decision boundary by projecting onto this unit vector. The point $x - d(x)\frac{w}{\|w\|}$ is an element of the decision boundary and, hence, satisfies the hyperplane equation. Rewriting this gives an expression for the distance
    \begin{gather}
        d(x) = \frac{w^Tx + b}{\|w\|}.
    \end{gather}
    To account for the direction of the arrow, this number should be multiplied by the class $\sgn(y)=\pm1$. This result is called the \textbf{geometric margin} $\gamma(x):=\sgn(y)d(x)$. The numerator in the geometric margin is called the \textbf{functional margin}. The geometric margin is preferable since it is invariant under simultaneous scale transformations of the parameters $w,b$.

    The optimization objective now becomes
    \begin{gather}
        \max_w\frac{\gamma}{\|w\|} \qquad\text{such that}\qquad  y_i(w^Tx_i+b)\geq\gamma\|w\|\qquad\forall 1\leq i\leq n,
    \end{gather}
    where $\gamma=\min_{i\in\{1,\ldots,n\}}\gamma(x_i)$ for $x_i$ ranging over the training set. The problem is formulated in terms of the functional margin $\gamma\|w\|$ to avoid the nonconvex constraint $\|w\|=1$. This allows the application of the Slater conditions for strong duality. Since the geometric margin is invariant under scale transformations, one can without loss of generality work with the assumption $\gamma\|w\|=1$. The optimization problem is then equivalent to the following minimization problem:
    \begin{gather}
        \min_w\|w\|^2 \qquad\text{such that}\qquad y_i(w^Tx_i+b)\geq1\qquad\forall 1\leq i\leq n.
    \end{gather}
    The KKT conditions for this problem give the following results:
    \begin{gather}
        w = \sum_{i=1}^n\beta_iy_ix_i
    \end{gather}
    and
    \begin{gather}
        \sum_{i=1}^n\beta_iy_i = 0,
    \end{gather}
    where the quantities $\beta_i$ are the KKT multipliers for the affine constraints $1-y_i(w^Tx_i+b)\leq0$. Using these relations the quantity $y$ can be expressed for a new data point as follows:
    \begin{gather}
        y \equiv w^Tx + b = \sum_{i=1}^n\beta_iy_i\langle x_i|x \rangle + b.
    \end{gather}
    Two observations can be made at this point. First of all, complementary slackness \ref{data:slackness} implies that the only relevant vectors $x_i$ in this calculation are the ones that satisfy $\gamma(x_i)=0$. These are called the \textbf{support vectors} and they give their name to a class of models called \textbf{support-vector machines} (SVMs)\index{support-vector machine}\nomenclature[A_SVM]{SVM}{support-vector machine}. These are the models that are trained using the above optimization problem. Furthermore, $y$ can be written in terms of an inner product. It is exactly this last observation that allows for the generalization of the above model to nonlinear decision boundaries. The previous section showed that inner products are equivalent to (Mercer) kernels. Hence, by choosing a nonlinear kernel function, one can implicitly work with nonlinear feature maps. This is often called the \textbf{kernel trick}\index{kernel!trick}. As an example, polynomial kernels represent feature maps from $x$ to monomials in the coefficients of $x$.

    However, as often happens with data analysis algorithms, this procedure is sensitive to outliers. This is especially the case for kernels that are based on feature maps to infinite-dimensional spaces (e.g. the \textit{RBF kernel}). To solve this problem one can introduce a regularization term in the cost function. The simplest such term for support-vector machines is a simple $\ell^1$-penalty:
    \begin{gather}
        \min_w\|w\|^2 + C\sum_{i=1}^n\xi \qquad\text{such that}\qquad
        \begin{cases}
            \xi_i\geq0&\forall 1\leq i\leq n\\
            y_i(w^Tx_i+b)\geq1-\xi_i&\forall 1\leq i\leq n.
        \end{cases}
    \end{gather}
    The resulting KKT conditions are as follows:
    \begin{gather}
        0\leq\beta_i\leq C
    \end{gather}
    and
    \begin{align}
        \beta_i = 0&\implies y_i(w^Tx_i+b)\geq1\\
        \beta_i = C&\implies y_i(w^Tx_i+b)\leq1\\
        \beta_i \in\ ]0, C[&\implies y_i(w^Tx_i+b)=1
    \end{align}

    ?? COMPLETE (e.g. geometry)??

\section{Time series analysis}

    \newdef{Time series}{\index{time series}
        A $\mathbb{N}$- or $\mathbb{Z}$-indexed stochastic process. Since $\mathbb{N}$ and $\mathbb{Z}$ are isomorphic in a very simple way, the two conventions for time series will be used interchangeably.
    }

\subsection{Stationarity}

    \newdef{Strict stationarity}{\index{stationarity}
        A time series $\seq{X}$ is (strictly) stationary if for any two integers $r,s\in\mathbb{N}$, the joint distribution satisfies the following condition:
        \begin{gather}
            P(X_{t_1},\ldots,X_{t_r}) = P(X_{t_1+s},\ldots,X_{t_r+s}).
        \end{gather}
    }

    \newdef{Weak stationarity}{
        A time series $\seq{X}$ is said to be weakly (or \textbf{covariance}) stationary if it satisfies the following conditions:
        \begin{enumerate}
            \item\textbf{Mean-stationary}: $\expect{X_n} = \expect{X_0}$ for all $n\in\mathbb{N}$.
            \item\textbf{Finite covariance}: $\mathrm{cov}(X_i,X_j)<\infty$ for all $i,j\in\mathbb{N}$.
            \item\textbf{Covariance-stationary}: $\mathrm{cov}(X_i,X_{i+j}) = \mathrm{cov}(X_0,X_j)$ for all $i,j\in\mathbb{N}$.
        \end{enumerate}
    }

    The following definition is a reformulation of Birkhoff ergodicity \ref{lebesgue:ergodic}:
    \newdef{Ergodicity}{\index{ergodic!series}
        A time series $\{X_t\}_{t\in\mathbb{Z}}$ is ergodic if for every measurable function $f$ the following equation holds for all $t\in\mathbb{Z}$:
        \begin{gather}
            \lim_{T\rightarrow\infty}\frac{1}{2T+1}\sum_{k=-T}^Tf(X_k) = \expect{f(X_t)}.
        \end{gather}
        Intuitively this means that state space averages can be evaluated as time averages.
    }

\subsection{Correlation}

    \newdef{Autocorrelation function}{\index{autocorrelation}\index{correlation|seealso{autocorrelation}}
        Consider a time series $\seq{X}$. The autocovariance (resp. autocorrelation) function of this time series is defined as the covariance (resp. autocorrelation) function of the random variables $\seq{X}$.
    }

    \newdef{Spectral density}{\index{spectral!density}\index{memory}
        Consider a (weakly) stationary time series $\seq{X}$. If the associated autocovariance is in $\ell^1$, one can define the spectral density as the discrete Fourier transform of the autocovariance function:
        \begin{gather}
            f(\omega) = \frac{1}{2\pi} \sum_{k=-\infty}^\infty\gamma(k)e^{i\omega k},
        \end{gather}
        where $\gamma(k)$ is the autocovariance function at lag $k$.

        Under the assumption that the spectral density exists, the time series is said to have \textbf{short memory} if $f(0)$ is finite. Otherwise the series is said to have \textbf{long memory}.
    }

    \newdef{Lag operator\footnotemark}{\index{lag}\index{backshift|see{lag}}
        \footnotetext{Also called the \textbf{backshift operator}.}
        The lag operator sends a variable in a time series to the preceding value:
        \begin{gather}
            BX_t = X_{t-1}.
        \end{gather}
        An important concept, especially in the context of autoregressive models, is that of a \textbf{lag polynomial} (the notation for these is not completely fixed in the literature, but the $\theta$-notation is a common choice):
        \begin{align}
            \theta(B) &= 1 + \sum_{i=1}^k\theta_iB^i\\
            \varphi(B) &= 1 - \sum_{i=1}^k\varphi_iB^i.
        \end{align}
    }
    \newnot{Difference operator}{\index{difference}
        The difference operator $\Delta$ is defined as follows:
        \begin{gather}
            \Delta = 1 - B.
        \end{gather}
        In a similar way one can define the \textbf{seasonal} difference operator:
        \begin{gather}
            \Delta_s = 1 - B^s.
        \end{gather}
    }

    \begin{method}[Ljung-Box test]\index{test!Ljung-Box}
        A test to see if a given set of autocorrelations of a time series is different from zero. Consider a time series of $n$ elements and let $\{\rho_i\}_{1\leq i\leq k}$ be the first $k$ lagged autocorrelation functions. The test statistic is defined as
        \begin{gather}
            Q = n(n+2)\sum_{i=1}^k\frac{\rho_k}{n-k}.
        \end{gather}
        If the null hypothesis ``there is no correlation'' is true, the $Q$-statistic will asymptotically follow a $\chi^2$-distribution with $k$ degrees of freedom.
    \end{method}

    \begin{method}[Augmented Dickey-Fuller test]\index{test!Dickey-Fuller}
        Consider a time series $\tseq{X}$. The (augmented) Dickey-Fuller test checks if the time series is (trend) stationary. For this test one considers the following regression model (similar to the ARIMA\textit{-models} discussed in the next section):
        \begin{gather}
            \Delta X_t = \alpha + \beta t + \gamma X_{t-1} + \sum_{i=1}^{p-1} \theta_i\Delta X_{t-i} + \varepsilon_t.
        \end{gather}
        The test statistic is
        \begin{gather}
            \mathrm{DF} = \frac{\gamma}{\mathrm{SE}(\gamma)},
        \end{gather}
        where $\mathrm{SE}$ denotes the standard error. The null hypothesis states that $\gamma=0$, i.e. there is a \textit{unit root} $(1-B)$ present in the model. Comparing the test statistic to tabulated critical values will give an indication whether to reject the hypothesis or not (the more negative the statistic, the more significant the result).
    \end{method}

\subsection{Autoregressive models}

    \newdef{$\mathrm{AR}(p)$-model}{\index{autoregressive model}
        Consider a time series $\tseq{X}$. The autogressive model of order $p$ is defined as the multiple linear regression model of $X_t$ with respect to the first $p$ lagged values $X_{t-1},\ldots,X_{t-p}$ of the time series:
        \begin{gather}
            X_t = \beta_0 + \beta_1X_{t-1} + \cdots + \beta_pX_{t-p} + \varepsilon_t.
        \end{gather}
    }

    \newdef{Partial autocorrelation function}{
        The $p^{th}$ autocorrelation function is defined as the $p^{th}$ coefficient in the $\mathrm{AR}(p)$-model.
    }
    \remark{The optimal order $p$ of an autoregressive model is the one for which all higher partial autocorrelation functions (almost) vanish.}

    \newdef{$\mathrm{MA}(p)$-model}{\index{moving average}
        Consider a time series $\tseq{X}$ where every $X_t$ contains a white noise contribution $\varepsilon_t\sim\mathcal{N}(0,\sigma^2)$. The moving average model of order $p$ is defined as the multiple linear regression model of $X_t$ with respect to the first $p$ lagged values $\varepsilon_{t-1},\ldots,\varepsilon_{t-p}$ of the error term:
        \begin{gather}
            X_t = \beta_0 + \beta_1\varepsilon_{t-1} + \cdots + \beta_p\varepsilon_{t-p} + \varepsilon_t.
        \end{gather}
        Since the error terms are assumed to have mean zero, one can see that the intercept term $\beta_0$ gives the mean of the time series.
    }
    \remark{The optimal order $p$ of an autoregressive model is the one for which all higher autocorrelation functions (almost) vanish.}

    \newdef{Invertibility}{\index{invertibility}\index{stationarity}
        An $\mathrm{MA}(q)$-model is said to be invertible if all roots of its associated lag polynomial $\theta(B)$ lie outside the unit circle. This condition implies that the polynomial is invertible, i.e. $1/\theta(B)$ can be written as a convergent series in the operator $B$. This in turn implies\footnote{Sometimes this is used as a definition of invertibility.} that one can write the $\mathrm{MA}(q)$-model as an $\mathrm{AR}(p)$-model, where possibly $p=\infty$. The analogous property for $\mathrm{AR}(p)$-models leads to a definition of \textbf{stationarity}.
    }

    In practice it is not always possible to describe a data set using either an autoregressive or a moving average model. However, these two types of models can be combined:
    \newdef{$\mathrm{ARMA}(p,q)$-model}{
        \nomenclature[A_ARMA]{ARMA}{autoregressive moving-average model}
        \begin{gather}
            X_t = \alpha_0 + \sum_{i=1}^p\alpha_iX_{t-i} + \sum_{j=1}^q\beta_j\varepsilon_{t-j} + \varepsilon_t
        \end{gather}
        As above, one can find the optimal values for $p$ and $q$ by analyzing the autocorrelation and partial autocorrelation functions.
    }

    Using the lag polynomials one can rewrite the $\mathrm{ARMA}(p,q)$-model as follows:
    \begin{gather}
        \varphi(B)X_t = \alpha_0 + \theta(B)\varepsilon_t.
    \end{gather}
    By considering the special case where the polynomial $\mathcal{B}^-_\alpha$ has a unit root $1-B$ with multiplicity $d$, one can obtain a generalization of the model:
    \begin{gather}
        \varphi(B)(1-B)^dX_t = \alpha_0 + \theta(B)\varepsilon_t.
    \end{gather}
    The interpretation of this additional factor $(1-B)^d$ is related to the stationarity of the time series. The operator $1-B$ is a finite difference operator:
    \begin{align*}
        (1-B)X_t &= X_t - X_{t-1}\\
        (1-B)^2X_t &= (X_t-X_{t-1}) - (X_{t-1}-X_{t-2})\\
        &\cdots
    \end{align*}
    By successive applications, one can obtain a stationary time series from a nonstationary time series. This combination of differencing, autoregression and moving averages is called the \textbf{ARIMA}-model\footnote{The 'I' stands for ``integrated''.}.

    \remark{Including so-called \textit{exogenous} variables, i.e. external predictors, leads to an \textbf{ARIMA\underline{X}}-model.}

    \begin{remark}[Fitting AR- and MA-models]
        As is clear from the definition of an $\mathrm{AR}(p)$-model, the parameters $\theta_i$ can easily be found using standard techniques for multivariate linear regression such as ordinary least squares. However, in contrast to AR-models where the predictors are known, the estimation of coefficients in MA-models is harder since the error terms $\varepsilon_t$ are by definition unknown.
    \end{remark}
    To estimate the coefficients in a MA-model, people have introduced multiple techniques (see for example \cite{MA_fit}). One of the most famous ones is the method by \textit{Durbin}:
    \begin{method}[Durbin]\index{Durbin method}
        By restricting to invertible $\mathrm{MA}(q)$-models (or by approximating a noninvertible model by an invertible one), one can first fit an $\mathrm{AR}(p)$-model with $p>q$ to obtain estimates for the errors $\varepsilon_t$ and then, in a second step, use a least squares-method to solve for the coefficients in the MA-model.
    \end{method}

    As a last modification one can introduce seasonal components. Simple trends such as a linear growth are easily removed from the time series by detrending or differencing. However, a periodic pattern is harder to remove and, in general, ARIMA-models are not suited to accompany this type of features. Luckily one can easily modify the ARIMA-model to incorporate seasonal variations. The multiplicative SARIMA-model is obtained by inserting operators similar to the ones of the ordinary ARIMA-model, where the lag operator $B$ is replaced by the seasonal lag operator $B^s$ (where $s$ is the period of the seasonal variation):
    \newdef{$\mathrm{ARIMA}(p,q,d)(P,Q,D)_s$-model}{
        \begin{gather}
            \Phi(B^s)\varphi(B)\Delta_s^D\Delta^dX_t = \theta(B)\Theta(B^s)\varepsilon_t
        \end{gather}
    }

\subsection{Causality}

    \newdef{Granger causality}{\index{causality!Granger}
        Consider two time series $\seq{X}$ and $\seq{Y}$. The time series $X_n$ is said to Granger-cause $Y_n$ if past values of $X_n$ help to predict future values of $Y_n$. More formally this can be stated as follows:
        \begin{gather}
            P[Y_{t+k}\in A\mid\Omega(t)]\neq P[Y_{t+k}\in A\mid\Omega\backslash X(t)]
        \end{gather}
        for some $k$, where $\Omega(t)$ and $\Omega\backslash X(t)$ denote the available information at time $t$ with and without removing the variable $X$ from the universe.

        This formulation of causality was introduced by \textit{Granger} under the following two assumptions:
        \begin{itemize}
            \item The cause always happens prior to the effect.
            \item The cause carries unique information about the effect.
        \end{itemize}
    }
    \remark{A slightly different but for computational purposes often more useful\footnote{In fact this was the original definition by \textit{Granger}.} notion of Granger-causality is as follows. A time series $\seq{X}$ is said to \textbf{Granger-cause} a time series $\seq{Y}$ if the variance of predictions of $Y_n$ becomes smaller when the information contained in $X_n$ is taken into account.}

    \remark{Assume that two uncorrelated models giving predictions of a time series are given. One way to check if they have the same accuracy is the \textit{Diebold-Mariano test}. However, when testing for Granger-causality one should pay attention. This test is not valid for nested models and, hence, is not applicable to two models that only differ by a set of extra predictors (in this case an external time series).\index{test!Diebold-Mariano}}

\section{Prediction regions}

    To characterize the quality of prediction regions one can use different measures. One of the truly probabilistic notions is that of validity:
    \newdef{Validity}{\index{validity}
        Consider a region predictor $C^\alpha$ at confidence level $\alpha\in[0,1]$. Two types of (marginal) validity can be defined:
        \begin{itemize}
            \item Conservative validity:
                \begin{gather}
                    P(y_\mathrm{true}\in C^\alpha)\geq 1-\alpha.
                \end{gather}
            \item Exact validity:
                \begin{gather}
                    P(y_\mathrm{true}\in C^\alpha) = 1-\alpha.
                \end{gather}
        \end{itemize}
    }
    In practice one will always prefer models that are as exact as possible.

\subsection{Conformal prediction}

    A very general framework for the construction of valid prediction intervals in a model-independent manner is given by the conformal prediction framework by \textit{Vovk et al}. The main ingredients for the construction are randomization and \textit{conformity measures}.

    The first step will be studying the behaviour under randomization of the existing data (be it measurements or past predictions). To ensure that the procedure satisfies the required confidence (or probability) bounds, one has to make some assumptions. One of the main benefits of this framework is that one can relax the condition of the data being i.i.d. to it being exchangeable:
    \newdef{Exchangeability}{\index{exchangeability}\index{bag}
        Consider an ordered data sample $(z_i)_{1\leq i\leq N}$. The joint distribution $P(z_1,\ldots,z_N)$ is said to be exchangeable if it is invariant under any permutation of the data points. A generalization for infinite data sets is obtained by requiring exchangeability of any finite subset.

        This definition can be restated in a purely combinatorial way. First, define the notion of a the \textbf{bag} obtained from the (ordered) data sample $(z_i)_{1\leq i\leq N}$ as the (unordered) set $\mathcal{B}$ containing these elements. The joint distribution $P$ is then said to be exchangeable if the probability of finding any sequence of data points is equal to the probability of drawing this same sequence from the bag of these elements. Since this probability is purely combinatorial and, hence, completely independent of the ordering, it should be clear that this coincides with the first definition.
    }

    \newdef{Nonconformity measure}{\index{measure!conformity}
        Consider a bag of elements $\mathcal{B}$ together with a new element $z^*$. A noncomformity measure is a function that gives a number indicating how different $z^*$ is from the content of $\mathcal{B}$. Although this definition does not state specific requirements for the functions, it should be obvious that the better the function detects dissimilarities, the better the resulting prediction regions will be.
    }
    \sremark{One could restate everyhting in terms of ``conformity measures'' and, hence, look at similarities instead of dissimilarities. It will become clear that the procedure is invariant under monotone transformations and, hence, everything can be multiplied by $-1$.}
    \begin{example}[Point predictors]
        A general class of nonconformity measures is obtained from point predictors. Given a point predictor $\rho$ that takes a bag $\mathcal{B}$ as input, one can define a nonconformity measure as follows:
        \begin{gather}
            A_\rho(\mathcal{B},z^*) := d(\rho(\mathcal{B}),z^*),
        \end{gather}
        where $d:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}$ is any distance function (often chosen to be the Euclidean distance).
    \end{example}

    ?? FORMALIZE MATHEMATICS ??

    \begin{construct}[Conformal predictor]\label{data:cp}
        Consider a data sample given as a bag $\mathcal{B}$ together with a nonconformity measure $A$. Let $\alpha$ denote the confidence level of the prediction region to be constructed. For any new element $z^*$ the algorithm proceeds as follows:
        \begin{enumerate}
            \item Denote the nonconformity $A(\mathcal{B},z^*)$ by $\mu^*$.
            \item For every element $z$ in $\mathcal{B}$, define $\mu_z$ by replacing $z$ by $z^*$ in the bag and calculating the nonconformity.
            \item Denote the fraction of elements $z$ of $\mathcal{B}$ for which $\mu_z\geq\mu^*$ by $p^*$.
            \item Include an element $z^*$ in the prediction region $C^\alpha$ if $p^*>\alpha$.
        \end{enumerate}
        It should be noted that in general the construction of these regions can be quite time-consuming. For low-dimensional regions it can often be achieved by solving inequalities derived from the specific form of the nonconformity measure.
    \end{construct}

    \begin{property}[Optimality]
        Given any confidence predictor satisfying the three properties below\footnote{Conformal predictors always satisfy these conditions.}, there exists a conformal predictor that is more efficient, i.e. produces smaller prediction regions:
        \begin{itemize}
            \item Ordering is irrelevant, i.e. the procedure only depends on the bag of prior elements.
            \item Regions are (conservatively) valid, i.e. $P(z^*\in C^\alpha)\geq1-\alpha$ and $P(p^*\leq\alpha)\leq\alpha$.
            \item Regions are nested, i.e. $\alpha\leq\alpha'\implies C^\alpha\subseteq C^{\alpha'}$.
        \end{itemize}
    \end{property}

    Now, one could wonder if the assumption of exchangeability is a realistic assumption. Obviously if one applies the procedure to independent observations, everything is fine (i.i.d. sequences are clearly exchangeable). However, some important sequences of data are clearly not exchangeable. The main example being time series. This kind of data often has intrinsic autocorrelation and, hence, the exchangeability assumption is almost always violated. However, a solution exists. One can restate the construction above using an explicit randomization as is done in \cite{cp_time_series}. There one replaces the nonconformity measure by a function that acts on ordinary sequences instead of unordered bags. The fraction $p^*$ can then be expressed as follows:
    \begin{gather}
        p^* = \frac{1}{|S_{N+1}|}\sum_{\sigma\in S_{N+1}}\mathbbm{1}(A(\sigma\cdot\vector{z})\geq A(\vector{z})),
    \end{gather}
    where $\vector{z}\equiv(z_1,\ldots,z_N,z^*)$. Using this language of explicit permutations one can generalize the construction to arbitrary randomization schemes, i.e. to subgroups of $S_{N+1}$. However, in general this will ruin the validity of the procedure.

    ?? FINISH ??

    At last a computationally efficient modification of the original CP algorithm is introduced. If the distance with respect to a point predictor is chosen as the nonconformity measure in Construction \ref{data:cp}, one has to retrain the predictor for every bag in step 2. For most applications, especially those in machine learning and big data, this leads to considerable computational overhead. To overcome this issue \textit{Papadopoulos et al.} introduced the following modification:
    \begin{construct}[Inductive CP]
        First, split the full dataset $\mathcal{D}$ into a training set $\mathcal{T}$ and a calibration set $\mathcal{C}$. Using $\mathcal{T}$ train the point predictor $\rho\equiv\rho(\mathcal{T})$. Then, for every point $z$ in $\mathcal{C}$, construct the nonconformity measure $\mu_z = d\big(\rho(z),z^*\big)$. For a new observation $z^*$ denote the fraction of points in $\mathcal{C}$ for which the nonconformity measure is larger than the one for $z^*$ by $p^*$. As in the original CP algorithm a new observation $z^*$ is included in the prediction region if $p^*>\alpha$.
    \end{construct}
    It is clear that the underlying point predictor only needs to be trained once. An easy way to proceed is to order all the nonconformity measures for $\mathcal{C}$ and look at the $\alpha$-quantile. This will be the critical value determining the prediction region $C^\alpha$.

    \begin{remark}
        The name ``inductive CP'' stems from the fact that the general behaviour is deduced from a small subset of all observations. For this reason one sometimes calls the original algorithm a ``transductive'' method.
    \end{remark}

    The above off-line ICP algorithm can be generalized to an on-line algorithm:
    \begin{construct}[On-line ICP]
        Consider an increasing sequence of positive integers $\seq{m}$ such that for every ``update threshold'' $m_k$ a calibration set \[\mathcal{C}_k:=\{(x_1,y_1),\ldots,(x_{m_k},y_{m_k})\}\] is given. The confidence region $C^\alpha(\mathcal{S})$ for the data sample $\mathcal{S}:=\{(x_1,y_1),\ldots,(x_n,y_n)\}$ is defined as follows:
        \begin{itemize}
            \item If $n\leq m_1$, use a fixed conformal predictor to construct $C^\alpha(\mathcal{S})$.
            \item If $n>m_1$, find the integer $k$ such that $m_k<n\leq m_{k+1}$ and construct $C^\alpha(\mathcal{S})$ as follows:
            \begin{gather}
                C^\alpha(\mathcal{S}) := \left\{y\in Y\,\middle\vert\,\frac{|\{m_k<j\leq n:\alpha_j\geq\alpha_n\}|}{n-m_k}>\varepsilon\right\},
            \end{gather}
            where
            \begin{align*}
                \alpha_j &:= A\big(\mathcal{B}(\mathcal{C}_k),(x_j, y_j)\big)\\
                \alpha_n &:= A\big(\mathcal{B}(\mathcal{C}_k),(x_n, y)\big).
            \end{align*}
        \end{itemize}
        It is clear that for $k\ll|\mathcal{C}|$ the off-line ICP algorithm approximates the on-line version.
    \end{construct}

    \begin{property}[Validity]
        It can be shown that the on-line ICP algorithm is conservative. Because of the remark about off-line ICP above, the off-line version produces approximately conservative prediction regions.
    \end{property}

    ?? COMPLETE ??
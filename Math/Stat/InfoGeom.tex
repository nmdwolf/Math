\chapter{\difficult{Information Geometry}}\label{chapter:info}

    The main reference for this chapter is \cite{amari}. For more information on differential geometry, see Chapter \ref{chapter:manifolds} and onwards.

\section{Statistical manifolds}

    In this section an important subclass of Riemannian manifolds that admit two related flat connections will be introduced. These manifolds will formalize the geometric backbone of many statistical concepts and methods.
    \newdef{Conjugate connections}{\index{connection!conjuagte}
        Consider a Riemannian manifold $(M,g)$ with an affine connection $\nabla$. The conjugate (or dual) connection $\widetilde{\nabla}$ is uniquely defined by the following equation:
        \begin{gather}
            X\left(g(Y,Z)\right) = g(\nabla_XY,Z) + g(Y,\widetilde{\nabla}_XZ)\,,
        \end{gather}
        where $X,Y,Z\in TM$. Moreover, this construction is involutive:
        \begin{gather}
            \overset{\approx}{\nabla}=\nabla.
        \end{gather}
    }

    \begin{property}
        Consider a pair of conjugate connections $\nabla,\widetilde{\nabla}$ on a Riemannian manifold $(M,g)$ and denote their parallel transport maps by $\mathcal{P}$ and $\mathcal{P'}$, respectively. Although the metric is in general not preserved under either $\mathcal{P}$ or $\mathcal{P}'$, it is preserved under conjugate (or dual) transport:
        \begin{gather}
            g(v,w) = g\left(\mathcal{P}_\gamma v,\widetilde{\mathcal{P}}_\gamma w\right)
        \end{gather}
        for every smooth path $\gamma$.
    \end{property}

    \begin{property}
        Consider two conjugate connections $\nabla,\widetilde{\nabla}$ on a Riemannian manifold $(M,g)$. The connection
        \begin{gather}
            \overline{\nabla} := \frac{\nabla+\widetilde{\nabla}}{2}
        \end{gather}
        is metric(-preserving), i.e.~$\overline{\nabla}g=0$. Furthermore, if both $\nabla$ and $\widetilde{\nabla}$ are torsion-free, then $\overline{\nabla}$ necessarily coincides with the Levi-Civita connection of $g$ by Theorem \ref{riemann:unique}.
    \end{property}

    The above properties lead to the following definition:
    \newdef{Statistical manifold}{\index{manifold!statistical}\index{Codazzi condition}\index{Amari-Chentsov tensor}
        A Riemannian manifold $(M,g)$ equipped with an affine connection that satisfies the \textbf{Codazzi condition}
        \begin{gather}
            \nabla_Xg(Y,Z) = \nabla_Yg(X,Z)\,,
        \end{gather}
        i.e.~$\nabla g$ is totally symmetric. The Codazzi condition implies vanishing torsion and vice versa. The rank-3 tensor $T:=\nabla g$ is sometimes called the \textbf{cubic tensor} or \textbf{Amari-Chentsov} tensor. In local coordinates the cubic tensor gives the difference between the Christoffel symbols of $\nabla$ and $\widetilde{\nabla}$:
        \begin{gather}
            T_{ijk} = \widetilde{\Gamma}_{ijk} - \Gamma_{ijk}.
        \end{gather}
        In the case where $\nabla$ has nonvanishing torsion, one can generalize this definition by also relaxing the Codazzi equation:
        \begin{gather}
            \nabla_Xg(Y,Z)-\nabla_Yg(X,Z) = -g\big(T^\nabla(X,Y),Z\big).
        \end{gather}
        If this equation is satisfied for all $X,Y$ and $Z\in TM$, the dual connection is torsion-free and the tuple $(M,g,\nabla)$ is called a \textbf{statistical manifold admitting torsion}.\footnote{This situation arises in the study of quantum systems and density operators.} The existence of a torsion-free connection is sufficient to turn a (pseudo-)Riemannian manifold into a statistical manifold admitting torsion.
    }
    \begin{remark}
        One can show that the above definition is equivalent to that of a Riemannian manifold admitting a totally symmetric rank-3 tensor.
    \end{remark}

    \newdef{Dually flat manifold}{\index{manifold!flat}\label{info:dually_flat}
        Consider a statistical manifold $(M,g,\nabla)$. If $\nabla$ is flat, its conjugate $\widetilde{\nabla}$ is also flat and the tuple $(M,g,\nabla,\widetilde{\nabla})$ is called a dually flat manifold.
    }

    Because the affine connections are flat, they endow the manifold with an \textit{affine structure}, i.e.~there exist coordinate charts such that the coordinate-induced vector fields satisfy
    \begin{gather}
        \nabla_{\partial_i}\partial_j = 0
    \end{gather}
    for all $i,j\leq n$ and such that the transition functions are affine transformations. It can be shown that the conjugate connection induces a similar $\widetilde{\nabla}$-affine coordinate chart such that the coordinate-induced vector fields satisfy the following orthonormality condition:
    \begin{gather}
        g\left(\pderiv{}{x^i},\pderiv{}{y_j}\right) = \delta^j_i.
    \end{gather}
    This coordinate system is called the \textbf{dual (coordinate) system}.

\subsection{Divergences}

    \newdef{Divergence}{\index{divergence}\label{info:divergence}
        Let $M$ be a set. A smooth function $D(\cdot\|\cdot):M\times M\rightarrow\mathbb{R}$ with the following properties is called a divergence (measure) on $M$:
        \begin{enumerate}
            \item\textbf{Positivity}: $D(p\|q) \geq 0$ for all $p,q\in M$, and
            \item\textbf{Nondegeneracy}: $D(p\|q)=0$ if and only if $p=q$.
        \end{enumerate}
        The \textbf{dual divergence} $D^*$ is defined by
        \begin{gather}
            D^*(p\|q):=D(q\|p)
        \end{gather}
        for all $p,q\in M$.
    }
    \begin{property}[Induced metric]
        An interesting feature of these functions is that one can use their Hessians (with respect to either of the two arguments) to construct a Riemannian metric if $M$ is a smooth manifold:
        \begin{gather}
            g_{ij}(\theta) := \left.\frac{\partial^2D}{\partial p^i\partial p^j}(p||q)\right|_{p=q=\theta} = \left.\frac{\partial^2D}{\partial q^i\partial q^j}(p||q)\right|_{p=q=\theta} = -\left.\frac{\partial^2D}{\partial p^i\partial q^j}(p||q)\right|_{p=q=\theta}.
        \end{gather}
    \end{property}

    \begin{example}[$f$-divergences and $\alpha$-divergences]\index{divergence!measure}
        Let $f$ be a smooth convex function such that $f(1)=0$ and let $p,q$ be two probability distributions such that $p$ is absolutely continuous with respect to $q$. The $f$-divergence is defined as follows:
        \begin{gather}
            D_f(p\|q) := \int_\Omega f\left(\deriv{p}{q}\right)\,dq.
        \end{gather}
        In the case where both $p$ and $q$ are absolutely continuous with respect to some given measure $\mu$ (with Radon-Nikodym derivatives $g,h$), one can rewrite the above formula as
        \begin{gather}
            D_f(p\|q) = \int_\Omega h(x)f\left(\frac{g(x)}{h(x)}\right)\,d\mu(x).
        \end{gather}
        It is not hard to see that $f$-divergences remain invariant under affine transformations of the form
        \begin{gather}
            f(x)\longrightarrow f(x) + c(x-1)\,,
        \end{gather}
        where the shift $x-1$ is necessary to preserve the condition $f(1)=0$. This implies that one can always choose $f'(1)=0$ without loss of generality. Moreover, one can also easily see that these divergences transform linearly under scale transformations and, hence, one can also always choose $f''(1)=1$. $f$-divergences with these properties are said to be \textbf{standard}.

        A particular class of $f$-divergences are the $\alpha$-divergences (in the sense of \textit{Csisz\'ar}\footnote{\textit{Tsallis} and \textit{R\'enyi} introduced different divergences/entropies with the same name.}) where
        \begin{gather}
            f_\alpha(x) = \frac{1-x^\alpha}{\alpha(1-\alpha)}.
        \end{gather}
        Note that some authors replace $(1-x^a)$ by $(x-x^a)$ since this does not make any difference when calculating the divergence for normalized distributions. For the cases $\alpha=0,1$ one can use a workaround. For $\alpha=0$ one can take the limit of the above definition to obtain $f_0(x) = -\ln x$. This results in $D_0(p\|q) = D_\mathrm{KL}(q\|p)$. For $\alpha=1$ one can look at the general expression of $D_\alpha$ and notice that it is invariant under the simultaneous exchanges $(\alpha\leftrightarrow1-\alpha)$ and $(p\leftrightarrow q)$. Using this trick one can see that $D_1(p\|q) = D_\mathrm{KL}(p\|q)$.
    \end{example}

    \begin{definition}[Bregman divergence]\index{Bregman divergence}\label{info:bregman_divergence}
        Let $F:\mathbb{R}^n\rightarrow\mathbb{R}$ be a convex function. Because the function is convex, at every point $q\in\mathbb{R}^n$ the tangent plane to the graph of $F$ is a supporting hyperplane, i.e.~it lies underneath the graph everywhere and it touches the graph only at $q$. Using this hyperplane one can define the Bregman divergence as follows:
        \begin{gather}
            D_F(p\|q) := F(p) - F(q) - \vec{\nabla}F(q)\cdot(p-q)\,,
        \end{gather}
        where the gradient is denoted by $\vec{\nabla}$ to avoid confusion with further occurences of the $\nabla$-symbol for affine connections. This function gives the difference in ``height'' between the function value at $p$ and the position of the tangent plane (defined by $q$) at $p$. Because the gradient of a convex function is monotonic, this difference will always increase the further the points are spread apart. (For convex functions this will in general only be nondecreasing. However, in the remainder of this chapter strict convexity will almost always be assumed.)
    \end{definition}
    \begin{example}[Kullback-Leibler divergence]\index{Kullback-Leibler divergence}
        The Kullback-Leibler divergence \ref{prob:kullback_leibler} can be obtained as the Bregman divergence associated to the (negative) Shannon entropy $F(\rho) := \sum_{i=1}^n\rho_i\ln\rho_i$. It is also equal to the $f$-divergence associated to the choice $f=x\ln(x)$.
    \end{example}

    A Bregman divergence can also be used to endow the underlying manifold with further structure. By restricting to strictly convex functions, i.e.~requiring that the Hessian is positive-definite, one can perform a Legendre transformation \ref{calculus:legendre} to obtain a new function:
    \begin{gather}
        \label{info:legendre}
        \widetilde{F}(x^*) := x^*\cdot y - F(y)\,,
    \end{gather}
    where $x^*=\vec{\nabla}F(y)$.\footnote{For general convex functions this relation is not necessarily invertible.} It can be shown that $\widetilde{F}$ is again (strictly) convex and, hence, also defines a Bregman divergence. This second Bregman divergence coincides with the dual divergence $D_F^*$:
    \begin{gather}
        D_F(p\|q) = D_{\widetilde{F}}(q^*\|p^*).
    \end{gather}
    Using this relation one can also rewrite the original expression for the Bregman divergence:
    \begin{gather}
        D_F(p\|q) = F(p) + \widetilde{F}(q) - x^i(p)y_i(q).
    \end{gather}
    Now, the two convex functions $F,\widetilde{F}$ define two coordinate systems that are related as follows:
    \begin{gather}
        y=\vec{\nabla}F\qquad\text{and}\qquad x=\vec{\nabla}\widetilde{F}(y).
    \end{gather}
    However, convexity of functions is not preserved under arbitrary coordinate transformations and, hence, one should restrict the class of allowed coordinate transformations. To preserve convexity only affine transformations are allowed. In affine coordinates one can express any geodesic, i.e.~any path $\gamma$ such that $\nabla_{\dot{\gamma}}\dot{\gamma}=0$, as a straight line:
    \begin{gather}
        \gamma_{q\rightarrow p}(t) \equiv tx(p) + (1-t)x(q).
    \end{gather}
    Geodesics for the conjugate connection are called \textbf{dual geodesics}. It is important to note that the Legendre transform that maps the primary coordinates to the dual coordinates is in general not an affine transformation and, hence, does not preserve the dual structure. Moreover, it can be shown that neither of the parallel transport maps, although completely trivial due to the affine structure, are metric-preserving. However, parallel transporting one vector by $\nabla$ and the other by $\widetilde{\nabla}$ does preserve the metric. The metric structures induced by the Hessians are also intertwined:
    \begin{gather}
        \label{info:bregman_metric}
        g_{ij} = \frac{\partial^2F}{\partial x^i\partial x^j}\qquad\text{and}\qquad \widetilde{g}^{\,ij} = \frac{\partial^2\widetilde{F}}{\partial y_i\partial y_j}
    \end{gather}
    are mutual inverses. It can be concluded that a Bregman divergence endows a set with the structure of a dually flat manifold \ref{info:dually_flat}.

    \begin{example}[Euclidean distance]
        On $\mathbb{R}^n$ the most common choice of divergence measure is the Euclidean distance:
        \begin{gather}
            D_\mathrm{eucl}(p\|q) := \frac{1}{2}\|p-q\|.
        \end{gather}
        It is not hard to show that this function is in fact self-dual with respect to Legendre transformations. This also implies that the primary and dual structures on $\mathbb{R}^n$ (with respect to the Euclidean distance) coincide. The associated connections are equal to the (trivial) Levi-Civita connection.
    \end{example}

    \begin{property}[Bregman divergence]
        The dually flat structure on a dually flat manifold $(M,g,\nabla,\widetilde{\nabla})$ enables one to define two affine coordinate systems through two functions $\psi,\phi$ (called \textbf{potentials}). Because the connection $\nabla$ is torsion-free and the metric is symmetric by definition, a function $\psi$ can (locally) be found such that
        \begin{gather}
            g_{ij} = \partial_i\partial_j\psi.
        \end{gather}
        The positive-definiteness of $g$ further implies that $\psi$ is convex. This implies that $\psi$ can be used to define a Bregman divergence. The induced dually flat structure is exactly $(M,g,\nabla,\widetilde{\nabla})$.
    \end{property}

\subsection{Exponential families}

    The primary and dual affine geodesics are often given the names \textit{e-geodesic} and \textit{m-geodesic} in the literature. In this and the following section, this terminology is explained.

    \newdef{Exponential family}{\index{distribution!exponential}
        Let $X:\Omega\rightarrow\mathbb{R}^k$ be a random variable. For every integer $n\in\mathbb{N}$, every collection of smooth functions $\{h_i:\mathbb{R}^k\rightarrow\mathbb{R}\}_{1\leq i\leq n}$ and any smooth function $\lambda:\mathbb{R}^k\rightarrow\mathbb{R}$ one can define the following family of distributions indexed by some parameter $\theta\in\mathbb{R}^n$:
        \begin{gather}
            p(X;\theta) := \exp\big(h_i(X)\theta^i + \lambda(X) - \psi(\theta)\big).
        \end{gather}
        The function $\psi(\theta)$ is introduced as a normalization function:
        \begin{gather}
            \label{info:free_energy}
            \psi(\theta) := \ln\int\exp(h_i(X)\theta^i)e^{\lambda(X)}\,d\mu(X).
        \end{gather}
        The function $\lambda$ can be removed through a redefinition of the measure: \[d\mu(X)\longrightarrow d\nu(X):=\exp^{\lambda(X)}\,d\mu(X).\]
    }
    \begin{remark}\index{cumulant}\index{energy!free}
        The function $\psi$ is actually the cumulant-generating function (or free energy in physics terminology) of the sufficient statistics $h_i(X)$.
    \end{remark}

    Such a family of exponential distributions forms a manifold with affine coordinates $\theta^i$ (these are also called the \textbf{natural parameters}). The dual coordinates $\nabla\psi(\theta)$ are the expectation values $\expect{X}$ and the associated dual convex function $\phi$ is the Shannon entropy. Accordingly, the Bregman divergence associated to $\psi$ is given by the \underline{dual} Kullback-Leibler divergence:
    \begin{gather}
        \label{info:KL_reversal}
        D_\psi(\theta\|\theta') = D_\mathrm{KL}(\theta'\|\theta).
    \end{gather}
    The metric induced by this structure is the Fisher information:\index{Fisher!information}
    \begin{gather}
        g_{ij} = \mathcal{I}_{ij}[X;\theta] := \text{E}\left[\left(\pderiv{}{\theta^i}\ln p(X;\theta)\right)\left(\pderiv{}{\theta^j}\ln p(X;\theta)\right)\right].
    \end{gather}
    Now, consider an affine combination of natural parameters, hence an affine geodesic in the manifold of an exponential family: \[\theta(t) := t\theta_2 + (1-t)\theta_1.\] The probability distributions along this path can themselves be interpreted as constituting an exponential family with natural parameter $t$ and, therefore, one calls the primary geodesic $\theta(t)$ an \textbf{e-geodesic} ('e' for exponential).

\subsection{Mixtures}

    Another important class of probability distributions is given by the mixture families:
    \newdef{Mixture}{\index{distribution!mixture}
        Consider a collection of probability distributions $\{p_i(X)\}_{i\leq n}$. For every point $\eta\equiv(\eta_0,\ldots,\eta_n)$ in the probability simplex $\Delta^n$, one can define the distribution
        \begin{gather}
            p(X;\eta) := \sum_{i=0}^n\eta_ip_i(X).
        \end{gather}
        This mixture family forms a manifold with affine coordinates $\{\eta_i\}_{1\leq i\leq n}$ (note that $\eta_0$ can be calculated from the other weights and is therefore not an independent coordinate).
    }
    The (negative) Shannon entropy of a mixture defines a convex function $\varphi$ and, as noted before, it induces the Kullback-Leibler divergence:
    \begin{gather}
        D_\varphi(\eta\|\eta') = D_\mathrm{KL}(\eta\|\eta').
    \end{gather}

    \begin{example}[Discrete distribution]
        An interesting example of mixtures is given by the class of discrete (or categorical) distributions:
        \begin{gather}
            p(X;\eta) = \sum_{i=0}^n\eta_i\delta_i(X)\,,
        \end{gather}
        where $\eta\in\Delta^n$. At the same time these models can be considered as distributions in an exponential family with affine coordinates $\theta^i:=\ln\frac{p_i}{p_0}$. For these models the primary coordinates $\overline{\theta}$, dual to $\eta$, coincide with the natural parameters $\theta$.
    \end{example}

    Consider two points with dual coordinates $\eta_1,\eta_2$. The dual geodesic connecting these points is of the form \[\eta(t) = t\eta_2 + (1-y)\eta_1.\] In the case of discrete distributions, where the dual coordinates are given by elements of the probability simplex $\Delta^n$, one can see that such a geodesic induces a linear interpolation of distributions and accordingly defines a mixture family. For this reason one generally calls a dual geodesic an \textbf{m-geodesic} ('m' for mixture).\footnote{For arbitrary families the dual geodesic does not necessarily induce a mixture of distributions.}

    \begin{remark}
        The reader should be aware of an important source of confusion. The above sections would point to the following naming convention:
        \begin{center}
            \begin{tabular}{lll}
                e-geodesic&$\leftrightarrow$&primary geodesic\\
                m-geodesic&$\leftrightarrow$&dual geodesic
            \end{tabular}
        \end{center}
        However, because the Kullback-Leibler divergence is the most widely used divergence measure, Equation \eqref{info:KL_reversal} where the KL-divergence is the dual divergence, made it possible that the above convention got reversed in the bulk of the literature. This reversal also leads one to interchange ``primary'' and ``dual'' in most statements such as the Pythagorean and projections theorems.

        To prevent confusion it is important that one pays attention to which divergence is used. In this text a distinction has been made (as much as possible) between the e/m-convention and the primary/dual convention. The second convention is the main choice here since this one is uniquely determined once one knows the divergence.
    \end{remark}

\subsection{Compatible divergences}

    The question to be answered in this section is the following one: ``Given a dually flat manifold, which divergences are compatible with this structure?''. In the previous sections it was shown that exponential families and mixture families naturally give rise to the Kullback-Leibler divergence. However, not all dually flat manifolds are induced by such families.

    A basic requirement, as is generally the case with geometric structures, is the requirement that the divergences are invariant under coordinate transformations. To this end one needs the \textit{monotonicity criterion of Chentsov}. This criterion states that no transformation should increase the divergence between two points (this corresponds to the idea that transformations can never increase the amount of information). Moreover, there exists a class of (noninvertible) transformations that leave the divergence invariant:
    \newdef{Sufficient statistic}{\index{Fisher-Neyman}
        Consider a random variable $X$ following the distribution $p(X;\theta)$. A transformation $\xi(X)$ is said to be sufficient (with respect to $\theta$) if the distribution of $X$ conditioned on $\xi(X)$ is independent of $\theta$. The \textbf{Fisher-Neyman factorization theorem} states that this is equivalent to the existence of the following decomposition
        \begin{gather}
            p(X;\theta) = f(X)g_\theta(\xi(X))
        \end{gather}
        for some nonnegative functions $f, g_\theta$.
    }
    The invariance criterion states that these transformations are the only transformations that leave the divergence invariant:
    \begin{axiom}[Invariance criterion]
        Consider a dually flat manifold $M$. Compatible divergences should satisfy the following inequality for all transformations $\overline{x}:=\xi(x)$:
        \begin{gather}
            \overline{D}(\theta\|\theta')\leq D(\theta\|\theta').
        \end{gather}
        The equality holds if and only if the transformed variable $\overline{x}$ is a sufficient statistic.
    \end{axiom}

    \begin{example}[$f$-divergences]
        An important class of invariant divergences on the manifold $\Delta^n$ is given by the $f$-divergences introduced in the beginning of this chapter. These also have the additional property that they are \textbf{decomposable}
        \begin{gather}
            D_f(p\|q) = \sum_{i=0}^n d(p_i, q_i)
        \end{gather}
        for some nonnegative function $d$.
    \end{example}
    The following result gives a partial characterization of invariant divergences on the manifold of discrete distributions $\Delta^n$:
    \begin{property}
        A divergence $D$ on $\Delta^n$ ($n>1$) is invariant and decomposable if and only if it it an $f$-divergence. If the induced geometric structure is required to be flat, then necessarily $D = D_\mathrm{KL}$. When extended to $\mathbb{R}^n_+$ (the discrete positive measures), the collection of all $\alpha$-divergences is recovered.
    \end{property}
    \result{Because every Bregman divergence is flat, one can see that $D_\mathrm{KL}$ is the only Bregman divergence that is also an $f$-divergence.}

    One can also go a step further and ask which metrics can arise on such invariant structures. The answer is quite simple (at least for discrete distributions):
    \begin{theorem}[Chentsov]\index{Chentsov}
        Up to scaling, the only invariant metric that exists on $\Delta^n$ is the Fisher information metric. Extending this result to the manifold $\mathbb{R}^n_+$ of discrete positive measures, the only invariant metric on $\mathbb{R}^n_+$ is the Euclidean metric.
    \end{theorem}
    \sremark{Extensions to other families/manifolds of distributions can be found in the literature. However, most of these theorems have to make additional assumptions.}

\subsection{Flat structures}

    In this paragraph the flat structures on $\mathbb{R}^n_+$ are considered. By one of the invariance results above, these are exactly the $\alpha$-divergences. Following \textit{Amari}, the transformation $\alpha=2q-1$ is performed (this maps the \textit{Csisz\'ar} divergences to the \textit{Tsallis} divergences). Given a discrete positive measure with coefficients $m_i$, the affine coordinates are defined as follows:
    \begin{gather}
        \theta^i \equiv h_\alpha(m_i) := m_i^{\frac{1-\alpha}{2}}.
    \end{gather}
    It is not hard to see that the inverse function $\theta^i\mapsto m_i$ is convex (for $|\alpha|\leq1$) and, hence, one can define a potential as follows:
    \begin{gather}
        \psi_\alpha(\theta) := \frac{1-\alpha}{2}\sum_{i=0}^n m_i = \frac{1-\alpha}{2}\sum_{i=0}^n \left(\theta^i\right)^{\frac{2}{1-\alpha}}.
    \end{gather}
    The dual coordinates are given by
    \begin{gather}
        \eta_i \equiv h_{-\alpha(m_i)} = m_i^{\frac{1+\alpha}{2}}.
    \end{gather}
    It should be noted that the normalization constraint $\sum_{i=0}^nm_i=1$ that embeds $\Delta^n$ in $\mathbb{R}^n_+$ is a nonlinear constraint on the affine coordinates except for $\alpha=-1$ (for the dual coordinates this happens for $\alpha=1$). This recovers the fact that the Kullback-Leibler divergence is the only flat, invariant and decomposable structure on $\Delta^n$.

    For any monotonic function $h$ the so-called \textbf{$h$-representation} of $x$ is defined as $h(x)$. Using this representation, one can define the \textbf{$h$-mean} as follows:
    \begin{gather}
        m_h(x,y) := h^{-1}\left(\frac{h(x)+h(y)}{2}\right).
    \end{gather}
    The $\alpha$-representations are exactly the ones inducing linearly scaling $h$-means:
    \begin{gather}
        m_\alpha(\lambda x,\lambda y) = \lambda m_\alpha(x,y).
    \end{gather}
    It is not hard to see that all well-known means, such as the ordinary, geometric and harmonic means, are examples of $\alpha$-means. Given an $\alpha$-representation, one can define an $\alpha$-mixture of distributions as the $\alpha$-mean of the distributions (up to normalization). By allowing for weighted sums, the so-called \textbf{$\alpha$-family} or \textbf{$\alpha$-integration of distributions} with coordinate system $\{w_i\}_{1\leq i\leq N}$ is obtained. The cases $\alpha=-1$ and $\alpha=1$ can be seen to correspond to mixtures and exponential families, respectively.

\section{Projections}

    The following theorem generalizes the classic Pythagorean theorem on $\mathbb{R}^n$ (and reduces to it when one chooses the Euclidean distance as the divergence measure)
    \begin{theorem}[Pythagoras]\index{Pythagoras}
        Consider a triangle $PQR$ on a dually flat manifold $M$ with canonical divergence $D$. If the geodesic $PQ$ and the dual geodesic $QR$ are orthogonal, the following equation holds:
        \begin{gather}
            D(P\|R) = D(P\|Q) + D(Q\|R).
        \end{gather}
        One obtains a conjugate statement by dualizing all quantities.
    \end{theorem}

    In Euclidean space (and in general Hilbert spaces) one of the most powerful theorems is the projection theorem \ref{functional:projection_theorem}, which states that the shortest path from a point to a subspace is given by orthogonal projection. This can be generalized to dually flat manifolds.
    \newdef{Orthogonal projection}{\index{projection!orthogonal}
        Consider a point $p\in M$ and a submanifold $S$ of a dually flat manifold $M$ such that $p\not\in S$. A geodesic (orthogonal) projection of $p$ on $S$ is a point $p^*\in\partial S$ such that the affine geodesic connecting $p$ and $p^*$ is orthogonal to all of $T_{p^*}S$. One can obtain the notion of a dual geodesic projection in a simlar way.
    }
    Because in general there exist multiple projections, a strict minimality theorem cannot be formulated:
    \begin{theorem}[Projection theorem]\index{projection!theorem}
        The extremal points of the map $s\mapsto D(p\|s)$ are geodesic projections of $p$ onto $S$. The dual statement also holds.
    \end{theorem}
    The strict projection theorem for Hilbert spaces only holds for affine subspaces. In the manifold setting this is reflected by a flatness condition:
    \begin{property}
        If the submanifold $S$ is $\widetilde{\nabla}$-flat, the geodesic projection of $p\not\in S$ is unique and it minimizes the map $s\mapsto D(p\|s)$. The dual statement also holds.
    \end{property}

    The e/m-terminology also exists for projections:
    \newdef{Projections}{
        The e- and m-projections are defined as follows:
        \begin{itemize}
            \item e-projection: $\pi_e(p) := \arg\min_{q\in S} D_\mathrm{KL}(q\|p)$, and
            \item m-projection: $\pi_m(p) := \arg\min_{q\in S} D_\mathrm{KL}(p\|q)$.
        \end{itemize}
    }

\subsection{Functional descent}

    The content of this section will be the derivation of an update scheme for probability measures using the functional analytic framework in the spirit of gradient descent. % This is based on the paper \cite{PFD}.

    Consider the space of finite Borel measures $M(X)$ on a compact Polish space $X$ (compactness will be assumed to freely apply the Riesz-Markov theorem \ref{distributions:riesz_markov}).
    \newdef{Probability functional}{
        A linear fuctional $J:M(X)\rightarrow\mathbb{R}$. By Riesz-Markov this is equivalent to a functional $J:C(X)^*\rightarrow\mathbb{R}$.
    }

    \newdef{Instance function}{
        Consider a probability functional $J\in M(X)^*$. An instance function for $J$ at $\mu\in M(X)$ is a function $\psi_\mu:X\rightarrow\mathbb{R}$ such that
        \begin{gather}
            \dr J(\mu;\nu) = \int_X\psi_\mu\,d\nu
        \end{gather}
        for all $\nu\in M(X)$.
    }
    \sremark{Note that the G\^ateaux differential \ref{functional:gateaux} only makes sense for functions on vector spaces, which is why $M(X)$ was used instead of $\mathbb{P}(X)$.}

    Locally around $\mu_0\in M(X)$ one can write down the Taylor expansion
    \begin{gather*}
        J(\mu)\approx J(\mu_0) + \dr J(\mu_0;\mu-\mu_0).
    \end{gather*}
    If $J$ admits an instance function at $\mu_0$, this can be rewritten as
    \begin{align*}
        J(\mu)&\approx J(\mu_0) + \int_X\psi_{\mu_0}(x)\,d\mu - \int_X\psi_{\mu_0}\,d\mu_0\\
        &= J(\mu_0) - \mathrm{E}_{x\sim\mu_0}[\psi_{\mu_0}] + \mathrm{E}_{x\sim\mu}[\psi_{\mu_0}].
    \end{align*}
    The first two terms are constants, so only the last expectation matters during minimization. An expression of this form is sometimes called a \textbf{von Mises representation}.\index{von Mises representation} The probability functional descent algorithm tries to improve the current estimate $\mu_0$ of the true distribution $\widehat{\mu}$ by iterately decreasing the expectation $\mathrm{E}_{x\sim\mu}[\psi_{\mu_0}]$.

    A different approach could be followed to define the probability functional and its G\^ateaux derivative. The set $\mathbb{P}(X)$ has the structure of a manifold inside $M(X)$ modelled on a locally convex space. Definition \ref{hdg:kinematic_tangent_bundle} then gives rise to the tangent bundle $T\mathbb{P}(X)$.

    ?? FORMALIZE THIS IDEA ??
\chapter{Probability}\label{chapter:probability}

    The majority of this chapter uses the language of measure theory. For an introduction see Chapter \ref{chapter:measure}.

\section{Probability}

    The Kolmogorov axioms of probability state when a set admits the definition of a probability theory:
    \newdef{Kolmogorov axioms}{\index{Kolmogorov!axioms}\index{probability}\index{sample space}
        A probability space $(\Omega,\Sigma,P)$ is a measure space \ref{lebesgue:measure_space} with finite measure $P(X)=1$. The set $\Omega$ is called the \textbf{sample space}.
    }

    \newdef{Random variable}{\index{random variable}
        Let $(\Omega,\Sigma,P)$ be a probability space. A function $X:\Omega\rightarrow\mathbb{R}$ is called a random variable if $\forall a\in\mathbb{R}:X^{-1}\big([a,\infty[\big)=\{\omega\in\Omega\mid X(\omega)\geq a\}\in\Sigma$.
    }

    \newdef{$\sigma$-algebra of a random variable}{\index{$\sigma$!algebra}
        Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following family of sets is a $\sigma$-algebra:
        \begin{gather}
            \label{prob:sigma_algebra_generated_random_variable}
            X^{-1}(\mathcal{B}) := \{S\in\Sigma\mid\exists B\in\mathcal{B}:S = X^{-1}(B)\}.
        \end{gather}
    }
    \begin{notation}
        The $\sigma$-algebra generated by the random variable $X$ is often denoted by $\mathcal{F}_X$, analogous to \ref{set:notation:generated_sigma_algebra}.
    \end{notation}

    \newdef{Event}{\index{event}
        Let $(\Omega,\Sigma,P)$ be a probability space. An element $S$ of the $\sigma$-algebra $\Sigma$ is called an event.

        From this definition it is clear that a single possible outcome of a measurement can be a part of multiple events. So, although only one outcome can occur at the same time, multiple events can occur simultaneously.
    }
    \begin{remark*}
        The Kolmogorov axioms use the $\sigma$-algebra \ref{set:sigma_algebra} of events instead of the power set \ref{set:power_set} of all events. Intuitively this seems to mean that some possible outcomes are not treated as events. However, one can make sure that the $\sigma$-algebra still contains all ``useful'' events by using a ``nice'' definition of probability spaces.
    \end{remark*}

    \begin{formula}[Union]\label{prob:union}
        Let $A,B$ be two events. The probability that at least one of them occurs is given by the following formula:
        \begin{gather}
            P(A\cup B) = P(A) + P(B) + P(A\cap B).
        \end{gather}
    \end{formula}

    \newdef{Disjoint events}{
        Two events $A$ and $B$ are said to be disjoint if they cannot happen at the same time:
        \begin{gather}
            P(A\cap B) = 0.
        \end{gather}
    }
    \result{If $A$ and $B$ are disjoint, the probability that both $A$ and $B$ occur is just the sum of their individual probabilities.}

    \newformula{Complement}{\index{complement}\label{prob:complement}
        Let $A$ be an event. The probability of $A$ being false is denoted as $P\left(\overline{A}\right)$ and is given by
        \begin{gather}
            P\left(\overline{A}\right) = 1 - P(A).
        \end{gather}
    }
    \begin{result}
        From the previous equation and de Morgan's laws \eqref{set:de_morgan_union} and \eqref{set:de_morgan_intersection}, one can derive the following formula:
        \begin{gather}
            P\left(\overline{A}\cap\overline{B}\right) = 1 - P(A\cup B).
        \end{gather}
    \end{result}

\section{Conditional probability}

    \newdef{Conditional probability}{\index{probability!conditional}\label{prob:conditional_probability}
        Let $A,B$ be two events. The probability of $A$ given that $B$ is true is denoted as $P(A|B)$:
        \begin{gather}
            P(A|B) = \stylefrac{P(A\cap B)}{P(B)}.
        \end{gather}
    }
    By interchanging $A$ and $B$ in previous equation and by observing that this has no effect on the quantity $P(A\cap B)$ the following important result can be derived:
    \begin{theorem}[Bayes]\index{Bayes}\label{prob:bayes}
        Let $A,B$ be two events.
        \begin{gather}
            P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
        \end{gather}
    \end{theorem}

    \begin{formula}
        Let $\seq{B}$ be a sequence of pairwise disjoint events. If $\bigsqcup_{n=1}^\infty B_n = \Omega$, the total probability of a given event $A$ can be calculated as follows:
        \begin{gather}
            \label{probability:total_probability_conditional}
            P(A) = \sum_{n=1}^\infty P(A|B_n)P(B_n).
        \end{gather}
    \end{formula}

    \newdef{Independent events}{\index{independence}
        Let $A,B$ be two events. $A$ and $B$ are said to be independent if they satisfy the following relation:
        \begin{gather}
            P(A\cap B) = P(A)P(B).
        \end{gather}
    }
    \begin{result}
        If $A$ and $B$ are two independent events, Bayes's theorem simplifies to
        \begin{gather}
            P(A|B) = P(A).
        \end{gather}
    \end{result}
    The above definition can be generalized to multiple events:
    \begin{definition}
        The events $A_1,\ldots,A_n$ are said to be independent if for each choice of $k$ events the probability of their intersection is equal to the product of their individual probabilities.
    \end{definition}
    This definition can be stated in terms of $\sigma$-algebras:
    \begin{definition}[Independence]\index{independence}
        The $\sigma$-algebras $\mathcal{F}_1,\ldots,\mathcal{F}_n$ defined on a probability space $(\Omega,\mathcal{F},P)$ are said to be independent if for all choices of distinct indices $i_1,\ldots,i_k$ and for all choices of sets $F_{i_n}\in\mathcal{F}_{i_n}$ the following equation holds:
        \begin{gather}
            \label{prob:independent_sigma_algebras}
            P(F_{i_1}\cap\cdots\cap F_{i_k}) = P(F_{i_1})\cdots P(F_{i_k}).
        \end{gather}
    \end{definition}
    \begin{result}
        Let $X,Y$ be two random variables. $X$ and $Y$ are independent if the $\sigma$-algebras generated by them are independent.
    \end{result}

\section{Probability distribution}

    \newdef{Probability distribution}{\index{probability!distribution}\label{prob:probability_distribution}
        Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following function is a measure on the Borel $\sigma$-algebra of $\mathbb{R}$:
        \begin{gather}
            P_X(B) = P(X^{-1}(B)).
        \end{gather}
        This measure is called the probability distribution of $X$.
    }

    \newdef{Density}{\index{density}\index{cumulative distribution function}
        \nomenclature[A_CDF]{CDF}{cumulative distribution function}
        Let $f\geq0$ be an integrable function and recall Property \ref{lebesgue:measure_by_integral}. The function $f$ is called the density of the measure $P(A):=\int_Af\,d\lambda$ (with respect to the Lebesgue measure $\lambda$). Measures of this form are often called \textbf{cumulative distribution functions} and denoted by $F$. More generally, by the Radon-Nikodym theorem from Section \ref{section:Radon-Nikodym}, every absolutely continuous distribution function $F$ is of the form
        \begin{gather}
            F(A) = \int_Af\,d\lambda
        \end{gather}
        for some integrable function $f$.
    }

    \begin{theorem}[Skorokhod's representation theorem]\index{Skorokhod}
        Let $F:\mathbb{R}\rightarrow[0,1]$ be a function that satisfies the following three properties:
        \begin{itemize}
            \item $F$ is nondecreasing.
            \item $\ds\lim_{x\rightarrow-\infty}F(x) = 0$ and $\ds\lim_{x\rightarrow\infty}F(x) = 1$.
            \item $F$ is right-continuous, i.e. $\ds\lim_{y\nearrow y_0}F(y)=F(y_0)$.
        \end{itemize}
        There exists a random variable $X:[0,1]\rightarrow\mathbb{R}$ defined on the probability space $([0,1],\mathcal{B},m_{[0,1]})$ such that $F=F_X$.
    \end{theorem}

    \begin{theorem}[Theorem of the unconscious statistician]\label{prob:unconscious_statistician}
        Consider a random variable $X$ on a probability space $(\Omega,\Sigma,P)$. The following equality holds for every integrable function $g\in L^1(\mathbb{R})$:
        \begin{gather}
            \int_\Omega g\circ X\,dP = \int_\mathbb{R}g(x)dP_X(x).
        \end{gather}
    \end{theorem}
    \begin{remark}
        The name of this theorem stems from the fact that many scientists take this equality to be a definition of the expectation value $\text{E}[g(X)]$. However, this equality should be proven since the measure on the right-hand side is the one belonging to the random variable $X$ and not $g(X)$.
    \end{remark}

    \begin{formula}
        Consider an absolutely continuous probability function $F$ defined on $\mathbb{R}^n$ and let $f$ be the associated density. Let $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be integrable with respect to $F$.
        \begin{gather}
            \int_{\mathbb{R}^n}g\,dF = \int_{\mathbb{R}^n}f(x)g(x)dx
        \end{gather}
    \end{formula}
    \begin{result}
        The previous formula together with Theorem \ref{prob:unconscious_statistician} gives rise to
        \begin{gather}
            \label{prob:omega_int_to_real_int}
            \int_\Omega g\circ X\,dP = \int_{\mathbb{R}^n}f_X(x)g(x)dx.
        \end{gather}
    \end{result}

    \begin{formula}
        Let $X$ be a random variable with density function $f_X$ and let $g:\mathbb{R}\rightarrow\mathbb{R}$ be smooth and strictly monotone. The random variable $g\circ X$ has an associated density $f_g$ given by
        \begin{gather}
            \label{prob:function_of_random_variable}
            f_g(y) = f(g^{-1}(y))\left|\deriv{g^{-1}}{y}(y)\right|.
        \end{gather}
    \end{formula}

    \newdef{Convergence in distribution}{\index{convergence!in distribution}
        A sequence $\seq{X}$ of random variables is said to converge in distribution to a random variable $Y$ if the associated distribution functions $F_{X_n}$ converge pointwise to $F_Y$, i.e. $\lim_{n\rightarrow\infty}F_{X_n}(x)=F_Y(x)$ for all $x\in\mathbb{R}$.
    }
    \begin{notation}
        If a sequence $\seq{X}$ converges in distribution to a random variable $Y$, this is often denoted by $X_n\overset{d}{\longrightarrow}Y$. Sometimes the $d$ (for ``distribution'') is replaced by the $\mathcal{L}$ (for ``law'').
    \end{notation}

    \begin{theorem}[Slutsky]\index{Slutsky}
        Let $\seq{X},\seq{Y}$ be two sequences of random variables converging in probability to a random variable $X$ and a constant $c$, respectively. The following statements hold:
        \begin{itemize}
            \item $X_n+Y_n\overset{d}{\longrightarrow}X+c$,
            \item $X_nY_n\overset{d}{\longrightarrow}cX$, and
            \item $X_n/Y_n\overset{d}{\longrightarrow}X/c$.
        \end{itemize}
    \end{theorem}

    \newdef{\difficult{Giry monad}}{\index{Giry monad}
        Consider the category $\mathbf{Meas}$ of measurable spaces. On this space one can define a monad \ref{cat:monad} that sends a set $X$ to its collection of probability distributions equipped with the $\sigma$-algebra generated by all evaluation maps $\mathrm{ev}_U$, where $U$ runs over the measurable subsets of $X$.

        The unit of the Giry monad $G$ is defined by assigning Dirac measures:
        \begin{gather}
            \eta_X(x) := \delta_x.
        \end{gather}
        The multiplication map is defined as follows:
        \begin{gather}
            \mu_X(Q)(U) := \int_{P\in GX}\mathrm{ev}_U(P)\,dQ.
        \end{gather}
    }

\section{Moments}
\subsection{Expectation value}

    \newdef{Expectation value}{\index{expectation}\label{prob:expectation_value}
        Let $X$ be random variable defined on a probability space $(\Omega,\Sigma,P)$.
        \begin{gather}
            \expect{X} := \int_\Omega X\,dP
        \end{gather}
    }
    \begin{notation}
        Other notations that are common in the literature are $\langle X \rangle$ and $\mu_X$.
    \end{notation}

    \newdef{Moment of order \texorpdfstring{$r$}{r}}{\index{moment}\label{prob:moment}
        The moment of order $r$ is defined as the expectation value of the $r^{th}$ power of $X$. By Equation \eqref{prob:omega_int_to_real_int} this becomes
        \begin{gather}
            \expect{X^r} = \int_\mathbb{R}x^rf_X(x)dx.
        \end{gather}
    }
    \newdef{Central moment of order \texorpdfstring{$r$}{r}}{\index{central!moment}\label{prob:central_moment}
        \begin{gather}
            \expect{(X-\mu)^r} = \int_\mathbb{R}(x-\mu)^rf_X(x)dx
        \end{gather}
    }
    \begin{remark}
        Moments of order $n$ are determined by central moments of order $k\leq n$ and, conversely, central moments of order $n$ are determined by moments of order $k\leq n$.
    \end{remark}
    \newdef{Variance}{\index{variance}
        The central moment of order 2 is called the variance:
        \begin{gather}
            \variance{X} := \expect{(X-\mu)^2}.
        \end{gather}
    }
    \newdef{Standard deviation}{\index{standard!deviation}
        \begin{gather}
            \sigma_X := \sqrt{V[X]}
        \end{gather}
    }

    \begin{property}
        If $\expect{|X|^n}$ is finite for $n>0$, then $\expect{X^k}$ exist and is finite for all $k\leq n$.
    \end{property}

    \newdef{Moment generating function}{\index{moment!generating function}\label{prob:moment_generating_function}
        \begin{gather}
            M_X(t) := \expect{e^{tX}} = \int_{-\infty}^\infty e^{tx}f_X(x)dx
        \end{gather}
    }
    \begin{property}
        If the moment generating function exists, the moments $\expect{X^n}$ can be expressed in terms of $M_X$ (using the series expansion of the exponential function):
        \begin{gather}
            \label{prob:moment_generating}
            \expect{X^n} = \left.\mderiv{n}{M_X(t)}{t}\right|_{t=0}.
        \end{gather}
    \end{property}

    \newdef{Characteristic function}{\index{characteristic!function}\label{prob:characteristic_function}
        \begin{gather}
            \varphi_X(t) := \expect{e^{itX}}
        \end{gather}
    }
    \begin{property}\label{prob:characteristic_function_properties}
        The characteristic function has the following properties:
        \begin{itemize}
            \item $\varphi_X(0) = 1$,
            \item $|\varphi_X(t)| \leq 1$, and
            \item $\varphi_{aX+b}(t) = e^{itb}\varphi_X(at)$ for all $a,b\in\mathbb{R}$.
        \end{itemize}
    \end{property}

    \begin{formula}
        If $\varphi_X(t)$ is $k$ times continuously differentiable, then $X$ has a finite $k^{th}$ moment and
        \begin{gather}
            \label{prob:characteristic_function_as_moment_generator}
            \expect{X^k} = \frac{1}{i^k}\mderiv{k}{}{t}\varphi_X(0).
        \end{gather}
        Conversely, if $X$ has a finite $k^{th}$ moment, then $\varphi_X(t)$ is $k$ times continuously differentiable and the above formula holds.
    \end{formula}

    \newformula{Inversion formula}{\index{inversion!formula}
        Let $X$ be a random variable. If the CDF of $X$ is continuous at $a,b\in\mathbb{R}$, then
        \begin{gather}
            \label{prob:inversion_formula}
            F_X(b) - F_X(a) = \lim_{c\rightarrow\infty}\frac{1}{2\pi}\int_{-c}^c\frac{e^{-ita} - e^{-itb}}{it}\varphi_X(t)dt.
        \end{gather}
    }
    \begin{formula}
        If $\varphi_X(t)$ is integrable, the CDF is given by:
        \begin{gather}
            f_X(x) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\varphi_X(t)dt.
        \end{gather}
    \end{formula}
    \remark{This formula implies that the density function and the characteristic function form a Fourier transform pair.}

\subsection{Correlation}

    \begin{property}\index{independence}\label{prob:independence_expectation_values}
        Two random variables $X,Y$ are independent if and only if $\expect{f(X)g(Y)} = \expect{f(X)}\expect{g(Y)}$ holds for all Borel-measurable bounded functions $f,g$.
    \end{property}

    The value $\expect{XY}$ is equal to the inner product $\langle X|Y \rangle$ as defined in \eqref{lebesgue:L2_inner_product}. It follows that independence of random variables implies orthogonality. To generalize this concept, the following notions are introduced:
    \newdef{Centred random variable}{\index{random variable}
        Let $X$ be a random variable with finite expectation value $\expect{X}$. The centred random variable $X_c$ is defined as $X_c = X-\expect{X}$.
    }
    \newdef{Covariance}{\index{covariance}
        The covariance of two random variables $X,Y$ is defined as follows:
        \begin{gather}
            \label{prob:covariance}
            \mathrm{cov}(X,Y) := \langle X_c|Y_c \rangle = \expect{(X-\expect{X})(Y-\expect{Y})}.
        \end{gather}
        Some basic math gives
        \begin{gather}
            \mathrm{cov}(X,Y) = \expect{XY} - \expect{X}\expect{Y}.
        \end{gather}
    }
    \newdef{Correlation}{\index{correlation}\label{prob:correlation}
        The correlation of two random variables $X,Y$ is defined as the cosine of the angle between $X_c$ and $Y_c$:
        \begin{gather}
            \rho_{XY} := \frac{\mathrm{cov}(X,Y)}{\sigma_X\sigma_Y}.
        \end{gather}
    }
    \result{From Theorem \ref{prob:independence_expectation_values} it follows that independent random variables are uncorrelated.}
    \result{If the random variables $X$ and $Y$ are uncorrelated, they satisfy $\expect{XY} = \expect{X}\expect{Y}$.}

    \begin{formula}[Bienaym\'e formula]\index{Bienaym\'e}\label{prob:bienayme}
        Let $\seq{X}$ be a sequence of independent (or uncorrelated) random variables. Their variances satisfy the following equation:
        \begin{gather}
            \label{prob:variance_of_sum}
            \variance{\sum_{i=1}^\infty X_i} = \sum_{i=1}^\infty\variance{X_i}.
        \end{gather}
    \end{formula}

\subsection{Conditional expectation}

    Let $(\Omega,\Sigma,P)$ be a probability space. Consider a random variable $X\in L^2(\Omega,\Sigma,P)$ and a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. Property \ref{lebesgue:L2_hilbert_space} implies that the spaces $L^2(\Sigma)$ and $L^2(\mathcal{G})$ are complete and, hence, the projection theorem \ref{functional:projection_theorem} can be applied. For every $X\in L^2(\Sigma)$ there exists a random variable $Y\in L^2(\mathcal{G})$ such that $X-Y$ is orthogonal to $L^2(\mathcal{G})$. This has the following result:
    \begin{gather}
        \forall Z\in L^2(\mathcal{G}):\langle X-Y|Z \rangle\equiv\int_\Omega(X-Y)ZdP = 0.
    \end{gather}
    Since $\mathbbm{1}_G\in L^2(\mathcal{G})$ for every $G\in\mathcal{G}$, Equation \eqref{lebesgue:domain_change} can be rewritten as
    \begin{gather}
        \label{prob:conditional_expectation_condition}
        \int_GX\,dP = \int_GY\,dP
    \end{gather}
    for all $G\in\mathcal{G}$. This leads to the introduction of the following definition:
    \newdef{Conditional expectation}{\index{expectation!conditional}\label{prob:conditional_expectation}
        Let $(\Omega,\Sigma,P)$ be a probability space and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\Sigma$. For every $\Sigma$-measurable random variable $X\in L^2(\Sigma)$ there exists a unique (up to a null set) random variable $Y\in L^2(\mathcal{G})$ that satisfies Equation \eqref{prob:conditional_expectation_condition} for every $G\in\mathcal{G}$. This variable $Y$ is called the conditional expectation of $X$ given $\mathcal{G}$ and it is denoted by $\expect{X|\mathcal{G}}$:
        \begin{gather}
            \int_G\expect{X|\mathcal{G}}\,dP = \int_GX\,dP.
        \end{gather}
    }
    \begin{remark}
        Although this construction was based on orthogonal projections, one could as well have used the (signed) Radon-Nikodym theorem \ref{lebesgue:signed_radon_nikodym} since $G\mapsto\int_GX\,dP$ is absolutely continuous with respect to $P|_{\mathcal{G}}$.
    \end{remark}

    \begin{property}\label{prob:conditional_expectation_props}
        Let $(\Omega,\Sigma,P)$ be a probability space and consider a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. If the random variable $X$ is $\mathcal{G}$-measurable, then
        \begin{gather}
            \expect{X|\mathcal{G}} = X\text{ a.s.}
        \end{gather}
        On the other hand, if $X$ is independent of $\mathcal{G}$, then
        \begin{gather}
            \expect{X|\mathcal{G}} = \expect{X}\text{ a.s.}
        \end{gather}
    \end{property}

\section{Joint distributions}

    \newdef{Joint distribution}{\index{distribution!joint}
        Let $X,Y$ be two random variables defined on the same probability space $(\Omega,\Sigma,P)$ and consider the vector random variable $(X,Y):\Omega\rightarrow\mathbb{R}^2$. The distribution of $(X,Y)$ isa probability measure defined on the Borel algebra of $\mathbb{R}^2$ defined by
        \begin{gather}
            P_{(X,Y)}(B) = P((X,Y)^{-1}(B)).
        \end{gather}
    }
    \newdef{Joint density}{
        If the probability measure from the previous definition can be written as
        \begin{gather}
            P_{(X,Y)}(B) = \int_Bf_{(X,Y)}(x,y)dxdy
        \end{gather}
        for some integrable $f_{(X,Y)}$, it is said that $X$ and $Y$ have a joint density.
    }

    \newdef{Marginal distribution}{\index{distribution!marginal}
        The distributions of the one-dimensional random variables is determined by the joint distribution:
        \begin{gather}
            P_X(A) = P_{(X,Y)}(A\times\mathbb{R})\\
            P_Y(A) = P_{(X,Y)}(\mathbb{R}\times A).
        \end{gather}
    }
    \begin{result}
        If the joint density exists, the marginal distributions are absolutely continuous and the associated density functions are given by
        \begin{gather}
            f_X(x) = \int_\mathbb{R}f_{(X,Y)}(x,y)dy\\
            f_Y(y) = \int_\mathbb{R}f_{(X,Y)}(x,y)dx.
        \end{gather}
        The converse, however, is not always true. The one-dimensional distributions can be absolutely continuous without the existence of a joint density.
    \end{result}

    \begin{property}[Independence]\index{independence}\label{prob:independent_densities}
        Let $X,Y$ be two random variables with joint distribution $P_{(X,Y)}$. $X$ and $Y$ are independent if and only if the joint distribution coincides with the product measure:
        \begin{gather}
            P_{(X,Y)} = P_X\otimes P_Y.
        \end{gather}
        If $X$ and $Y$ are absolutely continuous, the previous properties also applies to the densities instead of the distributions.
    \end{property}

    \begin{formula}[Sum of random variables]
        Consider two independent random variables $X,Y$ and let $Z=X+Y$ denote their sum. The density $f_Z$ is given by the following convolution:
        \begin{gather}
            f_Z(z) := f\ast g(z) = \int_{-\infty}^\infty g(x)h(z-x)dx = \int_{-\infty}^\infty g(z-y)h(y)dy,
        \end{gather}
        where $g,h$ denote the densities of $X,Y$ respectively.
    \end{formula}
    \begin{formula}[Product of random variables]
        Consider two independent random variables $X,Y$ and let $Z=XY$ denote their product. The density $f_Z$ is given by
        \begin{gather}
            f_Z(z) = \int_{-\infty}^\infty g(x)h(z/x)\frac{dx}{|x|} = \int_{-\infty}^\infty g(z/y)h(y)\frac{dy}{|y|},
        \end{gather}
        where $g,h$ denote the densities of $X,Y$ respectively.
    \end{formula}
    \begin{result}
        Taking the Mellin transform \ref{distributions:mellin} of both the positive and negative part of the above integrand (to be able to handle the absolute value) gives the following relation:
        \begin{gather}
            \mathcal{M}\{f\} = \mathcal{M}\{g\}\mathcal{M}\{h\}.
        \end{gather}
    \end{result}

    \newformula{Conditional density}{\index{conditional density}
        Let $X,Y$ be two random variables with joint density $f_{(X,Y)}$. The conditional density of $Y$ given $X\in A$ is
        \begin{gather}
            \label{prob:conditional_distribution}
            h(y|X\in A) = \frac{\int_Af_{(X,Y)}(x,y)dx}{\int_Af_X(x)dx}.
        \end{gather}
        For $X=\{a\}$ this equation is ill-defined since the denominator would become 0. However, it is possible to avoid this problem by formally setting
        \begin{gather}
            \label{prob:formal_conditional}
            h(y|A=a) := \frac{f_{(X,Y)}(a,y)}{f_X(a)},
        \end{gather}
        where $f_X(a)\neq0$. This last condition is nonrestrictive\marginpar{\dbend} because the probability of having a measurement $(X,Y)\in\{(x,y)\mid f_X(x) = 0\}$ is 0 (for nonsingular measures). One can thus define the conditional probability of $Y$ given $X=a$ as follows:
        \begin{gather}
            P(Y\in B|X=a) := \int_B h(y|X=a)dy.
        \end{gather}
    }

    \newformula{Conditional expectation}{\index{expectation!conditional}
        \begin{gather}
            \expect{Y|X}(\omega) = \int_\mathbb{R}yh(y|X(\omega))dy
        \end{gather}
        Let $\mathcal{F}_X$ denote the $\sigma$-algebra generated by the random variable $X$ as before. Using Fubini's theorem one can prove that for all sets $A\in\mathcal{F}_X$ the following equality holds:
        \begin{gather}
            \int_A\expect{Y|X}\,dP = \int_AY\,dP.
        \end{gather}
        This implies that the conditional expectation $\expect{Y|X}$ on $\mathcal{F}_X$ coincides with Definition \ref{prob:conditional_expectation}.
    }
    Applying Property \ref{prob:conditional_expectation_props} to the case $\mathcal{G}=\mathcal{F}_X$ gives the law of total expectation:
    \begin{property}[Law of total expectation]
        \begin{gather}
            \expect{\expect{Y|X}} = \expect{Y}
        \end{gather}
    \end{property}

    \begin{theorem}[Bayes's theorem]\index{Bayes}\label{prob:bayes_density}
        The conditional density can be computed without prior knowledge of the joint density:
        \begin{gather}
            g(x|y) = \frac{h(y|x)f_X(x)}{f_Y(y)}.
        \end{gather}
    \end{theorem}

\section{Stochastic calculus}

    \newdef{Stochastic process}{\index{stochastic!process}
        A sequence of random variables $\tseq{X}$ for some index set $T$. In practice $T$ will often be a totally ordered set, e.g. $(\mathbb{R},\leq)$ in the case of a time series. This will be assumed from here on.
    }

    \newdef{Filtered probability space}{\index{probability!space}
        Consider a probability space $(\Omega,\Sigma,P)$ together with a filtration \ref{set:filtration} of $\Sigma$, i.e. a collection of $\sigma$-algebras $\mathbb{F}=\tseq{\mathbb{F}}$, such that $i\leq j\implies\mathbb{F}_i\subseteq\mathbb{F}_j$. The quadruple $(\Omega,\Sigma,\mathbb{F},P)$ is called a filtered probability space.

        Often the filtration is required to be exhaustive and separated (where $\emptyset$ is replaced by $\mathbb{F}_0=\{\emptyset,\Omega\}$ since any $\sigma$-algebra has to contain the total space).
    }

    \newdef{Adapted process}{\index{adapted!process}
        A stochastic process $\tseq{X}$ on a filtered probability space $(\Omega,\Sigma,\mathbb{F},P)$ is said to be adapted to the filtration $\mathbb{F}$ if $X_t$ is $\mathbb{F}_t$-measurable for all $t\in T$.
    }
    \newdef{Predictable process}{\index{predictable}
        A stochastic process $\tseq{X}$ on a filtered probability space $(\Omega,\Sigma,\mathbb{F},P)$ is said to be predictable if $X_{t+1}$ is $\mathbb{F}_t$-measurable for all $t\in T$.
    }

    \newdef{Stopping time}{\index{stopping time}
        Consider a random variable $\tau$ on filtered probability space $(\Omega,\Sigma,\mathbb{F},P)$ where the codomain of $\tau$ coincides with the index set of $\mathbb{F}$. This variable is called a stopping time for $\mathbb{F}$ if
        \begin{gather}
            \{\tau\leq t\}\in\mathbb{F}_t
        \end{gather}
        for all $t$. The stopping time is a ``time indicator'' that only depends on the knowledge of the process up to time $t\in T$.
    }

\subsection{Martingales}

    From here on the index set $T$ will be $\mathbb{R}_+\equiv[0,\infty[$ so that the index $t$ can be interpreted as a true time parameter. The discrete case $T=\mathbb{N}$ can be obtained as the restriction of most definitions or properties and, if necessary, this will be made explicit.

    \newdef{Martingale}{\index{martingale}
        Consider a filtered probability space $(\Omega,\Sigma,\mathbb{F},P)$. A stochastic process $\tseq{X}$ is called a martingale relative to $\mathbb{F}$ if it satisfies the following conditions:
        \begin{enumerate}
            \item $\tseq{X}$ is adapted to $\mathbb{F}$.
            \item Each random variable $X_t$ is integrable, i.e. $X_t\in L^1(P)$ for all $t\geq0$.
            \item For all $t>s\geq0:\expect{X_{t}|\mathbb{F}_s}=X_s$.
        \end{enumerate}
        If the equality in the last condition is replaced by the inequality $\leq$ (resp. $\geq$), the stochastic process is called a \textbf{supermartingale} (resp. \textbf{submartingale}).
    }

    \begin{theorem}[Doob decomposition]\index{Doob}
        Any integrable adapted process $\tseq{X}$ can be decomposed as $X_t=X_0+M_t+A_t$, where $\tseq{M}$ is a martingale and $\tseq{A}$ is a predictable process. These two processes are constructed iteratively as follows:
        \begin{align}
            A_0 = 0\qquad&\qquad M_0 = 0\\
            \Delta A_t = \expect{\Delta X_t|\mathbb{F}_{t-1}}\qquad&\qquad\Delta M_t = \Delta X_t - \Delta A_t.
        \end{align}
        Furthermore, $\tseq{X}$ is a submartingale if and only if $\tseq{A}$ is (almost surely) increasing.
    \end{theorem}
    \begin{result}\index{variation!quadratic}
        Consider the special case $X=Y^2$ for some martingale $Y$. One can show the following property:
        \begin{gather}
            \Delta A_t = \expect{(\Delta Y_t)^2|\mathbb{F}_{t-1}}\qquad\forall t\in\mathbb{R}_+.
        \end{gather}
        The process $\tseq{A}$ is often called the \textbf{quadratic variation process} of $\tseq{X}$ and is denoted by $\tseq{[X]}$.
    \end{result}

    \newdef{Discrete stochastic integral\footnotemark}{\index{integral!stochastic}\index{martingale!transform|see{integral, stochastic}}
        \footnotetext{Sometimes called the \textbf{martingale transform}.}
        Let $\seq{M}$ be a martingale on a filtered probability space $(\Omega,\Sigma,\mathbb{F},P)$ and let $\seq{X}$ be a predictable stochastic process with respect to $\mathbb{F}$. The (discrete) stochastic integral of $X$ with respect to $M$ is defined as follows:
        \begin{gather}
            (X\cdot M)_t(\omega) := \sum_{i=1}^tX(\omega)_i\Delta M_i(\omega),
        \end{gather}
        where $\omega\in\Omega$. For $t=0$ the convention $(X\cdot M)_0=0$ is used.
    }
    \begin{property}
        If the process $\seq{X}$ is bounded, the stochastic integral itself defines a martingale.
    \end{property}

    \begin{property}[It\^o isometry]\index{It\^o!isometry}
        Consider a martingale $\seq{M}$ and a predictable process $\seq{X}$. Using the Doob decomposition theorem one can show the following equality for all $n\geq0$:
        \begin{gather}
            \expect{\left(X\cdot M\right)_n^2} = \expect{(X^2\cdot[M])_n}.
        \end{gather}
    \end{property}
    It is this property that allows for the definition of integrals with respect to continuous martingales, since although the martingales are not in general of bounded variation (and hence do not induce a well-defined Lebesgue-Stieltjes integral), their quadratic variations are (e.g. the Wiener process).

\subsection{Markov processes}

    \newdef{Markov process}{\index{Markov!process}
        A Markov process (or chain) is a stochastic process $\tseq{X}$ adapted to a filtration $\tseq{\mathbb{F}}$ such that
        \begin{gather}
            P(X_t|\mathbb{F}_s) = P(X_t|X_s)
        \end{gather}
        for all $t,s\in T$. For discrete processes, the first-order Markov chains are the most common. These satisfy
        \begin{gather}
            P(X_t|X_{t-1},\ldots,X_{t-r}) = P(X_t|X_{t-1})
        \end{gather}
        for all $t,r\in\mathbb{N}$.
    }

\section{Information theory}

    \newdef{Self-information}{\index{information}
        The self-information of an event $x$ described by a distribution $P$ is defined as follows:
        \begin{gather}
            I(x) := -\ln P(x).
        \end{gather}
        This definition is modeled on the following (reasonable) requirements:
        \begin{itemize}
            \item Events that are almost surely going to happen, i.e. events $x$ such that $P(x)=1$, contain only little information: $I(x)=0$.\footnote{And by extension $P(x)\approx1\implies I(x)\approx0$.}
            \item Events that are very rare contain a lot of information.
            \item Independent events contribute additively to the information.
        \end{itemize}
    }
    \newdef{Shannon entropy}{\index{entropy!Shannon}\label{prob:shannon_entropy}
        The amount of uncertainty in a discrete distribution $P$ is characterized by its (Shannon) entropy
        \begin{gather}
            H(P) := \expect{I(X)} = -\sum_iP_i\ln(P_i).
        \end{gather}
    }

    \newdef{Kullback-Leibler divergence}{\index{Kullback-Leibler divergence}\index{entropy!relative}\label{prob:kullback_leibler}
        Let $P,Q$ be two probability distributions. The Kullback-Leibler divergence (or \textbf{relative entropy}) of $P$ with respect to $Q$ is defined as follows:
        \begin{gather}
            D_\mathrm{KL}(P\|Q) := \int_\Omega\log\left(\frac{P}{Q}\right)\,dP.
        \end{gather}
        This quantity can be interpreted as the information gained when using the distribution $P$ instead of $Q$. Instead of a base-10 logarithm, any other logarithm can be used since this simply changes the result by a (positive) scaling constant.
    }

    \begin{property}[Gibbs's inequality]
        By noting that the logarithm is a concave function and applying Jensen's equality \ref{calculus:jensen_inequality}, one can prove that the Kullback-Leibler divergence is nonnegative:
        \begin{gather}
            D_\mathrm{KL}(P\|Q)\geq0.
        \end{gather}
        Furthermore, the Kullback-Leibler divergence is zero if and only if $P$ and $Q$ are equal almost everywhere.
    \end{property}

\section{Extreme value theory}

    \newdef{Conditional excess}{
        Consider a random variable $X$ with distribution $P$. The conditional probability that $X$ is larger than a given threshold is given by the conditional excess distribution:
        \begin{gather}
            F_u(y) = \mathrm{Pr}(X-u\leq y|X>u) = \frac{P(u+y)-P(u)}{1-P(u)}.
        \end{gather}
    }

    \newdef{Extreme value distribution}{
        The extreme value distribution is given by the following formula:
        \begin{gather}
            F(x;\xi) = \exp\left(-(1+x\xi)^{-1/\xi}\right).
        \end{gather}
        In the case that $\xi=0$, one can use the definition of the Euler number to rewrite the definition as
        \begin{gather}
            F(x;0)=\exp(-e^{-x}).
        \end{gather}
        The number $\xi$ is called the \textbf{extreme value index}.
    }

    \newdef{Maximum domain of attraction}{
        The (maximum) domain of attraction of a distribution function $H$ consist of all distribution functions $F$ for which there exist sequences $(a_n>0)_{n\in\mathbb{N}}$ and $\seq{b}$ such that $F^n(a_nx+b_n)\longrightarrow H(x)$.
    }

    \begin{theorem}[Fischer, Tippett \& Gnedenko]
        Consider a sequence of i.i.d. random variables with distribution $F$. If $F$ lies in the domain of attraction of $G$, then $G$ has the form of an extreme value distribution.
    \end{theorem}

    \begin{theorem}[Pickands, Balkema \& de Haan]
        Consider a sequence of i.i.d. random variables with conditional excess distribution $F_u$. If the distribution $F$ lies in the domain of attraction of the extreme value distribution, the conditional excess distribution $F_u$ converges to the generalised Pareto distribution when $u\longrightarrow\infty$.
    \end{theorem}

\section{Copulas}

    \begin{property}
        Consider a continuous random variable $X$ and let $U$ be the result of the probability integral transformation, i.e. $U = F_X(X)$. This transformed random variable has a uniform cumulative distribution, i.e. $F_U(u) = u$.
    \end{property}

    \newdef{Copula}{\index{copula}
        The joint cumulative distribution function of a random variable with uniform marginal distributions.
    }
    The following alternative definition is more analytic in nature:
    \newadef{Copula}{
        A function $C:[0,1]^d\rightarrow[0,1]$ satisfying the following properties:
        \begin{enumerate}
            \item\textbf{Normalization} $C(x_1,\ldots,x_d)=0$ if any of the $x_i$ is zero.
            \item\textbf{Uniformity:} $C(1,1,\ldots,x_i,1,\ldots)=x_i$ for all $1\leq i\leq d$.
            \item\textbf{$d$-nondecreasing:} For every box $B=\prod_{1\leq i\leq d}[a_i,b_i]\subseteq[0,1]^d$ the $C$-volume is nonnegative:
            \begin{gather}
                \int_BdC := \sum_{\mathbf{z}\in\prod_i\{a_i,b_i\}}(-1)^{N_b(\mathbf{z})}C(\mathbf{z})\geq0,
            \end{gather}
            where $N_B(\mathbf{z}) = \mathrm{Card}(\{i\mid a_i=z_i\})$.
        \end{enumerate}
    }

    \begin{theorem}[Sklar]
        For every joint distribution function $H$ with marginals $F_i$ there exists a unique copula $C$ such that
        \begin{gather}
            H(x_1,\ldots,x_d) = C(F_1(x_1),\ldots,F_d(x_d)).
        \end{gather}
    \end{theorem}

    \begin{property}[Fr\'echet-Hoeffding]
        Every copula $C:[0,1]^d\rightarrow[0,1]$ is bounded in the following way:
        \begin{gather}
            \max\left(\sum_{i=1}^du_i-d+1,0\right)\leq C(u_1,\ldots,u_d)\leq \min_iu_i
        \end{gather}
        for all $(u_1,\ldots,u_d)\in[0,1]^d$. Furthermore, the upper bound is sharp, i.e. $\min_iu_i$ is itself a copula.\footnote{The lower bound is only a copula for $d=2$. In general this bound is only pointwise sharp.}
    \end{property}

    \newdef{Extreme value copula}{
        A copula $C$ for which there exists a copula $\widetilde{C}$ such that
        \begin{gather}
            \left[\widetilde{C}(u_1^{1/n},\ldots,u_d^{1/n})\right]^n\longrightarrow C(u_1,\ldots,u_d)
        \end{gather}
        for all $(u_1,\ldots,u_d)\in[0,1]^d$.
    }
    \begin{property}
        A copula $C$ is an extreme value copula if and only if it is stable in the following sense:
        \begin{gather}
            C(u_1,\ldots,u_d) = \left[C(u_1^{1/n},\ldots,u_d^{1/n})\right]^n
        \end{gather}
        for all $n\geq1$.
    \end{property}

\section{\difficult{Randomness}}

    This section is strongly related to Section \ref{section:turing} on computability theory.

    \newdef{Kolmogorov randomness}{\index{Kolmogorov!randomness}
        Consider a \textit{universal Turing machine} $U$. The \textbf{Kolmogorov complexity} $C(\kappa)$ of a finite bit string $\kappa$ (with respect to $U$) is defined as
        \begin{gather}
            C(\kappa) := \min\{|\sigma|\mid\sigma\text{ is finite}, U(\sigma)=\kappa\}.
        \end{gather}
        A finite bit string is said to be Kolmogorov random (with respect to $U$) if there exists an integer $n\in\mathbb{N}$ such that $C(\kappa)\geq|\sigma|-n$.
    }

    \begin{property}
        For every universal Turing machine there exists at least one Kolmogorov random string. This easily follows from the pigeonhole principle since for every $n\in\mathbb{N}$ there are $2^n$ strings of length $n$ but only $2^n-1$ programs of length less than $n$.
    \end{property}
    \remark{Note that, although universal Turing machines can emulate each other, the randomness of a string is not absolute. Its randomness depends on the chosen machine.}

    It would be pleasing if this notion of randomness could easily be extended to infinite bit strings, for example by giving such a string the label random if there exists a uniform choice of constant $k$ such that all initial segments of the string are $k$-random. However, by a result of \textit{Martin-L\"of}, there does not exist any string satisfying this condition.
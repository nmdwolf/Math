\chapter{Statistics}\label{chapter:statistics}

    In this chapter, most definitions and formulas will be based on either a standard calculus approach or a data-driven approach. For a measure-theoretic approach, see Chapter \ref{chapter:probability}. For some sections the language of information geometry will be used as introduced in the previous chapter.

\section{Data samples}
\subsection{Moment estimators}

    \newformula{$r^{th}$ sample  moment}{\index{moment}\label{statistics:sample_moment}
        \begin{gather}
            \overline{x^r} := \frac{1}{N}\sum_{i=1}^Nx_i^r
        \end{gather}
    }
    \begin{example}[Arithmetic mean]\index{mean}\label{statistics:arithmetic_mean}
        The arithmetic mean is used to average out differences between measurements. It is defined as the first sample moment:
        \begin{gather}
            \overline{x} := \frac{1}{N}\sum_{i=1}^Nx_i.
        \end{gather}
    \end{example}

    \begin{theorem}[Weak law of large numbers\footnotemark]\index{law!of large numbers}\index{Khinchin's law}
        \footnotetext{Also called \textbf{Khinchin's law}.}
        Assume that the sequence $\seq{X}$ of random variables is i.i.d. The sample average converges in probability \ref{prob:convergence_in_probability} to the expectation value:
        \begin{gather}
            \lim_{n\rightarrow\infty}\mathrm{Pr}\left(|\overline{X}_n-\mu|>\varepsilon\right)=0
        \end{gather}
        for all $\varepsilon>0$.
    \end{theorem}
    \begin{theorem}[Strong law of large numbers\footnotemark]\index{law!of large numbers}\index{Kolmogorov!law}\label{prob:strong_lln}
        \footnotetext{Also called \textbf{Kolmogorov's law}.}
        Assume that the sequence $\seq{X}$ of random variables is i.i.d. The sample average converges almost surely \ref{lebesgue:almost_everywhere} to the expectation value:
        \begin{gather}
            \mathrm{Pr}\left(\lim_{n\rightarrow\infty}\overline{X}_n=\mu\right)=0.
        \end{gather}
        In fact, the i.i.d. assumption can be weakened. The convergence
        \begin{gather}
            \overline{X}_n\overset{\text{a.s.}}{\longrightarrow}\expect{\overline{X}_n}
        \end{gather}
        holds as long as the random variables are independent, have finite second moment and satisfy
        \begin{gather}
            \sum_{k=1}^\infty\frac{1}{k^2}\variance{X_k}<\infty.
        \end{gather}
    \end{theorem}

    \newformula{$r^{th}$ central sample moment}{\label{statistics:central_sample_moment}
        \begin{gather}
            m_r := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^r
        \end{gather}
    }

    \newdef{Weighted mean}{\label{statistics:weighted_mean}
        Let $f:\mathbb{R}\rightarrow\mathbb{R}^+$ be a weight function. The weighted mean is given by:
        \begin{gather}
            \overline{x} := \frac{\sum_if(x_i)x_i}{\sum_if(x_i)}.
        \end{gather}
    }
    \begin{example}[Binned mean]\label{statistics:binned_arithmetic_mean}
        If the data has been grouped in bins, the weight function is given by the number of elements in each bin.
        \begin{gather}
            \overline{x} = \frac{1}{N}\sum_{i=1}n_ix_i.
        \end{gather}
    \end{example}
    \begin{remark}
        In the above definitions, the measurements $x_i$ can be replaced by function values $f(x_i)$ to calculate the mean of the function $f(x)$. This follows from Theorem \ref{prob:unconscious_statistician}. However, it is also important to keep in mind that $\overline{f(x)} \neq f(\overline{x})$. The equality only holds for linear functions.
    \end{remark}

    \newdef{Geometric mean}{\label{statistics:geometric_mean}
        Let $\{x_i\}$ be a data set taking values in either $\mathbb{R}_+$ or $\mathbb{R}_-$. The geometric mean is used to average out \textit{normalized} measurements, i.e. ratios with respect to a reference value.
        \begin{gather}
            g := \left(\prod_{i=1}^Nx_i\right)^{1/N}
        \end{gather}
        The following relation exists between the arithmetic and geometic mean:
        \begin{gather}
            \ln g = \overline{\ln x}.
        \end{gather}
    }

    \newdef{Harmonic mean}{\label{statistics:harmonic_mean}
        \begin{gather}
            h := \left(\frac{1}{N}\sum_{i=1}^Nx_i^{-1}\right)^{-1}
        \end{gather}
        The following relation exists between the arithmetic and harmonic mean:
        \begin{gather}
            \frac{1}{h} = \overline{x^{-1}}.
        \end{gather}
    }

    \begin{property}
        Let $\{x_i\}$ be a data set taking values in $\mathbb{R}_+$.
        \begin{gather}
            h\leq g\leq\overline{x}
        \end{gather}
        The equalities only hold when all $x_i$ are equal.
    \end{property}

    \newdef{Mode}{\index{mode}
        The most occurring value in a data set.
    }
    \newdef{Median}{\index{median}
        The element $x_i$ in a data set such that half of the values is greater than $x_i$ and half of the values is smaller than $x_i$.
    }

\subsection{Dispersion}

    \newdef{Range}{\index{range}
        The simplest indicator for statistical dispersion:
        \begin{gather}
            R := x_{\max} - x_{\min}.
        \end{gather}
        However, it is very sensitive for outliers.
    }

    \newdef{Mean absolute difference}{
        \begin{gather}
            \mathrm{MD} := \frac{1}{N}\sum_{i=1}^N|x_i - \overline{x}|
        \end{gather}
    }

    \newdef{Sample variance}{\index{variance}\label{statistics:sample_variance}
        \begin{gather}
            \mathrm{Var}(x) := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^2
        \end{gather}
    }
    \begin{formula}
        The variance can also be rewritten in the following way:
        \begin{gather}
            \label{statistics:variance_without_sum}
            \mathrm{Var}(x) = \overline{x^2} - \overline{x}^2.
        \end{gather}
    \end{formula}
    \begin{remark}[Bessel corection]\index{Bessel!correction}
        A better estimator for the variance of a sample is given by the following formula:
        \begin{gather}
            \label{statistics:bessel_correction}
            \hat{s}^2 := \frac{1}{N-1}\sum_{i=1}^N(x_i - \overline{x})^2.
        \end{gather}
        See Remark \ref{statistics:variance_bessel_correction} for more information.
    \end{remark}

    \newdef{Skewness}{\index{skewness}\label{statistics:skewness}
        The skewness $\gamma$ describes the asymmetry of a distribution. It is defined as the proportionality constant relating the third central moment and the standard deviation:
        \begin{gather}
            m_3 = \gamma\sigma^3.
        \end{gather}
        A positive skewness indicates a tail to the right or alternatively a median smaller than $\overline{x}$. A negative skewness indicates a median larger than $\overline{x}$.
    }
    \newdef{Pearson's mode skewness}{\index{Pearson!skewness|see{skewness}}\label{statistics:pearsons_skewness}
        \begin{gather}
            \gamma_P := \frac{\overline{x} - \mathrm{mode}}{\sigma}
        \end{gather}
    }

    \newdef{Kurtosis}{\index{kurtosis}\label{statistics:kurtosis}
        The kurtosis $c$ is an indicator for the ``tailedness''. It is defined as the proportionality constant relating the fourth central moment and the standard deviation:
        \begin{gather}
            m_4 = c\sigma^4.
        \end{gather}
    }
    \newdef{Excess kurtosis}{
        The excess kurtosis is defined as $c-3$. This fixes the excess kurtosis of all univariate normal distributions at 0. A positive excess is an indicator for long ``fat'' tails, a negative excess indicates short ``thin'' tails.
    }

    \newdef{Percentile}{\index{percentile}
        The $p$-percentile $c_p$ is defined as the value that is larger than $p\%$ of the measurements. The median is the 50-percentile.
    }

    \newdef{Interquartile range}{
        The difference between the upper and lower quartile (75- and 25-percentiles respectively).
    }

    \newdef{Full Width at Half Maximum}{\index{FWHM}
        \nomenclature[A_FWHM]{FWHM}{full width at half maximum}
        The difference between the two values of the independent variable where the dependent variable is half of its maximum. This quantity is often denoted by the abbreviation \textbf{FWHM}.
    }
    \begin{property}
        For Gaussian distributions the following relation exists between the FWHM and the standard deviation:
        \begin{gather}
            \mathrm{FWHM} = 2.35\sigma.
        \end{gather}
    \end{property}

\subsection{Multivariate data sets}

    When working with bivariate (or even multivariate) distributions it is useful to describe the relationship between the different random variables.

    \newdef{Covariance}{\index{covariance}\label{statistics:covariance}
        The covariance of two data sequences is defined as follows:
        \begin{gather}
            \mathrm{cov}(x,y) := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})(y_i - \overline{y}) = \overline{xy} - \overline{x}\ \overline{y}.
        \end{gather}
        The covariance is also often denoted by $\sigma_{xy}$ because of the next property:
    }
    \begin{property}
        The covariance and standard deviation are related by the following equality:
        \begin{gather}
            \sigma_x^2 = \sigma_{xx}.
        \end{gather}
    \end{property}

    \newdef{Correlation coefficent}{\index{correlation}\label{statistics:correlation_coefficient}
        \begin{gather}
            \rho_{xy} := \frac{\mathrm{cov}(x,y)}{\sigma_x\sigma_y}
        \end{gather}
        The correlation coefficient is bounded to the interval $[-1,1]$. It should be noted that its magnitude is only an indicator for the linear dependence.
    }
    \begin{remark}
        For multivariate distributions the above definitions can be generalized using matrices:
        \begin{align}
            \label{statistics:covariance_matrix}
            V_{ij} &= \mathrm{cov}(x_{(i)},x_{(j)})\\
            \label{statistics:correlation_matrix}
            \rho_{ij} &= \rho_{(i)(j)}.
        \end{align}
    \end{remark}

\section{Probability distributions}

    In the following sections and subsections, all distributions will be taken to be continuous. The formulas can be generalized to discrete distributions by replacing the integral with a summation.

    \begin{definition}[Percentile]
        The $p$-percentile $c_p$ of a distribution $F$ is defined as:
        \begin{gather}
            c_p = F^{-1}(p).
        \end{gather}
    \end{definition}

    \newdef{Parametric family}{\index{para-!metric family}
        A family of probability densities indexed by one or more parameters $\theta$.
    }

    \begin{example}[Mixture family]\index{mixture}
        Consider a collection of distributions $\mathcal{P}=\{P_i\}_{i\leq n}$. The mixture family generated by $\mathcal{P}$ consist of all convex combintations of elements in $\mathcal{P}$:
        \begin{gather}
            \left\{\sum_{i=1}^nw_iP_i\,\middle\vert\,w_i\geq0,\sum_{i=1}^nw_i = 1\right\}.
        \end{gather}
        Every element of this family is called a \textbf{mixture distribution}.
    \end{example}

\subsection{Empirical distribution}

    \newdef{Empirical distribution function}{\index{distribution!empirical}\label{statistics:empirical_distribution}
        The (discrete) empirical probability distribution function is defined as the uniform mixture distribution with Dirac measures at the observations:
        \begin{gather}
            F_n := \frac{1}{n}\sum_{i=1}^n\delta_{x_i}.
        \end{gather}
    }

    \begin{theorem}[Borel's law of large numbers]\index{law!of large numbers}\label{statistics:large_numbers}
        If the sample size approaches infinity, the observed frequencies approach the theoretical propabilities.
    \end{theorem}
    \begin{result}[Frequentist probability\footnotemark]
        \footnotetext{Also called the \textbf{empirical probability}.}
        \begin{gather}
            \label{statistics:frequentist_probability}
            \mathrm{Pr}(x) := \lim_{n\rightarrow\infty}\frac{f_n(x)}{n}
        \end{gather}
    \end{result}

    The law of large numbers can also be phrased in terms of the empirical distribution function:
    \begin{theorem}[Glivenko-Cantelli]\index{Glivenko-Cantelli}\label{statistics:glivenko_cantelli}
        Consider a cumulative distribution function $F$ on a probability space $\Omega$. Denote the empirical distribution function of $n$ random variables on $\Omega$ by $F_n$. If the random variables are i.i.d. according to $F$, then
        \begin{gather}
            \sup_{x\in\Omega}|F(x)-F_n(x)|\overset{\text{a.s.}}{\longrightarrow}0.
        \end{gather}
    \end{theorem}
    \begin{remark}
        The law of the large numbers implies pointwise convergence of the empirical distribution function, while the Glivenko-Cantelli theorem strengthens this to uniform convergence.
    \end{remark}

    The quantity in the Glivenko-Cantelli theorem is important enough to get its own name:
    \newdef{Kolmogorov-Smirnov statistic}{\label{statistics:kolmogorov_smirnov_statistic}
        Let $F$ be a given cumulative distribution function. The $n^{th}$ Kolmogorov-Smirnov statistic is defined as follows:
        \begin{gather}
            D_n := \sup_{x\in\Omega}|F_n(x) - F(x)|.
        \end{gather}
    }

    \newdef{Kolmogorov distribution}{\index{distribution!Kolmogorov}\label{statistics:kolmogorov_distribution_cumulative}
        \begin{gather}
            F_\mathrm{Kol}(x) := 1 - 2\sum_{i=1}^\infty(-1)^{i-1}e^{-2i^2x^2} = \frac{\sqrt{2\pi}}{x}\sum_{i=1}^\infty e^{-(2i-1)^2\pi^2/(8x^2)}
        \end{gather}
    }

    \newprop{Kolmogorov-Smirnov test}{\index{test!Kolmogorov-Smirnov}
        Let the null hypothesis $H_0$ state that a given data sample is described by a cumulative distribution function $F$. The null hypothesis is rejected at significance level $\alpha$ if
        \begin{gather}
            \sqrt{n}D_n > K_{\alpha},
        \end{gather}
        where $K_\alpha$ is defined by the Kolmogorov distribution: $F_\mathrm{Kol}(K_\alpha) = 1-\alpha$.
    }

    \newdef{Glivenko-Cantelli class}{\index{Glivenko-Cantelli!class}\label{statistics:glivenko_cantelli_class}
        Consider a set of measurable functions $\mathcal{F}$ on a measurable space $(\Omega,\Sigma)$. For every probability measure $P$ on $\Omega$, one can define the $\mathcal{F}$-norm as follows:
        \begin{gather}
            \|P\|_\mathcal{F} := \sup\{\mathrm{E}_P[f]\mid f\in\mathcal{F}\}.
        \end{gather}
        A class $\mathcal{F}$ of measurable functions is said to be Glivenko-Cantelli with respect to a probability measure $P$ if it satisfies
        \begin{gather}
            \|P_n-P\|_\mathcal{F}\overset{\text{a.s.}}{\longrightarrow}0,
        \end{gather}
        where $P_n$ is the empirical measure.\footnote{If the convergence only holds in probability, the class is said to be \textbf{weakly GC}.} The Glivenko-Cantelli theorem \ref{statistics:glivenko_cantelli} says that the indicator functions of the sets $]-\infty,x]$ form a Glivenko-Cantelli class.\footnote{Because every indicator function is uniquely associated to a set, one can also speak of GC classes of measurable sets.} In fact, they are \textbf{universally GC} because this theorem applies to all probability measures on $\Omega$. A class is said to be \textbf{uniformally GC} if the convergence holds uniformly over all probability measures.
    }
    \begin{remark}
        Note that by the law of large numbers every singleton class is Glivenko-Cantelli (also universally and uniformly). The above definition strengthens the convergence of all elements of $\mathcal{F}$ to uniform convergence.
    \end{remark}

    \begin{property}[Bracketing number]
        Consider a collection of measurable functions $\mathcal{F}$ on a meaurable space $(\Omega,\Sigma)$ and recall Definition \ref{functional:bracket} of the bracketing number. If the bracketing number $N_{[\,]}(\varepsilon,\mathcal{F},\|\cdot\|_1)$ is finite for all $\varepsilon>0$, then $\mathcal{F}$ is Glivenko-Cantelli (with respect to the probability measure that induces the $L^1$-norm $\|\cdot\|_1$).
    \end{property}
    \begin{property}[Metric entropy]\label{statistics:entrop_GC}
        Consider a collection of measurable functions $\mathcal{F}$ on a meaurable space $(\Omega,\Sigma)$ and recall Definition \ref{metric:covering_number} of the metric entropy. Moreover, assume that $\mathcal{F}$ admits an integrable envelope $F$. Let $\mathcal{F}_M$ denote the collection of functions $f\mathbbm{1}_{F\leq M}$, where $f\in\mathcal{F}$. If
        \begin{gather}
            \frac{1}{n}\ln N_C(\varepsilon,\mathcal{F}_M,\|\cdot\|_1)\overset{d}{\longrightarrow}0,
        \end{gather}
        where $\|\cdot\|_1$ is the $L^1$-norm associated to the empirical measure $P_n$, for all $\varepsilon>0$ and $M>0$, then $\mathcal{F}$ is Glivenko-Cantelli.
    \end{property}

    \begin{property}[Symmetrized empirical measure]
        Let $\{\sigma_1,\ldots,\sigma_n\}$ be a set of i.i.d. Rademacher variables \ref{prob:rademacher}. The symmetrized empirical measure is defined as follows:
        \begin{gather}
            P^\sigma_n:f\mapsto\frac{1}{n}\sum_{i=1}^n\sigma_if(x_i).
        \end{gather}
        Given a collection $\mathcal{F}$ of measurable functions on $(\Omega,\Sigma,P)$, the following inequality holds:
        \begin{gather}
            \expect{\|P_n-P\|_\mathcal{F}}\leq2\expect{\|P^\sigma_n\|_\mathcal{F}}.
        \end{gather}
    \end{property}

    \begin{theorem}[Donsker]\index{Donsker}
        Consider an empirical distribution function $F_n$ and define its normalized empirical process as
        \begin{gather}
            G_n := \sqrt{n}(F_n-F),
        \end{gather}
        where $F$ is the cumulative distribution function of the random variables $X_i$. The central limit theorem says that the empirical process converges in distribution to a standard normal distribution for every $x\in\mathbb{R}$. This can be strengthened as follows:
        \begin{gather}
            G_n\overset{d}{\longrightarrow}U
        \end{gather}
        in $D(\mathbb{R},\|\cdot\|_\infty)$, where $U$ is a standard Brownian bridge and $D(\mathbb{R},\|\cdot\|_\infty)$ denotes the space of c\`adl\`ag functions equipped with the supremum metric \ref{metric:supremum_distance}.
    \end{theorem}

\subsection{Common distributions}

    \newdef{Uniform distribution}{\index{distribution!uniform}\label{statistics:uniform_distr}
        \begin{align}
            f(x;a,b) &:=
            \begin{cases}
                \frac{1}{b-a}&a\leq x\leq b\\
                0&\text{elsewhere}
            \end{cases}\\\nonumber\\
            \expect{x} &= \frac{a+b}{2}\\\nonumber\\
            \variance{x} &= \frac{(b-a)^2}{12}
        \end{align}
    }

    \newdef{Gaussian distribution}{\index{distribution!normal}\index{Gauss!distribution}\label{statistics:normal_distr}
        Let $\sigma$ be a positive number.
        \begin{gather}
            \mathcal{N}(x;\mu,\sigma) := \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
        \end{gather}
        This distribution is also called a (univariate) \textbf{normal distribution} with \textbf{mean} $\mu$ and \textbf{standard deviation} $\sigma$ (or \textbf{variance} $\sigma^2$).

        This formula can also be generalized to multivariate distributions. Let $\Sigma$ be a positive-definite matrix, the \textbf{covariance} matrix. The associated multivariate normal distribution is given by
        \begin{gather}
            \mathcal{N}(\vector{x};\vector{\mu},\Sigma) = \frac{1}{\sqrt{2\pi}\det(\Sigma)}\exp\left(-\frac{(\vector{x}-\vector{\mu})^T\Sigma^{-1}(\vector{x}-\vector{\mu})}{2}\right).
        \end{gather}
    }

    \newdef{Standard normal distribution}{\index{error!function}\label{statistics:standard_normal_distr}
        \begin{gather}
            \mathcal{N}(z) := \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
        \end{gather}
        The cumulative distribution of $\mathcal{N}$ is called the \textbf{error function} $\mathrm{erf}(x)$.
    }
    \begin{remark}\index{standardization}
        Every Gaussian distribution can be transformed into a standard normal distribution by passing to the random variable $Z=\tfrac{X-\mu}{\sigma}$. This transformation is often called \textbf{standardization}.
    \end{remark}

    \begin{theorem}[Central limit theorem]\index{central!limit theorem}\label{statistics:CLT}
        A sum of $n$ i.i.d. random variables $X_i$ distributed according to a distribution with mean $\mu$ and variance $\sigma^2$ satisfies the following property:
        \begin{gather}
            \sqrt{n}\left(\sum_{i=1}^nX_i - \mu\right)\overset{d}{\longrightarrow}\mathcal{N}(0,\sigma^2).
        \end{gather}
    \end{theorem}
    \begin{remark}
        If the random variables are not independent, the CLT will not hold. However, a generalization to distributions that are not identical exists. These are the \textit{Lyapunov} and \textit{Lindeberg} CLTs. (This generalization does require additional conditions on the higher moments.)
    \end{remark}
    \begin{formula}
        The sum of any number of (independent) Gaussian random variables is again Gaussian with the sum of the means and variances as parameters:
        \begin{gather}
            \forall i\in I:X_i\sim\mathcal{N}(\mu_i,\sigma^2_i)\implies\sum_{i\in I}X_i\sim\mathcal{N}\left(\sum_{i\in I}\mu_i,\sum_{i\in I}\sigma^2_i\right).
        \end{gather}
    \end{formula}

    \newdef{Exponential distribution}{\index{distribution!exponential}\label{statistics:exponential_distr}
        \begin{align}
            f(x;\tau) &:= \frac{1}{\tau}e^{-\frac{x}{\tau}}\\\nonumber\\
            \expect{x} &= \tau\\\nonumber\\
            \variance{x} &= \tau^2.
        \end{align}
    }
    \begin{property}\index{memory}\label{statistics:memoryless_exponential_distribution}
        The exponential distribution is \textbf{memoryless}:
        \begin{gather}
            \mathrm{Pr}(X>x_1+x_2\mid X>x_2) = \mathrm{Pr}(X>x_1).
        \end{gather}
    \end{property}

    \newdef{Bernoulli distribution}{\index{distribution!Bernoulli}\index{Bernoulli|seealso{distribution}}\label{statistics:bernoulli_distr}
        A random variable that can only take 2 possible values is described by a Bernoulli distribution. When the possible values are 0 and 1, with respective chances $\rho$ and $1-\rho$, the distribution is given by
        \begin{align}
            p(k;\rho) &:= \rho^k(1-\rho)^{1-k}\\\nonumber\\
            \expect{k} &= \rho\\\nonumber\\
            \variance{k} &= \rho(1-\rho).
        \end{align}
    }

    \newdef{Binomial distribution}{\index{distribution!binomial}\label{statistics:binomial_distr}
        A process with $n$ i.i.d.~Bernoulli trials with probability $\rho$, is described by a binomial distribution:
        \begin{align}
            \mathrm{Binom}(k;\rho,n) &:= \binom{n}{k}\rho^k(1-\rho)^{n-k}\\\nonumber\\
            \expect{k} &= n\rho\\\nonumber\\
            \variance{k} &= n\rho(1-\rho).
        \end{align}
    }

    \newdef{Poisson distribution}{\index{distribution!Poisson}\label{statistics:poisson_distr}
        A process with known possible outcomes but an unknown number of events is described by a Poisson distribution with average expected number of events $\lambda$.
        \begin{align}
            \mathrm{Poisson}(r;\lambda) &:= \frac{e^{-\lambda}\lambda^r}{r!}\\\nonumber\\
            \expect{r} &= \variance{r} = \lambda.
        \end{align}
    }
    \begin{formula}
        If multiple independent Poisson processes occur simultaneously, the probability of $r$ events is also described by a Poisson distribution:
        \begin{gather}
            \forall i\in I:X_i\sim\mathrm{Poisson}(\lambda_i)\implies\sum_{i\in I}X_i\sim\mathrm{Poisson}\left(\sum_{i\in I}\lambda_i\right).
        \end{gather}
        The number of events coming from the process described by $\lambda_i|r=\sum_{i\in I}\lambda_i$ is given by a binomial distribution $\mathrm{Binom}(r_i;\Lambda_i,r)$ with $\Lambda_i = \frac{\lambda_i}{\sum_{i\in I}\lambda_i}$.
    \end{formula}
    \begin{remark}
        For $\lambda\longrightarrow\infty$, the Poisson distribution $\mathrm{Poisson}(r;\lambda)$ can be approximated by a Gaussian distribution $\mathcal{N}(x;\lambda,\sqrt{\lambda})$.
    \end{remark}
    \begin{theorem}[Raikov]\index{Raikov}
        If the sum of two independent random variables is Poisson, the individual random variables are also Poisson.
    \end{theorem}

    \newdef{$\chi^2$-distribution}{\index{distribution!$\chi^2$}\label{statistics:chi_squared_distr}
        The sum of $k$ squared independent (standard) normally distributed random variables $Y_i$ defines the random variable:
        \begin{gather}
            \chi^2_k := \sum_{i=1}^kY_i^2,
        \end{gather}
        where $k$ is said to be the number of \textbf{degrees of freedom}. The associated density is
        \begin{gather}
            f(\chi^2;n) := \frac{\chi^{n-2}e^{-\frac{\chi^2}{2}}}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}.
        \end{gather}
    }
    \begin{property}
        Due to the CLT \ref{statistics:CLT}, the $\chi^2$-distribution approximates a Gaussian distribution for large $n$:
        \begin{gather}
            f(\chi^2;n)\overset{n>30}{\longrightarrow}\mathcal{N}(\sqrt{2\chi^2};\sqrt{2n-1},1).
        \end{gather}
    \end{property}

    \newdef{Student-$t$ distribution}{\index{distribution!Student-$t$}\label{statistics:student_t_distr}
        The Student-$t$ distribution describes the difference between the true mean and a sample average with estimated standard deviation $\hat{\sigma}$:
        \begin{gather}
            f(t;n) := \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\ \Gamma\left(\frac{n}{2}\right)\left(1 + \frac{t^2}{n}\right)^{\frac{n+1}{2}}},
        \end{gather}
        where
        \begin{gather}
            t := \frac{(x-\mu)/\sigma}{\hat{\sigma}/\sigma} = \frac{z}{\sqrt{\chi^2/n}}.
        \end{gather}
    }

    \newdef{Cauchy distribution\footnotemark}{\index{Cauchy!distribution}\index{Breit-Wigner|see{Cauchy distribution}}\label{statistics:cauchy_distribution}
        \footnotetext{Also known, especially in particle physics, as the \textbf{Breit-Wigner} distribution.}
        The general density $f(x;x_0,\gamma)$ is given by
        \begin{gather}
            f(x;x_0,\gamma) := \frac{1}{\pi}\frac{\gamma}{(x - x_0)^2 + \gamma^2}.
        \end{gather}
        The associated characteristic function is given by
        \begin{gather}
            \expect{e^{itx}} = e^{ix_0t - \gamma|t|}.
        \end{gather}
    }
    \begin{remark}
        Both the mean and variance of the Cauchy distribution are undefined.
    \end{remark}

\section{Errors}

    \newdef{Systematic error}{\index{error}
        Errors that always have the same effect independent of the measurements itself, i.e. they shift all values in the same way and cannot be directly inferred from the measurements. Note that they are not necessarily independent of each other.
    }

    \begin{formula}[Inverse-variance averaging]
        When performing a sequence of measurements $x_i$ with different variances $\sigma_i^2$, it is impossible to use the arithmetic mean \ref{statistics:arithmetic_mean} in a meaningful way because the measurements are not of the same type. Therefore, it is also impossible to apply the CLT \ref{statistics:CLT}.

        These problems can be resolved by the using the weighted mean \ref{statistics:weighted_mean}:
        \begin{gather}
            \overline{x} := \frac{\sum_i\frac{x_i}{\sigma_i^2}}{\sum_i\frac{1}{\sigma_i^2}}.
        \end{gather}
        The variation of the weighted mean is given by
        \begin{gather}
            \label{statistics:weighted_mean_variance}
            \mathrm{Var}(\overline{x}) := \frac{1}{\sum_i\sigma_i^{-2}}.
        \end{gather}
    \end{formula}

    \newformula{Error propagation}{\index{propagation!of errors}
        Let $X$ be a vector random variable such that functions of $X$ admit a good first-order Taylor approximation around the mean (this usually means that the covariance is small). The variance of a general function of $X$ is given by
        \begin{gather}
            \label{statistics:error_propagation}
            \variance{f(X)} \approx \sum_{i=1}^n\left(\pderiv{f}{X_i}\right)^2\variance{X_i} + \sum_{i\neq j}\left(\pderiv{f}{X_i}\right)\left(\pderiv{f}{X_j}\right)\mathrm{cov}[X_i,X_j].
        \end{gather}
    }
    \begin{result}
        The correlation coefficient \ref{statistics:correlation_coefficient} of a random variable $X$ and a \textbf{linear} function of $X$ is independent of $\sigma_x$ and is always equal to $\pm1$.
    \end{result}

    \newdef{Fractional error}{\index{fractional error}
        Let $X,Y$ be two independent random variables. The standard deviation of $f(X,Y) = XY$ is given by the fractional error:
        \begin{gather}
            \label{statistics:fractional_error}
            \left(\frac{\sigma_f}{f}\right)^2 = \left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2.
        \end{gather}
        The fractional error of a variable is equal to the fractional error of the reciprocal of that variable.
    }

    \begin{property}[Logarithm]
        Let $X$ be a random variable. The error of the logarithm of $X$ is equal to the fractional error of $X$.
    \end{property}

    \newformula{Covariance of functions}{\index{covariance}
        \begin{gather}
            \label{statistics:covariance_functions}
            \mathrm{cov}[f,g] \approx \sum_{i,j}\left(\pderiv{f}{X_i}\right)\left(\pderiv{g}{X_j}\right)\mathrm{cov}[X_i,X_j]
        \end{gather}
    }
    \begin{result}
        Let $f\equiv(f_1,\ldots,f_k)$ be a vector-valued function. The covariance matrix is to first order given by
        \begin{gather}
            \variance{f(X)} \approx J\variance{X}J^T,
        \end{gather}
        where $J$ is the Jacobian matrix of $f$.
    \end{result}

\section{Parameter estimation}
\subsection{General properties}

    \newdef{Consistency}{\index{consistency}\label{statistics:consistency}
        An estimator $\hat{a}$ is said to be consistent if it is asymptotically equal to the true parameter:
        \begin{gather}
            \lim_{N\rightarrow\infty}\hat{a} = a.
        \end{gather}
    }
    \newdef{Unbiased estimator}{\label{statistics:unbiased_estimator}
        An estimator $\hat{a}$ is said to be unbiased if its expectation value is equal to the true parameter:
        \begin{gather}
            \langle\hat{a}\rangle = a.
        \end{gather}
        Note that neither consistency, nor unbiasedness implies the other.
    }

    \newdef{Bias}{\index{bias}\label{statistics:bias}
        \begin{gather}
            B(\hat{a}) := |\langle\hat{a}\rangle - a|.
        \end{gather}
    }

    \newdef{Mean squared error}{\label{statistics:mean_squared_error}
        \begin{gather}
            \mathrm{MSE}(\hat{a}) := B(\hat{a})^2 + \mathrm{Var}(\hat{a}).
        \end{gather}
    }
    \remark{If an estimator is unbiased, the MSE is equal to the variance of the estimator.}

\subsection{Common estimators}

    \begin{property}[Unbiased mean]
        The CLT \ref{statistics:CLT} implies that the sample mean \ref{statistics:arithmetic_mean} is a consistent and unbiased estimator of the population mean.
    \end{property}
    \begin{formula}[Standard error of the mean]\index{standard!error}
        Using the Bienaym\'e formula \ref{prob:bienayme} one can show that the standard error of the mean, i.e. the standard deviation of the sample mean, is given by the following formula:
        \begin{gather}
            \label{statistics:standard_error}
            \variance{\overline{x}} = \frac{\sigma^2}{N}.
        \end{gather}
    \end{formula}

    \newformula{Variance estimator for known mean}{\index{variance!estimator}
        If the true mean $\mu$ is known, a consistent and unbiased estimator for the variance is given by
        \begin{gather}
            \widehat{\variance{X}} = \frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2.
        \end{gather}
    }
    \newformula{Variance estimator for unknown mean}{\index{Bessel!correction}\label{statistics:variance_bessel_correction}
        If the true mean is unknown and the sample mean has been used to estimate it, a consistent and unbiased estimator is given by
        \begin{gather}
            \hat{s}^2 = \frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x})^2.
        \end{gather}
        The modified factor $\frac{1}{N-1}$ is called the \textbf{Bessel correction}. It corrects the bias of the estimator given by the sample variance \ref{statistics:sample_variance}. The consistency is guaranteed by the CLT.
    }

    \begin{property}[Characterization of normal distributions]
        The class of normal distributions is uniquely characterized by those distributions for which the sample mean and sample variance are independent.
    \end{property}

\subsection{Estimation error}

    \newformula{Variance of the estimator of the variance}{
        \begin{gather}
            \mathrm{Var}\left(\widehat{\variance{X}}\right) =  \frac{(N-1)^2}{N^3}\langle(x - \langle x \rangle)^4\rangle - \frac{(N-1)(N-3)}{N^3}\langle(x - \langle x \rangle)^2\rangle^2
        \end{gather}
    }
    \newformula{Variance of the estimator of the standard deviation}{
        \begin{gather}
            \mathrm{Var}(\widehat{\sigma}) = \frac{1}{4\sigma^2}\mathrm{Var}\left(\widehat{\variance{X}}\right)
        \end{gather}
    }
    \begin{remark}
        The previous result is a little odd, as one has to know the true standard deviation to compute the variance of the estimator. This problem can be solved in two ways. Either a value (hopefully close to the real one) inferred from the sample is used as an estimator, or a guess is used in the design phase of an experiment to see what the possible outcomes are.
    \end{remark}

\subsection{Likelihood function}

    \newdef{Likelihood}{\index{likelihood}\label{statistics:likelihood}
        The likelihood $\mathcal{L}(a;\mathbf{x})$ is the joint density of a set of measurements $\mathbf{x} := \{x_1,\ldots,x_N\}$:
        \begin{gather}
            \mathcal{L}(a;\mathbf{x}) = \prod_{i=1}^Nf(x_i;a).
        \end{gather}
    }

    \begin{theorem}[Cramer-Rao bound]\index{Cramer-Rao}\index{minimum!variance bound}\label{statistics:minimum_variance_bound}
        The variance of an \textbf{unbiased} estimator has a lower bound called the Cramer-Rao bound or \textbf{minimum variance bound (MVB)}:
        \begin{gather}
            \mathrm{Var}(\hat{a})\geq\frac{1}{\left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle}.
        \end{gather}
        For a biased estimator with bias $b$, the MVB takes on the following form:
        \begin{gather}
            \label{statistics:biased_minimum_variance_bound}
            \mathrm{Var}(\hat{a})\geq\frac{\left(1+\deriv{b}{a}\right)^2}{\left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle}.
        \end{gather}
    \end{theorem}
    \begin{remark}
        \begin{gather}
            \left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle = -\left\langle\mderiv{2}{\ln\mathcal{L}}{a}\right\rangle
        \end{gather}
    \end{remark}

    \newdef{Fisher information}{\index{Fisher!information}\label{statistics:fisher_information}
        \begin{gather}
            I_X(a) := \left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle = N\int\left(\deriv{\ln f}{a}\right)^2f\,d\mu
        \end{gather}
        Using this definition one can rewrite the Cramer-Rao inequality as follows:
        \begin{gather}
            \mathrm{Var}(\hat{a})\geq I_X(a).
        \end{gather}
    }

    \newdef{Finite-sample efficiency}{\index{efficient}
        An unbiased estimator is said to be (finite-sample) efficient if it saturates the Cramer-Rao bound. In general the \textbf{efficiency} of (unbiased) estimators is defined through the Cramer-Rao bound as follows:
        \begin{gather}
            e(\hat{a}) := \frac{I_X(a)^{-1}}{\mathrm{Var}(\hat{a})}.
        \end{gather}
    }

\subsection{Maximum likelihood estimation}

    From Definition \ref{statistics:likelihood} it follows that the estimator $\hat{a}_\mathrm{MLE}$ that makes the given measurements most probable is the value of $a$ for which the likelihood function is maximal. It is therefore not the most probable estimator.

    Using Bayes's theorem one finds $f(a\mid x) = f(x\mid a)\frac{f(a)}{f(x)}$. The prior density $f(x)$ is fixed since the values $x_i$ are given by the measurement and, hence, does not vary. The density $f(a)$ is generally assumed to be uniform if there is no prior knowledge about $a$. It follows that $f(a\mid x)$ and $f(x\mid a)$ are proportional and, hence, the logarithms of these functions differ only by an additive constant. This leads to following method for finding an estimator $\hat{a}$:
    \newmethod{Maximum likelihood estimator}{\index{likelihood!estimator}
        The maximum likelihood estimator $\hat{a}$ is obtained by solving the following equation:
        \begin{gather}
            \label{statistics:maximum_likelihood_estimator}
            \left.\deriv{\ln\mathcal{L}}{a}\right|_{a=\hat{a}} = 0.
        \end{gather}
    }
    \remark{MLE estimators are mostly consistent but often biased.}
    \begin{property}
        MLE estimators are invariant under parameter transformations.
    \end{property}
    \result{The invariance implies that the two estimators $\hat{a}$ and $\widehat{f(a)}$ cannot both be unbiased at the same time.}

    \begin{property}
        Every consistent estimator asymptotically becomes unbiased and efficient.
    \end{property}

    \begin{property}[Minimizing KL-divergence]\label{statistics:minimizing_KL}
        It can be shown that maximizing the log-likelihood is equivalent to minimizing the Kullback-Leibler divergence \ref{prob:kullback_leibler} between the would-be density $f(x;\theta)$ and the true density $q(x)$:
        \begin{align*}
            \arg\max_\theta\ln\mathcal{L} &= \arg\max_\theta\sum_{i\in I}\ln f(x_i;\theta)\\
            &= \arg\max_\theta\sum_{i\in I}\ln f(x_i;\theta) - \ln q(x_i)\\
            &= \arg\min_\theta\frac{1}{n}\sum_{i\in I}\ln\frac{q(x_i)}{f(x_i;\theta)}\\
            &\longrightarrow\arg\min_\theta\int q(x;\theta)\ln\frac{q(x)}{f(x;\theta)}\,dx = \arg\min_\theta D_\mathrm{KL}(q\|f_\theta),
        \end{align*}
        where the law of large numbers was used in the last line.
    \end{property}

\subsection{Least squares estimation}

    To fit a (parametric) function $y = f(x;a)$ to a set of 2 variables $(x,y)$, where the $x$ values are exact and the $y$ values have an uncertainty $\sigma_i$, one can use the following method:
    \newmethod{Least squares}{\index{least squares}$ $
        \begin{enumerate}
            \item For every event $(x_i,y_i)$ define the residual $d_i := y_i - f(x_i;a)$.
            \item Determine the $\chi^2$-statistic (analytically):
                \begin{gather}
                    \chi^2 := \sum_i\frac{d_i^2}{f_i},
                \end{gather}
                where $f_i = f(x_i;a)$.
            \item Find the most probable value of $\hat{a}$ by solving the equation
                \begin{gather}
                    \deriv{\chi^2}{a} = 0.
                \end{gather}
        \end{enumerate}
    }
    \begin{property}
        The optimal $\chi^2$-value is asymptotically distributed according to a $\chi^2$-distribution with $n$ degrees of freedom. The parameter $n$ is equal to the number of events $N$ minus the number of fitted parameters $k$. (See more in Section \ref{section:chi_squared_test}.)
    \end{property}

    \newformula{Linear fit}{\index{linear!fit}
        When all uncertainties $\sigma_i$ are equal, the slope $\hat{m}$ and intercept $\hat{c}$ are given by the following formulas:
        \begin{align}
            \label{statistics:least_squares_slope}
            \hat{m} &:= \frac{\overline{xy} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{\mathrm{cov}(x,y)}{\variance{x}}\\
            \label{statistics:least_squares_intercept}
            \hat{c} &:= \overline{y} - \hat{m}\overline{x} = \frac{\overline{x^2} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2}.
        \end{align}
    }
    \remark{The equation $\overline{y} = \hat{c} + \hat{m}\overline{x}$ says that the linear fit passes through the center of mass $(\overline{x},\overline{y})$.}

    \newformula{Errors of linear fit}{
        \begin{align}
            \label{statistics:least_squares_slope_variance}
            \variance{\hat{m}} &= \frac{1}{N(\overline{x^2} - \overline{x}^2)}\sigma^2\\&\nonumber\\
            \label{statistics:least_squares_intercept_variance}
            \variance{\hat{c}} &= \frac{\overline{x^2}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2\\&\nonumber\\
            \label{statistics:least_squares_linear_fit_covariance}
            \mathrm{cov}(\hat{m},\hat{c}) &= \frac{-\overline{x}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2
        \end{align}
    }

    The least squares method is very useful to fit data that has been grouped in bins (histograms):
    \newmethod{Binned least squares}{$ $
        \begin{enumerate}
            \item $N$ i.i.d.~events with density $f(X;a)$ divided in $N_B$ intervals, where the interval $j$ is centered on the value $x_j$, has a width $W_j$ and contains $n_j$ events.
            \item The ideally expected number of events in the $j^{th}$ interval: $f_j = NW_jf(x_j;a)$.
            \item The real number of events has a Poisson distribution: $\overline{n}_j = \sigma_j^2 = f_j$.
            \item Define the binned $\chi^2$ as
                \begin{gather}
                    \chi^2 := \sum_i^{N_B}\frac{(n_i - f_i)^2}{f_i^2}.
                \end{gather}
        \end{enumerate}
    }

\subsection{Geometric approach}

    Consider a sample $\mathbf{x}:=\{x_1,\ldots,x_n\}$ drawn from a distribution $f(x;\theta)$ in an exponential family. The likelihood \ref{statistics:likelihood} is given by \[\mathcal{L}(\theta;\mathbf{x}) := \prod_{i=1}^nf(x_i;\theta).\] The $m$-coordinates of the observed point are
    \begin{gather}
        \eta = \frac{1}{n}\sum_{i=1}^nx_i = \overline{x}.
    \end{gather}
    The optimal value for $\theta$ can be found by maximizing the log-likelihood as before. In Property \ref{statistics:minimizing_KL} it was shown that this is equivalent to minimizing the Kullback-Leibler divergence between the ``true'' distribution $p(x;\xi)$ and the variational solution $f(x;\theta)$. However, in practice the true distribution is not known. Luckily one can replace the true distribution by the empirical distribution in the proof of \ref{statistics:minimizing_KL}. Minimization then corresponds to $m$-projecting the observed point $\eta$ on the submanifold $S$ of ``admissible'' distributions.

    \begin{theorem}[Sanov]\index{Sanov}
        Consider a probability distribution $q$ on a finite set $S$ and draw $n$ i.i.d. samples. Let $P_n$ be the empirical distribution function of the samples \eqref{statistics:empirical_distribution}. Further, let $\Gamma$ be a collection of probability distributions such that $P_n\in\Gamma$. The joint distribution $q^n$ satisfies the following inequality:
        \begin{gather}
            q^n(P_n\in\Gamma) \leq (n+1)^{|S|}2^{-n D_\mathrm{KL}(p^*\|q)},
        \end{gather}
        where $p^*$ is the information projection of $q$ on $\Gamma$. If $\Gamma=\overline{\Gamma^\circ}$, this can be restated as
        \begin{gather}
            \lim_{n\rightarrow\infty}\frac{1}{n}q^n(P_n\in\Gamma) = - D_\mathrm{KL}(p^*\|q).
        \end{gather}
    \end{theorem}

\section{Bayesian modelling}

    \newdef{Conjugate distributions}{\index{conjugate!distribution}
        Consider a prior distribution $F(\theta)$ and a posterior distribution $F(\theta\mid X)$. If these distributions belong to the same family, e.g. they are both Gaussians, they are said to be conjugate. In this case the prior $F(\theta)$ is said to be a \textbf{conjugate prior} for the likelihood $F(X\mid\theta)$.
    }
    \begin{example}
        The simplest example is the case of binomial distributions, where the conjugate prior is the \textit{$\beta$-distribution}. This can be generalized to multi-class situations. The conjugate prior of a categorical (or even \textit{multinomial}) distribution is the \textit{Dirichlet distribution}.
    \end{example}

\section{Confidence intervals}\index{confidence}\label{section:confidence}

    The true value of a parameter $\varepsilon$ can never be known exactly. However, it is possible to construct an interval $I$ in which this value should lie with a certain confidence $C$.
    \begin{example}[Prediction interval]
        Let $X$ be a normally distributed random variable. A measurement will lie in the interval $[\mu - 1.96\sigma,\mu+1.96\sigma]$ with 95\% \underline{probability}. The true value $\mu$ lies in the interval $[x - 2\sigma, x+2\sigma]$ with 95\% \underline{confidence}.
    \end{example}
    \begin{remark*}
        In the previous example some assumptions were made. All possible values (left or right side of peak) are given the same probability due to the Gaussian distribution. If one removes this symmetry condition, a more careful approach is required. Furthermore, the apparent symmetry between the uncertainty and confidence levels is only valid for Gaussian distributions.
    \end{remark*}

\subsection{Interval types}

    \newdef{Two-sided confidence interval}{\label{statistics:two_sided_interval}
        \begin{gather}
            \mathrm{Pr}(x_-\leq X\leq x_+) = \int_{x_-}^{x_+}f(x)\,dx = C
        \end{gather}
        There are three possible (often used) two-sided intervals:
        \begin{itemize}
            \item\textbf{symmetric interval}: $x_+ - \mu = \mu - x_-$,
            \item\textbf{shortest interval}: $|x_+ - x_-|$ is minimal, or
            \item\textbf{central interval}: $\int_{-\infty}^{x_-}f(x)\,dx = \int_{x_+}^\infty f(x)\,dx = \frac{1-C}{2}$.
        \end{itemize}
        The central interval is the most widely used confidence interval.
    }
    \remark{For Gaussian distributions these three definitions are equivalent.}

    \newdef{One-sided confidence interval}{
        \begin{align}
            \label{statistics:one_sided_interval1}
            \mathrm{Pr}(x\geq x_-) &= \int_{x_-}^\infty f(x)\,dx = C\\
            \label{statistics:one_sided_interval2}
            \mathrm{Pr}(x\leq x_+) &= \int_{-\infty}^{x_+}f(x)\,dx = C
        \end{align}
    }

    \newdef{Discrete central confidence interval}{
        For a discrete distribution it is often impossible to find integers $x_{\pm}$ such that the real value lies with exact confidence $C$ in the interval $[x_-,x_+]$.
        \begin{align}
            \label{statistics:central_discreteInterval_lower_bound}
            x_- &= \arg\min_\theta\left[\frac{1-C}{2} - \sum_{x=0}^{\theta - 1}p(x)\right]\\
            \label{statistics:central_discreteInterval_upper_bound}
            x_+ &= \arg\min_\theta\left[\frac{1-C}{2} - \sum_{x=\theta + 1}^\infty p(x)\right]
        \end{align}
    }

\subsection{General construction}

    For every value of the true parameter $X$ it is possible to construct a confidence interval. This leads to the construction of two functions $x_-(X)$ and $x_+(X)$. The 2D diagram obtained by plotting $x_-(X)$ and $x_+(X)$ with the $x$-axis horizontally and $X$-axis vertically is called the \textbf{confidence region}.
    \begin{method}
        Let $x_0$ be a point estimate of the parameter $X$. From the confidence region it is possible to infere a confidence interval $[X_-(x),X_+(x)]$, where the upper limit $X_+$ is not the limit such that there is only a $\frac{1-C}{2}$ chance of having a true parameter $X\geq X_+$, but the limit such that if the true parameter $X\geq X_+$ then there is a chance of $\frac{1-C}{2}$ to have a measurement $x_0$ or smaller.
    \end{method}

\subsection{Interval for a sample mean}

    \newformula{Interval with known variance}{
        If the sample size is large enough, the real distribution is unimportant, because the CLT ensures a Gaussian distribution of the sample mean $\overline{X}$. The $\alpha$-level confidence interval such that $\mathrm{Pr}(-z_{\alpha/2}<Z<z_{\alpha/2})$ with $Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{N}}$ is given by
        \begin{gather}
            \left[\overline{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{N}},\overline{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{N}}\right].
        \end{gather}
    }
    \remark{If the sample size is not sufficiently large, the measured quantity must follow a normal distribution.}

    \newformula{Interval with unknown variance}{
        To account for the uncertainty of the estimated standard deviation $\hat{\sigma}$, the student-$t$ distribution \ref{statistics:student_t_distr} is used instead of a Gaussian distribution to describe the sample mean $\overline{X}$. The $\alpha$-level confidence interval is given by
        \begin{gather}
            \left[\overline{X} - t_{\alpha/2;(n-1)}\frac{s}{\sqrt{N}},\overline{X} + t_{\alpha/2;(n-1)}\frac{s}{\sqrt{N}}\right],
        \end{gather}
        where $s$ is the estimated standard deviation \ref{statistics:variance_bessel_correction}.
    }

    \newformula{Wilson score interval}{\index{Wilson score interval}
        For a sufficiently large sample, a sample proportion $\hat{P}$ is approximately Gaussian distributed with expectation value $\pi$ and variance $\frac{\pi(\pi-1)}{N}$. The $\alpha$-level confidence interval is given by
        \begin{gather}
            \left[\frac{(2N\hat{P} + z^2_{\alpha/2}) - z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})},\frac{(2N\hat{P} + z^2_{\alpha/2}) + z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})}\right].
        \end{gather}
    }
    \sremark{The expectation value and variance are these of a binomial distribution \ref{statistics:binomial_distr} with $r = X/N$.}

\section{Hypothesis testing}\index{hypothesis}

    \newdef{Simple hypothesis}{
        A hypothesis where the distribution is fully specified.
    }
    \newdef{Composite hypothesis}{
        A hypothesis where the distribution is given relative to some parameter values.
    }

\subsection{Testing}

    \newdef{Type I error}{\index{error}
        Rejection of a true null hypothesis.
    }
    \newdef{Type II error}{
        Acceptance of a false null hypothesis.
    }

    \newdef{Significance}{\index{significance}\label{statistics:significance}
        The probability of making a type I error:
        \begin{gather}
            \alpha := \int P_\mathrm{I}(x)\,dx.
        \end{gather}
    }
    \begin{property}
        Let $\alpha_1>\alpha_2$. An $\alpha_2$-level test is also significant at the $\alpha_1$-level.
    \end{property}
    \remark{For discrete distributions it is not always possible to achieve an exact level of significance.}
    \sremark{Type I errors occur occasionally. They cannot be prevented, one can only try to control them.}

    \newdef{Power}{\index{power}
        The probability of not making a type II error:
        \begin{gather}
            \beta := \int P_\mathrm{II}(x)\,dx \quad\longrightarrow\quad \text{power: }1-\beta.
        \end{gather}
    }
    \begin{remark}
        A good test is a test with a small significance and a large power. The probabilities $P_\mathrm{I}$ and $P_\mathrm{II}$ should be as different as possible.
    \end{remark}

    \newdef{Shapiro-Wilk test}{\index{test!Shapiro-Wilk}
        After obtaining a data sample it is often interesting to see if the data is distributed normally, since many other tests and methods assume a normal distribution. The Shapiro-Wilk test considers the following test statistic:
        \begin{gather}
            W := \frac{\sum_{i=1}^n(a_ix_{(i)})^2}{\sum_{i=1}^n(x_i-\overline{x})^2},
        \end{gather}
        where
        \begin{itemize}
            \item $x_{(i)}$ are the order statistics,
            \item $(a_i,\ldots,a_n) := \frac{m^TV}{\|V^{-1}m\|}$,
            \item $m\equiv(m_1,\ldots,m_n)$ are the expectation values of the order statistics of i.i.d. standard normal distributions, and
            \item $V$ is the covariance matrix of $m$.
        \end{itemize}
        The test statistic does not follow a known distribution and all critical values are calculated with Monte-Carlo simulations.
    }

    \begin{definition}[Likelihood ratio test]\index{test!likelihood ratio}\index{likelihood|seealso{test}}\label{statistics:likelihood_ratio}
        The null hypothesis $H_0:\theta=\theta_0$ is rejected in favour of the alternative hypothesis $H_1:\theta=\theta_1$ if the likelihood ratio $\Lambda$ satisfies the following condition:
        \begin{gather}
            \Lambda(x) = \frac{\mathcal{L}(\theta_0\mid x)}{\mathcal{L}(\theta_1\mid x)}\leq\eta,
        \end{gather}
        where $P(\Lambda(x)\leq\eta\mid H_0) = \alpha$.
    \end{definition}
    \sremark{In some references the reciprocal of $\Lambda$ is used as the definition of the likelihood ratio.}
    \begin{theorem}[Neyman-Pearson lemma]\index{Neyman-Pearson}\label{statistics:neyman_pearson}
        The likelihood ratio test is the most powerful test at significance level $\alpha$.
    \end{theorem}

    \newdef{Family-wise error}{\index{error}
        Given a collection of hypothesis tests, the family-wise error is defined as the probability of making at least one type-I error.
    }
    \begin{construct}[Bonferroni correction]\index{Bonferroni correction}
        Consider a set of hypotheses $\{H_i\}_{1\leq i\leq n}$. The higher the number of tests, the higher the chance that by statistical fluctuations at least one of these hypotheses will be rejected. To avoid this problem of multiple comparisons, one can try to control the family-wise error rate, i.e. the probability of falsely rejecting at least one hypothesis. The easiest way to control this error rate is by modifying the individual significance levels:
        \begin{gather}
            \alpha\longrightarrow\frac{\alpha}{n}.
        \end{gather}
    \end{construct}

\subsection{Comparison tests}

    \newdef{McNemar test}{\index{test!McNemar}
        Consider two models or hypotheses describing a given data set. Construct the contingency table describing the number of true positives and true negatives for both models:
        \begin{gather}
            \begin{array}{c||c|c}
                &\text{TP (model 1)}&\text{TN (model 1)}\\
                \hline
                \text{TP (model 2)}&a&b\\
                \hline
                \text{TN (model 2)}&c&d
            \end{array}
        \end{gather}
        The null hypothesis of the McNemar test is that there is no significant difference between the predictive power of the models, i.e. $ p_a+p_c = p_a+p_b$ and $p_b+p_d = p_c+p_d$, where $p_i$ indicates the proportion of class $i$. In fact it is easy to see that the diagonal values are irrelevant for this hypothesis:
        \begin{align*}
            H_0&:b=c\\
            H_1&:b\neq c.
        \end{align*}
        The test statistic is the McNemar chi-squared statistic:
        \begin{gather}
            \chi^2 = \frac{(b-c)^2}{b+c}.
        \end{gather}
        When the values of $b$ and $c$ are large enough ($>25$), one can approximate this distribution by an ordinary $\chi^2$-distribution with 1 degree of freedom.
    }
    \begin{remark}[Edwards correction]\index{Edwards correction}
        It is common to apply a continuity correction (similar to the \textit{Yates-correction} for the ordinary chi-squared test):
        \begin{gather}
           \chi^2 := \frac{(|b-c|-1)^2}{b+c}.
        \end{gather}
        This follows from the fact that for small $b,c$ the exact $p$-values should be compared with a binomial test which compares $b$ to $b+c$ (note the factor of 2):
        \begin{gather}
           p = 2\sum_{i=b}^{b+c}\binom{b+c}{i}0.5^i(1 - 0.5)^{b+c-i}.
        \end{gather}
    \end{remark}

    \newdef{Wilcoxon signed-rank test}{\index{test!Wilcoxon}
        Consider a paired data sample, i.e. two dependent data samples for which the entries are uniquely paired. This test checks if the population means (more generally, the location parameters) are different. The test statistic is defined as follows:
        \begin{quote}
            First, calculate the differences $d_i$ and rank their absolute values (ties are assigned an average rank). Then, calculate the sums of the ranks $R_+,R_-$ for positive and negative differences and take the smallest of these:
            \begin{gather}
                T:=\min(R_+,R_-).
            \end{gather}
            For small data samples ($n<25$) one can look up critical values in the literature. For larger data samples one can (approximately) use a standard normal distribution with statistic \[z := \frac{T-\frac{1}{4}n(n+1)}{\sqrt{\frac{1}{24}n(n+1)(2n+1)}}.\]
        \end{quote}
    }
    \begin{remark}
        The main benefit of this test over a signed $t$-test is that the Wilcoxon test does not require the data samples to be drawn from a normal distribution. However in the case where the assumptions for a paired $t$-test are met, the $t$-test is more powerful.
    \end{remark}
    \begin{remark}[Independent samples]\index{test!Mann-Whitney}
        There exists a similar rank-based test for unpaired data samples. This is the \textbf{Wilcoxon rank-sum test} or \textbf{Mann-Whitney $U$-test}.
    \end{remark}

    \newdef{Friedman test}{\index{test!Friedman}
        Consider $k$ models tested on $N$ data sets. For every data set one ranks the models according to decreasing performance. For every $i\leq k$ one defines the average rank $R_i=\frac{1}{N}\sum_{j\leq N}r^j_i$, where $r^j_i$ is the rank of the $i^{th}$ model on the $j^{th}$ data set. Under the null hypothesis ``all models perform equally well'', the average ranks should be the same for all models.

        The Friedman statistic
        \begin{gather}
            \chi^2_F := \frac{12N}{k(k+1)}\left(\sum_{i\leq k}R_i^2 - \frac{k(k+1)^2}{4}\right)
        \end{gather}
        follows a $\chi^2$-distribution with $k-1$ degrees of freedom when $N>10$ and $k>5$. For smaller values of these parameters one can look up the exact critical values in the literature.
    }
    \begin{remark}
        It was shown that the original Friedman test is rather conservative and that a better statistic is
        \begin{gather}
            F := \frac{(N-1)\chi^2_F}{N(k+1)-\chi^2_F}.
        \end{gather}
        This follows an $F$-distribution with $k-1$ and $(N-1)(k-1)$ degrees of freedom. A further remark is that the (nonparametric) Friedman test is weaker than the (parametric) \textit{repeated-measures ANOVA} whenever the assumptions for the latter hold (similar to the case of the Wilcoxon signed-rank test).
    \end{remark}

\subsection{Post-hoc tests}

    After successfully using one of the multi-model tests from the previous section to reject the null hypothesis of equal performance, one is often interested in exactly which model outperforms the others. For this one can use one of the following pairwise tests:

    \newdef{Nemenyi test}{\index{test!Nemenyi}
        Consider the average ranks $R_i$ from the Friedman test. As a test statistic one uses
        \begin{gather}
            z := \frac{R_i - R_j}{\sqrt{\frac{k(k+1)}{6N}}},
        \end{gather}
        where $k$ is the number of models and $N$ is the number of data sets. The exact critical values can either be found in the literature or one can approximately use a normal distribution.
    }
    \begin{remark}[Bonferroni-Dunn test]\index{test!Bonferonni-Dunn}
        If all one wants to do is see if a particular model performs better than a given baseline model, the Nemenyi test is too conservative since it corrects for $k(k-1)/2$ model comparisons instead of $k-1$. Therefore it is better to use a general method to control the family-wise error for multiple measurements. The Bonferroni-Dunn test modifies the Nemenyi test by performing a Bonferroni correction with $n-1$ degrees of freedom.
    \end{remark}

    A more powerful test is given by the following strategy:
    \newdef{Holm test}{\index{test!Holm}
        Consider the $p$-values of the Nemenyi test. Instead of comparing all values to a single Bonferroni-corrected significance, one can use a so-called ``step-down'' method. First one orders the $p$-values in ascending order and compares the smallest one to $\frac{\alpha}{k-1}$. If this value is significant, i.e. the hypothesis that the associated models perform equally well is rejected, one compares $p_2$ to $\frac{\alpha}{k-2}$ and so on until one finds a hypothesis that cannot be rejected. All remaining hypotheses are retained as well.
    }

    \begin{remark}[Power]
        It is possible that the post-hoc test fails to report a significant difference even though the Friedman test rejected the null hypothesis. This is a consequence of the lower power of post hoc tests.
    \end{remark}

\section{Goodness of fit}\index{goodness of fit}

    \newdef{Akaike information criterion}{\index{Akaike information criterion}
        \nomenclature[A_Ak]{AIC}{Akaike information criterion}
        Consider a model $f(x;\theta)$ with $k$ parameters fitted to a given data sample and let $\mathcal{L}_0$ be the maximum of the associated likelihood function. The Akaike information criterion is defined a follows:
        \begin{gather}
            \text{AIC} := 2k - 2\ln(\mathcal{L}_0).
        \end{gather}
        From this definition it is immediately clear that the AIC rewards goodness-of-fit but penalizes overfitting due to the first term.

        This criterion is often useful when trying to select the best model/parameters to describe a certain data set. However, it should be noted that it is not an absolute measure of quality.
    }

\subsection{\texorpdfstring{$\chi^2$}{Chi squared}-test}\index{test!$\chi^2$}\label{section:chi_squared_test}

    \begin{property}\label{statistics:chi_square}
        If there are $N - n$ fitted parameters one has:
        \begin{gather}
            \int_{\chi^2}^\infty f_{\chi^2}(x\mid n)\,dx \approx 1\implies
            \begin{cases}
                \bullet\text{ good fit}\\
                \bullet\text{ errors were overestimated}\\
                \bullet\text{ selected measurements}\\
                \bullet\text{ lucky shot}
            \end{cases}
        \end{gather}
    \end{property}
    \begin{property}[Reduced $\chi^2$]
        The reduced chi-squared statistic is defined as follows:
        \begin{gather}
            \chi^2_\text{red} := \chi^2/n,
        \end{gather}
        where $n$ is the number of degrees of freedom. Depending on the value of this statistic one can draw the following conclusions (under the right assumptions):
        \begin{itemize}
            \item $\chi^2_\text{red}\gg1$: poor modelling,
            \item $\chi^2_\text{red}>1$: bad modelling or underestimation of the uncertainties,
            \item $\chi^2_\text{red}\approx1$: good fit, or
            \item $\chi^2_\text{red}<1$: (improbable) overestimation of the uncertainties.
        \end{itemize}
    \end{property}

\subsection{Runs test}\index{test!runs}

    A good $\chi^2$-test does not mean that the fit is good. As mentioned in Property \ref{statistics:chi_square}, it is possible that the errors were overestimated. Another condition for a good fit is that the data points vary around the fit, i.e. there are no long sequences of points that lie above/underneath the fit. This condition is tested with a runs test \ref{statistics:runs_distribution}.

    \begin{remark}
        The $\chi^2$-test and runs test are complementary. The $\chi^2$-test only takes the absolute value of the differences between the fit and data points into account, the runs test only takes the signs of the differences into account.
    \end{remark}

    \newformula{Runs distribution}{\index{distribution!runs}\label{statistics:runs_distribution}
        Let $N_+$ and $N_-$ denote the number of points above and below the fit. Under the hypothesis that all points were independently drawn from the same distribution the number of runs is distributed as follows (approximately Gaussian):
        \begin{gather}
            \begin{aligned}
                P(r_\text{even}) &= 2\frac{\binom{N_+ - 1}{\frac{r}{2} - 1}\binom{N_- - 1}{\frac{r}{2} - 1}}{\binom{N}{N_+}}\\\\
                P(r_\text{odd}) &= \frac{\binom{N_+ - 1}{\frac{r - 3}{2}}\binom{N_- - 1}{\frac{r - 1}{2}} + \binom{N_- - 1}{\frac{r - 3}{2}}\binom{N_+ - 1}{\frac{r - 1}{2}}}{\binom{N}{N_+}},
            \end{aligned}
        \end{gather}
        where $C^n_k$ is the binomial coefficient $\binom{n}{k}$. The first two moments of this distribution are given by the following formulas:
        \begin{align}
            \expect{r} &= 1 + 2\frac{N_+ N_-}{N}\\
            \variance{r} &= 2\frac{N_+ N_-}{N}\frac{2N_+ N_- - N}{N(N-1)}.
        \end{align}
    }
    \remark{For $r > 15$, the runs distribution approximates a Gaussian distribution.}
\section{Matrices}

    \begin{notation}\label{linalgebra:matrix_set}
        The vector space of all $m\times n$-matrices defined over the field $K$ is denoted by $M_{m,n}(K)$. If $m=n$, the space is denoted by $M_n(K)$ or $M(n,K)$.
    \end{notation}

    \begin{property}[Dimension]\index{dimension!of matrix}\label{linalgebra:dimension_of_matrix_space}
        The dimension of $M_{m,n}(K)$ is $mn$.
    \end{property}

    \newdef{General linear group}{\index{general linear group}\label{linalgebra:GL_matrices}
        \nomenclature[S_GLn]{$\GL(n,K)$}{general linear group: the group of all invertible $n\times n$-matrices over the field $K$}
        The set of invertible matrices is called the general linear group and is denoted by $\GL_n(K)$ or $\GL(n,K)$.
    }
    \begin{property}
        For all $A\in\GL_n(K)$ one has:
        \begin{itemize}
            \item $A^T\in\GL_n(K)$, and
            \item $\left(A^T\right)^{-1}=\left(A^{-1}\right)^T$.
        \end{itemize}
    \end{property}

    \newdef{Trace}{\index{trace}\label{linalgebra:trace}
        Let $A\equiv(a_{ij})\in M_n(K)$. The trace of $A$ is defined as follows:
        \begin{gather}
            \tr(A) := \sum_{i=1}^na_{ii}.
        \end{gather}
    }
    \begin{property}\label{linalgebra:trace_commutative}
        Let $A,B\in M_n(K)$. The trace satisfies the following properties:
        \begin{itemize}
            \item $\tr:M_n(K)\rightarrow K$ is a linear map,
            \item $\tr(AB) = \tr(BA)$, and
            \item $\tr(A^T) = \tr(A)$.
        \end{itemize}
    \end{property}

    \newformula{Hilbert-Schmidt norm}{\index{Frobenius!norm|see{Hilbert-Schmidt}}\index{Hilbert-Schmidt!norm}\label{linalgebra:hilbert_schmidt_norm}
        The Hilbert-Schmidt (or \textbf{Frobenius}) norm is defined by the following formula:
        \begin{gather}
            \|A\|^2_{HS} := \sum_{i,j}|A_{ij}|^2 = \tr\left(A^\dag A\right).
        \end{gather}
        If one identifies $M_{n}(\mathbb{C})$ with $\mathbb{C}^{2n}$, this norm equals the standard Hermitian norm.
    }

    \newformula{Hadamard product}{\index{Hadamard!product}\label{linalgebra:hadamard_product}
        The Hadamard product of two matrices is defined as the entry-wise product:
        \begin{gather}
            (A\circ B)_{ij} := A_{ij}B_{ij}.
        \end{gather}
    }

    \begin{property}\label{linalgebra:dim_columns_rows}
        Let $A\in M_{m,n}(K)$. Denote the set of columns as $\{A_1,A_2,\ldots,A_n\}$ and the set of rows as $\{R_1,R_2,\ldots,R_m\}$. The set of columns is a subspace of $K^m$ and the set of rows is a subspace of $K^n$. Their spans satisfy the following property:
        \begin{gather}
            \dim(\mathrm{span}(A_1,\ldots,A_n)) = \dim(\mathrm{span}(R_1,\ldots,R_m)).
        \end{gather}
    \end{property}
    \newdef{Rank}{\index{rank!of a matrix}\label{linalgebra:matrix_rank}
        Using the invariance relation from the previous property, one can define the rank of a matrix $A\in M_{m,n}(K)$ as follows:
        \begin{gather}
            \rk(A) := \dim(\mathrm{span}(A_1,\ldots,A_n)) = \dim(\mathrm{span}(R_1,\ldots,R_m)).
        \end{gather}
    }

    \begin{property}\label{linalgebra:rank_properties}
        Let $A\in M_{m,n}(K),B\in\GL_n(K),C\in M_{n,r}(K)$ and $D\in M_{r,n}(K)$. The ranks of these matrices satisfy the following properties:
        \begin{itemize}
            \item $\rk(AC)\leq\rk(A)$,
            \item $\rk(AC)\leq\rk(C)$,
            \item $\rk(BC)=\rk(C)$, and
            \item $\rk(DB)=\rk(D)$.
        \end{itemize}
    \end{property}
    \begin{property}\label{linalgebra:matrix_left_multiplication}
        Let $A\in M_{m,n}(K)$. The linear map
        \begin{gather}
            L_A:K^n\rightarrow K^m:v\mapsto Av
        \end{gather}
        satisfies $\im(L_A) = \mathrm{span}(A_1,\ldots,A_n)$.
    \end{property}

\subsection{System of equations}\label{section:system_of_equations}

    \begin{property}\label{linalgebra:matrix_and_equations}
        Let $Ax=b$ with $A\in M_{m,n}(K),x\in K^n$ and $b\in K^m$ be a system of $m$ equations in $n$ variables and let $L_A$ be the linear map as defined in Property \eqref{linalgebra:matrix_left_multiplication}. The following properties hold:
        \begin{itemize}
            \item The system is inconsistent if and only if $b\not\in\im(L_A)$.
            \item If the system is not inconsistent, the solution set is an affine space. If $x_0\in K^n$ is a solution, the solution set is given by: $x_0+\ker(L_A)$.
            \item If the system is homogeneous, i.e.~$b=0$, the solution set is equal to $\ker(L_A)$.
        \end{itemize}
    \end{property}
    \begin{property}[Uniqueness]\label{linalgebra:rank_unique_solution}
        Let $Ax=b$ with $A\in M_n(K)$ be a system of $n$ equations in $n$ variables. If $\rk(A)=n$, the system has a unique solution.
    \end{property}

    \newformula{Cramer's rule}{\index{Cramer's rule}\label{linalgebra:cramers_rule}
        Let $Ax=b$ be a system of linear equations where the matrix $A$ has a nonzero determinant. There exists a unique solution:
        \begin{gather}
            x_i = \frac{\det(A_i)}{\det(A)},
        \end{gather}
        where $A_i$ is the matrix obtained by replacing the $i^{th}$ column of $A$ by the column vector $b$.
    }

\subsection{Coordinates and matrix representations}

    \newdef{Coordinate vector}{\index{coordinate}\label{linalgebra:coordinate_vector}
        Let $\mathcal{B}=\{b_1,\ldots,b_n\}$ be a basis of $V$ and consider the vector $v=\sum_{i=1}^n\lambda_ib_i$. The coordinate vector of $v$ with respect to $\mathcal{B}$ is defined as the column vector $(\lambda_1,\ldots,\lambda_n)^T$. The scalars $\lambda_i$ are called the \textbf{coordinates} of $v$ with respect to $\mathcal{B}$.
    }
    \newdef{Coordinate isomorphism}{\label{linalgebra:coordinate_isomorphism}
        With the previous definition in mind one can define the coordinate isomorphism induced by $\mathcal{B}$ as follows:
        \begin{gather}
            \beta:V\rightarrow K^n:\sum_{i=1}^n\lambda_ib_i\mapsto(\lambda_1,\ldots,\lambda_n)^T.
        \end{gather}
    }

    \begin{construct}[Matrix representation]\label{linalgebra:matrix_representation}
        Let $V,W$ be $m$- and $n$-dimensional vector spaces with bases $\mathcal{B}=\{b_1,\ldots,b_m\},\mathcal{C}=\{c_1,\ldots,c_n\}$ and consider a linear map $f:V\rightarrow W$. The matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$ is defined as the matrix $A_{f,\mathcal{B},\mathcal{C}}$ that satisfies the following condition for all vectors $v\in V$. Let $(\lambda_1,\ldots,\lambda_n)^T$ be the coordinate vector of $v$ with respect to $\mathcal{B}$ and let $(\mu_1,\ldots,\mu_m)^T$ be the coordinate vector of $f(v)$ with respect to $\mathcal{C}$, then
        \begin{gather}
            \label{linalgebra:matrix_representation_equation}
            \begin{pmatrix}
                \mu_1\\\vdots\\\mu_m
            \end{pmatrix}
            = A_{f,\mathcal{B},\mathcal{C}}
            \begin{pmatrix}
                \lambda_1\\\vdots\\\lambda_n
            \end{pmatrix}.
        \end{gather}
        This matrix can be constructed as follows. For every $j\in\{1,\ldots,m\}$, write $f(b_j) = \sum_{i=1}^na_{ij}c_i$. The matrix $A_{f,\mathcal{B},\mathcal{C}}\equiv(a_{ij})\in M_{n,m}(K)$ is called the matrix representation of $f$. The $j^{th}$ column of $A_{f,\mathcal{B},\mathcal{C}}$ coincides with the coordinate vector of $f(b_j)$ with respect to $\mathcal{C}$.
    \end{construct}

    The following property shows that the matrix algebra $M_{m,n}(K)$ is isomorphic to the algebra\footnote{The multiplication is given by the composition of linear maps.} of linear maps $\mathcal{L}(K^n,K^m)$, thereby explaining why the same notation for the space of invertible matrices \ref{linalgebra:GL_matrices} and the space of automorphisms \ref{linalgebra:automorphism} was used:
    \begin{property}[Matrices and linear maps]\label{linalgebra:map_matrix_relation}
        For every matrix $A\in M_{m,n}(K)$ there exists a linear map $f:K^n\rightarrow K^m$ such that $A_{f,\mathcal{B},\mathcal{C}}=A$. Conversely, for every linear map $f:K^m\rightarrow K^n$ there exists a matrix $A\in M_{n,m}(K)$ such that $f=L_A$ (given by the previous construction).
    \end{property}
    \begin{result}\label{linalgebra:matrix_invertible_map}
        Let $f\in\End(V)$ and let $A_f$ be the corresponding matrix representation. The linear map $f$ is invertible if and only if $A_f$ is invertible. Furthermore, if $A_f$ is invertible, \[\left(A_f\right)^{-1} = A_{f^{-1}}.\] In other words, the linear isomorphism $\End(V)\rightarrow M_n(K)$ descends to a group isomorphism
        \begin{gather}
            \GL_K(V)\rightarrow\GL_n(K):f\mapsto A_f,
        \end{gather}
        where $n=\dim(V)$.
    \end{result}

    \begin{formula}[Linear forms]
        Let $V\cong K^n$ and consider a linear form $f\in V^*$. Equation \eqref{linalgebra:matrix_representation_equation} can be rewritten as
        \begin{gather}
            f\left((\lambda_1,\ldots,\lambda_n)^T\right) = (f(e_1), \ldots, f(e_n))(\lambda_1,\ldots,\lambda_n)^T = \sum_{i=1}^nf(e_i)\lambda_i,
        \end{gather}
        where $\{e_i\}_{i\in I}$ is the standard basis of $K^n$. In terms of the standard dual basis $\{\varepsilon_1,\ldots,\varepsilon_n\}$ this becomes:
        \begin{gather}
            \label{linalgebra:map_in_function_of_dual_basis}
            f = \sum_{i=1}^nf(e_i)\varepsilon_i.
        \end{gather}
    \end{formula}

    \begin{property}[Transpose]
        Let $f:V\rightarrow W$ be a linear map and let $f^*:W^*\rightarrow V^*$ be the corresponding dual map. If $A_f$ is the matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$, the transpose $A_f^T$ is the matrix representation of $f^*$ with respect to the dual basis of $\mathcal{C}$ and the dual basis of $\mathcal{B}$.
    \end{property}
    \begin{result}\index{adjoint!Hermitian}
        The Hermitian adjoint of a linear map \ref{linalgebra:adjoint_operator} induces the (Hermitian) adjoint of matrices $A\in\mathbb{C}^{m\times n}$. It is given by
        \begin{gather}
            A^\dag = \overline{A}^T,
        \end{gather}
        where $\overline{A}$ denotes the complex conjugate of $A$.
    \end{result}

\subsection{Coordinate transformations}

    \newdef{Transition matrix}{\label{linalgebra:transition_matrix}
        Let $\mathcal{B} = \{b_1,\ldots,b_n\}$ and $\mathcal{B}' = \{b_1',\ldots,b_n'\}$ be two bases of $V$. By definition, every element of $\mathcal{B}'$ can be written as a linear combination of elements in $\mathcal{B}$:
        \begin{gather}
            b_j' = q_{1j}b_1 + \cdots + q_{nj}b_n.
        \end{gather}
        The matrix $Q\equiv(q_{ij})\in M_n(K)$ is called the transition matrix from $\mathcal{B}$ to $\mathcal{B}'$.
    }

    \begin{property}\label{linalgebra:transition_matrix_properties}
        Let $\mathcal{B},\mathcal{B}'$ be two bases of $V$ and let $Q$ be the transition matrix from $\mathcal{B}$ to $\mathcal{B}'$. The following statements hold:
        \begin{itemize}
            \item $Q\in\GL_n(K)$ and $Q^{-1}$ is the transition matrix from $\mathcal{B}'$ to $\mathcal{B}$.
            \item Let $\mathcal{C}$ be an arbitrary basis of $V$ with $\gamma$ the corresponding coordinate isomorphism and define the following matrices: \[B:=(\gamma(b_1),\ldots,\gamma(b_n)) \quad\text{and}\quad B':=(\gamma(b_1'),\ldots,\gamma(b_n')).\] In terms of these matrices one finds that $BQ = B'$.
            \item Consider $v\in V$. Let $(\lambda_1,\ldots,\lambda_n)^T$ be the coordinate vector with respect to $\mathcal{B}$ and let $(\lambda_1',\ldots,\lambda_n')^T$ be the coordinate vector with respect to $\mathcal{B}'$, then
                \begin{gather}
                    Q
                    \begin{pmatrix}
                        \lambda_1'\\\vdots\\\lambda_n'
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        \lambda_1\\\vdots\\\lambda_n
                    \end{pmatrix}
                    \quad\text{and}\quad
                    \begin{pmatrix}
                        \lambda_1'\\\vdots\\\lambda_n'
                    \end{pmatrix}
                    = Q^{-1}
                    \begin{pmatrix}
                        \lambda_1\\\vdots\\\lambda_n
                    \end{pmatrix}.
                \end{gather}
        \end{itemize}
    \end{property}
    \begin{result}[Basis change]\label{linalgebra:transition_matrix_representation}
        Let $V,W$ be two finite-dimensional vector spaces. Consider two bases $\mathcal{B},\mathcal{B}'$ of $V$ and two bases $\mathcal{C},\mathcal{C}'$ of $W$. Let $Q,P$ be the transition matrices from $\mathcal{B}$ to $\mathcal{B}'$ and from $\mathcal{C}$ to $\mathcal{C}'$, respectively. The matrix representations $A=A_{f,\mathcal{B},\mathcal{C}}$ and $A' = A_{f,\mathcal{B}',\mathcal{C}'}$ of a linear map $f:V\rightarrow W$ are related in the following way:
        \begin{gather}
            A' = P^{-1}AQ.
        \end{gather}
    \end{result}

    \begin{remark}
        From the definition of the transition matrix and the above property it follows that the basis vectors and coordinate representations transform by $Q$ and $Q^{-1}$ respectively. That they transform in an inverse manner makes sense, since a vector should be independent from its coordinate representation: \[v'=\sum_{i=1}^n\lambda_i'e_i'=\sum_{i,j,k=1}^nQ^{-1}_{ji}\lambda_jQ_{ik}e_k = \sum_{i,j,k=1}^n\delta_{jk}\lambda_je_k = v.\]
    \end{remark}
    This remark gives a new way to define a vector $v\in V$:
    \newadef{Vector}{\index{vector}\label{linalgebra:vector_alternative}
        Consider an $n$-dimensional vector space $V$. One can define an equivalence relation on the set $K^n\times FV$, where $FV$ denotes the set of all bases of $V$, by saying that the pairs $(c,\mathfrak{b})$ and $(c',\mathfrak{b}')$ are equivalent if and only if there exists a matrix $A\in\GL_n(K)$ such that $c'=Ac$ and $\mathfrak{b}=A\mathfrak{b}'$. A vector $v\in V$ is then defined as an equivalence class of such pairs.
    }

    \newdef{Matrix conjugation}{\index{conjugacy class}\index{matrix!conjugation}\label{linalgebra:conjugacy_class}
        Let $A\in M_n(K)$. The set
        \begin{gather}
            \big\{Q^{-1}AQ\,\big\vert\,Q\in\GL_n(K)\big\}
        \end{gather}
        is called the conjugacy class of $A$ in accordance with group theory (Definition \ref{group:normal_subgroup}). Another term for conjugation is \textbf{similarity transformation}.
    }
    \begin{remark}
        If $A$ is a matrix representation of a linear operator $f$, the conjugacy class of $A$ consists of all matrix representations of $f$.
    \end{remark}

    \begin{property}[Trace]\label{linalgebra:trace_invariance}
        Property \ref{linalgebra:trace_commutative} implies that the trace of a matrix is invariant under conjugation:
        \begin{gather}
            \tr(Q^{-1}AQ) = \tr(A).
        \end{gather}
    \end{property}

    \newdef{Matrix congruence}{\index{congruence}\label{linalgebra:matrix_congruence}
        Let $A,B\in M_n(K)$. The matrices are said to be congruent if there exists a matrix $P$ such that
        \begin{gather}
            A = P^TBP.
        \end{gather}
    }
    \begin{property}
        Every matrix congruent to a symmetric matrix is also symmetric.
    \end{property}

    \begin{property}[Orthogonality of basis changes]\label{linalgebra:orthogonal_transition_matrix}
        Let $V$ be an inner product space and let $\mathcal{B},\mathcal{B}'$ be two orthonormal bases of $V$ with transition matrix $Q$. $Q$ is \textit{orthogonal} (Definition \ref{linalgebra:orthogonal_group}):
        \begin{gather}
            Q^TQ = \mathbbm{1}_n.
        \end{gather}
    \end{property}

\subsection{Determinant}

    \newdef{Minor}{\index{minor}
        The $(i,j)$-th minor of $A$ is defined as $\det(A_{ij})$ where $A_{ij}\in M_{n-1}(K)$ is the matrix obtained by removing the $i^{th}$ row and the $j^{th}$ column from $A$.
    }
    \newdef{Cofactor}{\index{co-!factor}
        The cofactor $\alpha_{ij}$ of the matrix element $a_{ij}$ is defined as $(-1)^{i+j}\det(A_{ij})$.
    }
    \newdef{Adjugate matrix}{\index{adjugate matrix}\label{linalgebra:adjugate_matrix}
        The adjugate matrix of $A\in M_n(K)$ is defined as follows:
        \begin{gather}
            \mathrm{adj}(A):=
            \begin{pmatrix}
                \alpha_{11}&\alpha_{21}&\dotsm&\alpha_{n1}\\
                \alpha_{12}&\alpha_{22}&\dotsm&\alpha_{n2}\\
                \vdots&\vdots&\ddots&\vdots\\
                \alpha_{1n}&\alpha_{2n}&\dotsm&\alpha_{nn}\\
            \end{pmatrix},
        \end{gather}
        or in terms of the cofactors: $\mathrm{adj}(A) = (\alpha_{ij})^T$, where the transpose is taken after the elements have been replaced by their cofactor.
    }

    \begin{formula}[Laplace]\index{Laplace!determinant formula}\label{linalgebra:laplace_formula}
        The determinant of a matrix $A\equiv(a_{ij})\in M_n(K)$ can be evaluated as follows:
        \begin{gather}
            \det(A) = \sum_{i=1}^n(-1)^{i+k}a_{ik}\det(A_{ik}).
        \end{gather}
    \end{formula}
    \begin{property}\label{linalgebra:determinant_properties}
        Let $A,B\in M_n(K)$ and denote the columns of $A$ by $A_1,\ldots,A_n$. The determinant has the following properties:
        \begin{itemize}
            \item $\det(AB) = \det(A)\det(B)$,
            \item $\det(A^T) = \det(A)$,
            \item $\det(A_1,\dotso, A_i+\lambda A_i',\dotso,A_n) = \det(A_1,\dotso,A_i,\dotso,A_n) + \lambda\det(A_1,\dotso,A_i',\dotso,A_n)$ for all $A_i,A_i'\in M_{n,1}(K)$, and
            \item $\det(A_{\sigma(1)},\dotso,A_{\sigma(n)}) = \sgn(\sigma)\det(A_1,\dotso,A_n)$
        \end{itemize}
        Items 2, 3 and 4 further imply that a matrix with two identical rows or columns has a vanishing determinant.
    \end{property}

    \begin{property}\label{linalgebra:theorem:rank_det_equivalence}
        Let $A\in M_n(K)$. The following statements are equivalent:
        \begin{itemize}
            \item $\det(A)\neq 0$,
            \item $\rk(A) = n$, or
            \item $A\in\GL_n(K)$.
        \end{itemize}
    \end{property}
    \begin{property}\label{linalgebra:adjugate_matrix_determinant}
        For all $A\in M_n(K)$ one finds that $A\,\mathrm{adj}(A) = \mathrm{adj}(A)A = \det(A)I_n$.
    \end{property}
    \begin{result}\label{linalgebra:determinant_inverse}
        For all $A\in\GL_n(K)$ one finds
        \begin{gather}
            A^{-1} = \det(A)^{-1}\,\mathrm{adj}(A).
        \end{gather}
    \end{result}

    \begin{adefinition}[Minor]\index{minor}
        Let $A\in M_{m,n}(K)$ and choose $k\leq\min(m,n)$. A $k\times k$-minor of $A$ is the determinant of a $k\times k$-partial matrix obtained by removing $m-k$ rows and $n-k$ columns from $A$.
    \end{adefinition}
    \begin{property}
        Let $A\in M_{m,n}(K)$ and choose $k\leq\min(m,n)$. Then $\rk(A)\geq k$ if and only if $A$ contains a nonzero $k\times k$-minor.
    \end{property}

    \begin{property}[Invariance of determinant]
        Let $f\in\End(V)$. The determinant of the matrix representation of $f$ is invariant under basis transformations.
    \end{property}
    \newdef{Determinant of a linear map}{\index{determinant}\label{linalgebra:operator_determinant}
        The previous property allows for an unambiguous definition of the determinant of $f\in\End(V)$:
        \begin{gather}
            \det(f) := \det(A)
        \end{gather}
        for any matrix representation $A$ of $f$.
    }

\subsection{Characteristic polynomial}

    \begin{definition}[Characteristic polynomial]\index{characteristic!polynomial}\index{characteristic!equation}\label{linalgebra:characteristic_polynomial}
        Consider a linear map $f\in\End(V)$ and denote its matrix representation by $A_f$. The function
        \begin{gather}
            \chi_f(x) := \det(x\mathbbm{1}_n-A_f)\in K[x]
        \end{gather}
        is a monic polynomial of degree $n$ in the variable $x$. The following equation is called the \textbf{characteristic equation} or \textbf{secular equation} of $f$:
        \begin{gather}
            \label{linalgebra:characteristic_equation}
            \chi_f(x) = 0.
        \end{gather}
    \end{definition}

    \begin{formula}\label{linalgebra:parts_of_characteristic_polynomial}
        Consider a matrix $A\equiv(a_{ij})\in M_n(K)$ with characteristic polynomial \[\chi_A(x) = x^n + c_{n-1}x^{n-1} + \dotso + c_1x + c_0.\] The first and last of the coefficients $c_i$ have a simple expression:
        \begin{gather}
            \begin{cases}
                c_0 = (-1)^n\det(A),\\
                c_{n-1} = -\tr(A)
            \end{cases}
        \end{gather}
    \end{formula}

    \begin{theorem}[Cayley-Hamilton]\index{Cayley-Hamilton}\label{linalgebra:cayley_hamilton}
        Consider a linear map $f\in\End(V)$ with characteristic polynomial $\chi_f$.
        \begin{gather}
            \chi_f(f) = f^n + \sum_{i=1}^{n-1}c_if^i=0.
        \end{gather}
    \end{theorem}
    \begin{result}
        From Property \ref{linalgebra:minimal_polynomial_divisor} and the Cayley-Hamilton theorem it follows that the minimal polynomial $\mu_f$ is a divisor of the characteristic polynomial $\chi_f$.
    \end{result}

\subsection{Matrix groups}\label{section:linear_groups}

    \newdef{Elementary matrix}{\index{elementary matrix}\label{linalgebra:elementary_matrix}
        An elementary matrix is a matrix of the form
        \[
            \begin{pmatrix}
                1&0&\dotsm&0\\
                0&1&a&0\\
                \vdots&\vdots&\ddots&\vdots\\
                0&0&\cdots&1
            \end{pmatrix}
            ,
            \begin{pmatrix}
                1&0&\dotsm&0\\
                0&1&\dotsm&0\\
                \vdots&b&\ddots&\vdots\\
                0&0&\cdots&1
            \end{pmatrix}
            ,\dotso
        \]
        i.e.~it is equal to the sum of an identity matrix and a multiple of a matrix unit $U_{ij}$. The elementary matrix with the scalar $c$ at position $(i,j)$ is denoted by $E_{ij}(c)$.

        A second type of elementary matrix is one of the form
        \[
            \begin{pmatrix}
                1&0&0&\dotsm&0\\
                0&0&0&a&0\\
                0&0&1&\cdots&0\\
                \vdots&a&\vdots&\ddots&\vdots\\
                0&0&0&\cdots&1
            \end{pmatrix}.
        \]
        These matrices are sometimes denoted by $T_{i,j}$.
    }
    \begin{property}[Invertibility]
        Elementary matrices have determinant 1 and, accordingly, are elements of $\GL_n(K)$.
    \end{property}
    \begin{property}
        Multiplication by an elementary matrix has the following properties:
        \begin{itemize}
            \item Left multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $i^{th}$ row of the matrix with the $i^{th}$ row plus $c$ times the $j^{th}$ row.
            \item Right multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $j^{th}$ column of the matrix with the $j^{th}$ column plus $c$ times the $i^{th}$ column.
            \item Left multiplication by an elementary matrix $T_{i,j}$ interchanges the $i^{th}$ and $j^{th}$ rows.
        \end{itemize}
    \end{property}

    \begin{property}\label{linalgebra:elementary_matrices}
        Every invertible matrix can be written as a product of elementary matrices.
    \end{property}

    \newdef{Special linear group}{\label{linalgebra:special_linear_group}
        \nomenclature[S_SLn]{$\mathrm{SL}_n(K)$}{special linear group: group of all invertible $n$-dimensional matrices with unit determinant over the field $K$}
        The subgroup of $\GL_n(K)$ consisting of all matrices with determinant 1:
        \begin{gather}
            \mathrm{SL}_n(K) := \{A\in\GL_n(K)\mid\det(A) = 1\}.
        \end{gather}
    }

    \newdef{Orthogonal group}{\index{orthogonal!group}\label{linalgebra:orthogonal_group}
        \nomenclature[S_Ortho]{$\mathrm{O}(n,K)$}{group of $n\times n$ orthogonal matrices over a field $K$}
        The orthogonal and special orthogonal group are defined as follows:
        \begin{align}
            \mathrm{O}(n,K) &:= \{A\in\GL_n(K)\mid AA^T = A^TA = \mathbbm{1}_n\}\nonumber\\
            \mathrm{SO}(n,K) &:= \mathrm{O}_n(K)\cap\mathrm{SL}_n(K).\nonumber
        \end{align}
    }

    \newdef{Unitary group}{\label{linalgebra:unitary_group}
        \nomenclature[S_Unit]{$\mathrm{U}(n,K)$}{group of $n\times n$ unitary matrices over a field $K$}
        Consider a field $K$ equipped with an involution $\sigma:\lambda\mapsto\overline{\lambda}$. The unitary and special unitary group are defined as follows:
        \begin{align*}
            \mathrm{U}_n(K,\sigma) &:= \{A\in\GL_n(K)\mid A\sigma(A)^T = \sigma(A)^TA = \mathbbm{1}_n\}\\
            \mathrm{SU}_n(K,\sigma) &:= \mathrm{U}_n(K)\cap\mathrm{SL}_n(K).
        \end{align*}
        In practice $K$ is often $\mathbb{C}$ with complex conjugation as the involution. For this reason the notation $A^\dag:=\sigma(A)^T$ is common. Moreover, in the case $K=\mathbb{C}$ the notation is further simplified to $\mathrm{U}(n)$ and $\mathrm{SU}(n)$.
    }

    \newdef{Unitary equivalence}{
        Let $A,B\in M_n(K)$ over a field $K$ with an involution. The matrices are said to be unitarily equivalent if there exists a unitary matrix $U$ such that \[A = U^\dag BU.\]
    }
    \begin{property}\label{linalgebra:con_equivalence}
        For orthogonal matrices, conjugacy \ref{linalgebra:conjugacy_class} and congruency \ref{linalgebra:matrix_congruence} coincide. More generally, for unitary matrices conjugacy and unitary equivalence coincide.
    \end{property}

    \newdef{Symplectic group}{\index{symplectic!group}\label{linalgebra:symplectic_group}
        \nomenclature[S_Symp]{$\mathrm{Sp}(n,K)$}{group of matrices preserving a canonical symplectic form over the field $K$}
        Consider a vector space $V$ with an antisymmetric nonsingular matrix $\Omega$. The symplectic group $\mathrm{Sp}(V,\Omega)$ is defined as follows:
        \begin{gather}
            \mathrm{Sp}(V,\Omega) := \{A\in\GL(V)\mid A^T\Omega A = \Omega\}.
        \end{gather}
        Over the real or complex numbers one can define the canonical \textbf{symplectic} matrix
        \begin{gather}
            \Omega_{st} :=
            \begin{pmatrix}
                0&-\mathbbm{1}\\
                \mathbbm{1}&0
            \end{pmatrix}.
        \end{gather}
        The groups of matrices that preserve this matrix are denoted by $\mathrm{Sp}(n,\mathbb{R})$ and $\mathrm{Sp}(n,\mathbb{C})$.
    }
    \begin{remark}
        Symplectic groups can only be defined on even-dimensional spaces because antisymmetric matrices can only be nonsingular if the dimension $n$ is even.
    \end{remark}
    \newdef{Compact symplectic group}{
        \nomenclature[S_SympC]{$\mathrm{Sp}(n)$}{compact symplectic group}
        The compact symplectic group is defined as follows (although the notation is confusing, it is standard):
        \begin{gather}
            \mathrm{Sp}(n) := \mathrm{Sp}(2n,\mathbb{C})\cap\mathrm{U}(2n).
        \end{gather}
        This group is in fact isomorphic to the \textit{quaternionic} unitary group in $n$ quaternionic dimensions.
    }
    \begin{property}
        \[\mathrm{Sp}(1)\cong\mathrm{SU}(2)\]
    \end{property}

\subsection{Matrix decompositions}

    \begin{method}[QR Decomposition]\index{QR!decompositon}
        Every square complex matrix $M$ can be decomposed as
        \begin{gather}
            M = QR
        \end{gather}
        with $Q$ unitary and $R$ upper-triangular. The easiest way to achieve this decomposition is by applying the Gram-Schmidt orthonormalization process:
        \begin{quote}
            Let $\{v_i\}_{i\leq n}$ be a basis for the column space of $M$. By applying the Gram-Schmidt process to this basis one obtains a new orthonormal basis $\{e_i\}_{i\leq n}$. The matrix $M$ can then be written as a product $QR$ of the following matrices:
            \begin{itemize}
                \item an upper-triangular matrix $R$ with entries $R_{ij} = \langle e_i|\mathrm{col}_j(M) \rangle$, where $\mathrm{col}_j(M)$ denotes the $j^{th}$ column of $M$.
                \item a unitary matrix $Q = (e_1,\ldots,e_n)$ constructed by setting the $i^{th}$ column equal to the $i^{th}$ basis vector $e_i$.
            \end{itemize}
        \end{quote}
    \end{method}
    \begin{property}
        If $M$ is invertible and if the diagonal elements of $R$ are required to have positive norm, the QR-decomposition is unique.
    \end{property}

    ?? COMPLETE (Cholesky, polar, ...) ??
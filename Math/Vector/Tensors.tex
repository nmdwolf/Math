\chapter{Vector \& Tensor Calculus}

    References for this chapter are \cite{jeevanjee, AMP1}. For a more geometric approach to some of the concepts and results in this chapter, see the content of Chapters \ref{chapter:curves_surfaces} and \ref{chapter:vector_bundles} and Section \ref{section:integration_manifolds}.

    \begin{remark*}\index{vector field}
        In this chapter a \textit{vector field} will mean a vector-valued function with smooth projections.
    \end{remark*}

\section{Nabla-operator}\label{section:nabla}

    \sremark{The geometric approach to this section is summarized in Remark \ref{bundle:vector_calculus}.}

    \newdef{Gradient}{\index{gradient}\label{vector:gradient}
        Let $\varphi:\mathbb{R}^3\rightarrow\mathbb{R}$ be a smooth function.
        \begin{gather}
            \nabla\varphi := \left(\pderiv{\varphi}{x},\pderiv{\varphi}{y},\pderiv{\varphi}{z}\right)
        \end{gather}
        This definition can easily be generalized to arbitrary dimensions.
    }
    \begin{property}
        The gradient of a smooth real-valued function is perpendicular to its level sets \ref{set:level_set}.
    \end{property}

    \begin{formula}[Differential]
        Let $\varphi:\mathbb{R}^3\rightarrow\mathbb{R}$ be a smooth function. The total differential $d\varphi$ can be rewritten as follows:
        \begin{gather}
            d\varphi = \nabla\varphi\cdot d\vector{r}.
        \end{gather}
    \end{formula}

    \newdef{Directional derivative}{\index{directional derivative}\label{vector:directional_derivative}
        Consider a smooth function $\varphi:\mathbb{R}^3\rightarrow\mathbb{R}$ and let $\hat{a}$ be a unit vector. The directional derivative $\nabla_{\hat{a}}\varphi$ is defined as the change of the function $\varphi$ in the direction of $\hat{a}$:
        \begin{gather}
            \nabla_{\hat{a}}\varphi := (\hat{a}\cdot\nabla)\varphi.
        \end{gather}
    }
    \begin{example}
        Let $\varphi:\mathbb{R}^3\rightarrow\mathbb{R}$ be a smooth real-valued function and let $\deriv{\vector{r}}{s}$ denote the tangent vector to a curve $\vector{r}(s)$ with natural parameter\footnote{See Definition \ref{diff:natural_parameter} for a formal definition.} $s$. The variation of $\varphi$ along $\vector{r}(s)$ is given by
        \begin{gather}
            \pderiv{\varphi}{s} = \deriv{\vector{r}}{s}\cdot\nabla\varphi.
        \end{gather}
    \end{example}

    \newdef{Conservative vector field}{\index{conservative!vector field}
        A vector field that can be expressed as the gradient of a scalar function.
    }

    \newdef{Gradient of a tensor}{\index{gradient}
        Let $T$ be a tensor field on $\mathbb{R}^3$ and let $\vector{e}_i$ be the coordinate basis. The gradient of $T$ is defined as follows:
        \begin{gather}
            \nabla T := \sum_{i=1}^3\pderiv{T}{x^i}\otimes\vector{e}_i.
        \end{gather}
    }

    \newdef{Divergence}{\index{divergence}\label{vector:divergence}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \nabla\cdot\vector{A} := \pderiv{A_x}{x} + \pderiv{A_y}{y} + \pderiv{A_z}{z}
        \end{gather}
    }
    \newdef{Solenoidal vector field}{\index{solenoidal}
        A vector field $\vector{A}$ that satisfies
        \begin{gather}
            \nabla\cdot\vector{A} = 0.
        \end{gather}
        Such a vector field is also said to be \textbf{divergence-free} due to Equation \eqref{vector:divergence_of_rotor} below.
    }

    \newdef{Rotor / curl}{\index{curl}\index{rotor|see{curl}}\label{vector:rotor}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \nabla\times\vector{A} := \left(\pderiv{A_z}{y} - \pderiv{A_y}{z}, \pderiv{A_x}{z} - \pderiv{A_z}{x}, \pderiv{A_y}{x} - \pderiv{A_x}{y}\right)
        \end{gather}
    }

    \newdef{Irrotational vector field}{\index{irrotational}
        A vector field $\vector{A}$ that satisfies
        \begin{gather}
            \nabla\times\vector{A} = 0.
        \end{gather}
    }

    \newdef{Laplacian}{\index{Laplace!operator}
        Let $\varphi$ and $\vector{A}$ be respectively a smooth function and smooth vector field on $\mathbb{R}^3$.
        \begin{align}
            \label{vector:laplacian}
            \Delta\varphi:=\nabla^2\varphi &= \mpderiv{2}{\varphi}{x} + \mpderiv{2}{\varphi}{y} + \mpderiv{2}{\varphi}{z}\\
            \label{vector:vector_laplacian}
            \Delta\vector{A}:=\nabla^2\vector{A} &= \nabla\left(\nabla\cdot\vector{A}\right) - \nabla\times \left(\nabla\times\vector{A}\right)\\
            &= \left(\Delta A_x,\Delta A_y,\Delta A_z\right)
        \end{align}
        The latter is sometimes called the \textbf{vector Laplacian}.
    }

    \begin{property}[Mixed properties]\label{vector:mixed_properties}
        The differential operators introduced above satisfy the following identities:
        \begin{align}
            \label{vector:rotor_of_gradient}
            \nabla\times\left(\nabla\varphi\right) &= 0\\
            \label{vector:divergence_of_rotor}
            \nabla\cdot\left(\nabla\times\vector{A}\right) &= 0.
        \end{align}
    \end{property}
    \begin{result}
        All conservative vector fields are irrotational. However, the converse is only true if the domain is simply-connected \ref{topology:simply_connected}. (All of this is formalized in the Poincar\'e lemma \ref{bundle:poincare}.)
    \end{result}

    \newformula{Helmholtz decomposition}{\index{Helmholtz!decomposition}\label{vector:helmholtz_decomposition}
        If $\vector{A}$ is a vector field that decays faster than $1/r$ when $r\longrightarrow\infty$, it can be written as
        \begin{gather}
            \vector{A} = \nabla\times\vector{B} + \nabla\varphi.
        \end{gather}
        for some smooth vector field $\vector{B}$ and smooth function $\varphi$
    }

    The differential operators introduced above can also be generalized to curvilinear coordinates. To do this one needs the scale factors as formally defined in Definition \ref{diff:scale_factor}. For the remainder of this section the Einstein summation convention will not be used to make everything as explicit as possible.
    \newformula{Unit vectors}{
        \begin{gather}
            \pderiv{\vector{r}}{q^i} = h_i\hat{e}_i
        \end{gather}
    }
    \newformula{Gradient}{
        \begin{gather}
            \nabla\varphi = \sum_{i=1}^3\frac{1}{h_i}\pderiv{\varphi}{q^i}\hat{e}_i
        \end{gather}
    }
    \newformula{Divergence}{
        \begin{gather}
            \nabla\cdot\vector{A} = \frac{1}{h_1h_2h_3}\left(\pderiv{}{q^1}(A_1h_2h_3) + \pderiv{}{q^2}(A_2h_3h_1) + \pderiv{}{q^3}(A_3h_1h_2)\right)
        \end{gather}
    }
    \newformula{Rotor}{
        \begin{gather}
            \left(\nabla\times\vector{A}\right)_i = \sum_{j,k=1}^3\frac{\varepsilon_{ijk}}{h_jh_k}\left(\pderiv{}{q^j}(A_kh_k) - \pderiv{}{q^k}(A_jh_j)\right),
        \end{gather}
        where $\varepsilon_{ijk}$ is the 3-dimensional Levi-Civita symbol \ref{vector:levi_civita_symbol}.
    }

    \newformula{Laplacian in different coordinate systems}{\label{vector:laplace_operator}
        In general the Laplace operator is defined as
        \begin{gather}
            \Delta f := \nabla\cdot\nabla f.
        \end{gather}
        The Laplacian can also be expressed in different coordinate systems:
        \begin{itemize}
            \item Cylindrical coordinates $(\rho,\phi,z)$:
            \begin{gather}
                \label{vector:cylindrical}
                \frac{1}{\rho}\pderiv{}{\rho}\left(\rho\pderiv{}{\rho}\right) + \frac{1}{\rho^2}\mpderiv{2}{}{\phi} + \mpderiv{2}{}{z}.
            \end{gather}
            \item Spherical coordinates $(r,\phi,\theta)$:
            \begin{gather}
                \label{vector:spherical}
                \frac{1}{r^2}\left[\pderiv{}{r}\left(r^2\pderiv{}{r}\right) + \frac{1}{\sin^2\theta}\mpderiv{2}{}{\phi} + \frac{1}{\sin\theta}\pderiv{}{\theta}\left(\sin\theta\pderiv{}{\theta}\right)\right].
            \end{gather}
        \end{itemize}
    }

\section{Integration}
\subsection{Line integrals}\index{line!integral}\index{path!integral|see{line integral}}

    \newformula{Line integral of a continuous function}{\label{vector:line_integral_scalar}
        Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ be a continuous function and let $\Gamma$ be a piecewise smooth curve $\vector{\varphi}:[a,b]\rightarrow\mathbb{R}^3$. The line integral of $f$ along $\Gamma$ is defined as follows:
        \begin{gather}
            \int_\Gamma f\,ds := \int_a^bf(\vector{\varphi}(t))\,\|\vector{\varphi}'(t)\|dt.
        \end{gather}
    }
    \newformula{Line integral of a continuous vector field}{\label{vector:line_integral_vector}
        Let $\vector{F}$ be a continuous vector field on $\mathbb{R}^3$ and let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}:[a,b]\rightarrow\mathbb{R}^3$. The line integral of $\vector{F}$ along $\Gamma$ is defined as follows:
        \begin{gather}
            \int_\Gamma\vector{F}\cdot d\vector{s} := \int_a^b\vector{F}(\vector{\varphi}(t))\cdot\vector{\varphi}'(t)dt.
        \end{gather}
    }

    \begin{property}[Conservative vector fields]
        A vector field is conservative if and only if its line integral is path-independent, i.e. if it only depends on the values at the end points. (This is a corollary of Stokes's theorem \ref{bundle:stokes_theorem}.)
    \end{property}

\subsection[Integral theorems]{Integral theorems\footnotemark}
    \footnotetext{These theorems follow from a more general theorem by Stokes \ref{bundle:stokes_theorem}.}

    \begin{theorem}[Fundamental theorem of calculus for line integrals]\index{fundamental theorem!for line integrals}\label{vector:fundamental_theorem}
        Let $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^3$ be a piecewise smooth curve defined on the interval $[a,b]$.
        \begin{gather}
            \int_\Gamma\nabla f\cdot d\vector{r} = \varphi(\Gamma(b)) - \varphi(\Gamma(a))
        \end{gather}
    \end{theorem}

    \begin{theorem}[Kelvin-Stokes theorem]\index{Kelvin-Stokes}\label{vector:kelvin_stokes_theorem}
        Let $\vector{A}$ be a vector field defined on $\mathbb{R}^3$ and let $S$ be a smooth surface with boundary $\partial S$.
        \begin{gather}
            \oint_{\partial S}\vector{A}\cdot d\vector{l} = \iint_S \left(\nabla \times \vector{A}\right)dS
        \end{gather}
    \end{theorem}

    \begin{theorem}[Divergence theorem\footnotemark]\index{divergence!theorem}\label{vector:divergence_theorem}
        \footnotetext{Also known as \textbf{Gauss's theorem} or the \textbf{Gauss-Ostrogradsky theorem}.}
        Let $\vector{A}$ be a vector field defined on $\mathbb{R}^3$.
        \begin{gather}
            \oiint_{\partial V}\vector{A}\cdot d\vector{S} = \iiint_V \left(\nabla \cdot \vector{A}\right)dV
        \end{gather}
    \end{theorem}
    \begin{result}[Green's identity]\index{Green!identity}\label{vector:green_indentity}
        Let $\phi,\psi$ be smooth real-valued functions defined on $\mathbb{R}^3$.
        \begin{gather}
            \oiint_{\partial V}\left(\psi\nabla\phi - \phi\nabla\psi\right)\cdot d\vector{S} = \iiint_V \left(\psi\nabla^2\phi - \phi\nabla^2\psi\right)dV
        \end{gather}
    \end{result}

\section{Tensors}\label{section:tensors}
\subsection{Tensor product}\index{tensor product!of vector spaces}
    \nomenclature[O_zsymbinten]{$V\otimes W$}{tensor product of the vector spaces $V$ and $W$}

    There are two possible (equivalent) ways to introduce the concept of a ``tensor'' on finite-dimensional vector spaces. One is to interpret tensors as multilinear maps, while the other is to work in a local fashion and express work with the expansion coefficients with respect to a chosen basis.

    \begin{definition}[Tensor product space]\index{outer!product}\label{vector:tensor_product}
        The tensor product of two finite-dimensional vector spaces $V$ and $W$ is defined as\footnote{``\textit{isomorphic to}'' would be better terminology. See the universal property \ref{vector:universal_property} below.} the set of bilinear maps on the Cartesian product $V^*\times W^*$. Let $v,w$ be vectors in respectively $V$ and $W$ and let $g,h$ be vectors in the corresponding dual spaces. The tensor product of $v$ and $w$ is then defined as follows:
        \begin{gather}
            (v\otimes w)(g,h) := v(g)w(h).
        \end{gather}
        In this incarnation the tensor product is sometimes known as the \textbf{outer product}. Outer products are also frequently called \textbf{pure} or \textbf{simple tensors}.
    \end{definition}
    \newdef{Tensor component}{
        Let $\mathbf{T}$ be a tensor that takes $r$ vectors and $s$ covectors as input and returns a scalar (element of the underlying field). The components of $\mathbf{T}$ with respect to a frame $\{e_i\}_{i\leq n}$ and a coframe $\{e^i\}_{i\leq n}$ are defined as $T_{i\ldots j}^{\ \ \ \ k\ldots  l} := \mathbf{T}(e_i,\ldots,e_j,e^k,\ldots,e^l)$.
    }

    The above definition can be restated as a universal property (this is also the right way to generalize tensors to infinite-dimensional spaces and avoid the awkward definition involving dual spaces):
    \begin{uproperty}\index{universal!property}\label{vector:universal_property}
        Let $Z$ be a vector space. For every bilinear map $T:V\times W\rightarrow Z$ there exists a unique linear map $f:V\otimes W\rightarrow Z$ such that $T = f\circ\varphi$, where $\varphi:V\times W\rightarrow V\otimes W$ is the bilinear map $(v,w)\mapsto v\otimes w$.
    \end{uproperty}
    \begin{result}
        The tensor product is unique up to linear isomorphisms. This results in the commutativity of the tensor product:
        \begin{gather}
           \label{vector:commutativity}
            V\otimes W \cong W\otimes V.
        \end{gather}
    \end{result}

    \begin{notation}[Tensor power]\index{tensor!power}\index{tensor!type}\label{vector:type}
        \begin{gather}
            V^{\otimes n} := \underbrace{V\otimes\cdots\otimes V}_{n\text{ copies}}
        \end{gather}
        More generally, the tensor product of $r$ copies of $V$ and $s$ copies of $V^*$ is denoted by
        \begin{gather}
            \mathcal{T}^r_s(V) = V^{\otimes r}\otimes V^{*\otimes s}.
        \end{gather}
        Tensors in this space are said to be of \textbf{type} $(r,s)$.
    \end{notation}
    \newdef{Scalar}{\index{scalar}
        The scalars, i.e. the elements of the underlying field are by definition the $(0,0)$-tensors.
    }
    \newdef{Tensor algebra}{\index{tensor!algebra}\label{vector:tensor_algebra}
        The tensor algebra over a vector space $V$ is defined as follows:
        \begin{gather}
            T(V) := \bigoplus_{k\geq0}V^{\otimes k}.
        \end{gather}
    }

    The following remark is strongly related to Property \ref{linalgebra:dual_space_dimension}:
    \begin{remark}
        For finite-dimensional vector spaces the space $\mathcal{T}^1_1V$ is isomorphic to $\End(V)$ and the space $\mathcal{T}^1_0V$ is isomorphic to $V$ itself.

        However, when including infinite-dimensional spaces, the space $\mathcal{T}^1_1V$ is only isomorphic to the endomorphism space $\End(V^*)$ of the dual. This isomorphism is given by the map $\hat{T}:V^*\rightarrow V^*:\omega\mapsto\mathbf{T}(-,\omega)$ for every $\mathbf{T}\in\mathcal{T}^1_1V$. Moreover, in this general setting, the spaces $\mathcal{T}^0_1V$ and $V^*$ are also isomorphic.
    \end{remark}

    The tensor product space can also be defined as follows:
    \begin{adefinition}[Tensor product]
        Consider two vector spaces $V,W$ over a field $K$. First, construct the free vector space $F(V\times W)$ over $K$. Then, construct the subspace $N$ of $F(V\times W)$ spanned by elements of the form
        \begin{itemize}
            \item $(v+v',w) - (v,w) - (v',w)$,
            \item $(v,w+w') - (v,w) - (v,w')$,
            \item $(\lambda v,w) - \lambda(v,w)$, or
            \item $(v,\mu w) - \mu(v,w)$,
        \end{itemize}
        where $v\in V,w\in W$ and $\lambda,\mu\in K$. The tensor product $V\otimes W$ is defined as the quotient $F(V\times W)/N$. It can be shown that this construction is associative, i.e. $U\otimes(V\otimes W)\cong(U\otimes V)\otimes W$, and as such these brackets will be omitted in all expressions.

        Now, consider the case where $W=V^*$. In this case the basis of the tensor product $\mathcal{T}^r_s(V)$ will be denoted by
        \[\underbrace{e_i\otimes\cdots\otimes e_j}_{r\text{ basis vector}}\ \ \otimes\underbrace{\varepsilon^k\otimes\cdots\otimes \varepsilon^l_{\textcolor{white}{a}}}_{s\text{ dual basis vectors}}\]
        and the expansion coefficients will be denoted by $T_{i\ldots j}^{\ \ \ \ k\ldots l}$.
    \end{adefinition}

    \newprop{Dimension}{
        From the previous construction it follows that the dimension of $\mathcal{T}^r_s(V)$ is equal to $rs$.
    }

    For completeness the proof that the values of the tensor operating on $r$ basis vectors and $s$ basis covectors are equal to the corresponding expansion coefficients is given:
    \begin{proof}
        Consider a general tensor $\mathbf{T} = T_{i\ldots j}^{\ \ \ \ k\ldots l} e_k\otimes\cdots\otimes e_l\otimes\varepsilon^i\otimes\cdots\otimes\varepsilon^j$. Combining Definition \ref{vector:tensor_product} and the pairing of dual vectors \eqref{linalgebra:dual_basis_2} gives
        \begin{align*}
            \mathbf{T}(\varepsilon^m, \ldots, \varepsilon^n, e_a, \ldots, e_b) &= T_{i\ldots j}^{\ \ \ \ k\ldots l}e_k(\varepsilon^m)\ldots e_l(\varepsilon^n)\varepsilon^i(e_a)\ldots\varepsilon^j(e_b)\\
            &= T_{i\ldots j}^{\ \ \ \ k\ldots l}\delta_k^m\ldots\delta_l^n\delta_a^i\ldots\delta_b^j\\
            &= T_{a\ldots b}^{\ \ \ \ m\ldots n}.
        \end{align*}
        \qed
    \end{proof}

\subsection{Transformation rules}

    In this section the behaviour of tensors under basis transformations of the form $e_i' = A^i_{\ j}\,e_j$ is considered.

    \begin{definition}[Contravariant]\index{contra-!variant}\label{vector:contravariant}
        A tensor component that transforms by the following rule is said to be contravariant:
        \begin{gather}
            v^i = A^i_{\ j}\,v'^j.
        \end{gather}
    \end{definition}
    \begin{definition}[Covariant]\index{co-!variant}\label{vector:covariant}
        A tensor component that transforms by the following rule is said to be covariant:
        \begin{gather}
            p'_i = A^j_{\ i}\,p_j.
        \end{gather}
    \end{definition}
    \begin{example}[Mixed tensor]
        This example gives the transformation rule of a mixed third-order tensor $T\in\mathcal{T}^1_2$:
        \begin{gather}
            T_{\ ij}^k = A^k_{\ w}(A^{-1})^u_{\ i}(A^{-1})^v_{\ j}T_{\ \ uv}'^w.
        \end{gather}
    \end{example}

    \begin{method}[Quotient rule]
        Assume that an equation such as $Q_i^{\ j}A_{jl}^{\ \ k}=B_{il}^{\ \ k}$ is given, with $A$ and $B$ two known tensors. The quotient rule asserts the following: ``\textit{If the equation holds under all transformations, then $Q$ is a tensor of the indicated type.}'' Note that this rule does not necessarily hold when $B=0$ because transformation rules are not well-defined for the zero tensor.
    \end{method}
    \sremark{This rule is a useful substitute for the ``illegal'' division of tensors.}

\subsection{Tensor operations}

    \newdef{Contraction}{\index{contraction}\label{vector:contraction}
        Let $A$ be a tensor of type $(m,n)$. Taking a subscript and superscript to be equal and summing over all possible values of this index gives a new tensor of type $(m-1,n-1)$. This operation is called the contraction of $A$. It is induced by the evaluation map/pairing \ref{linalgebra:natural_pairing}.
    }

    \newdef{Direct product}{\index{direct product}
        Let $A$ and $B$ be two tensors. The tensor constructed by the componentwise multiplication of $A$ and $B$ is called the direct product of $A$ and $B$. This is a generalization of the Hadamard product \ref{linalgebra:hadamard_product}.
    }
    \begin{example}
        Let $A_{\ k}^i$ and $B_{\ lm}^j$ represent two tensors. Their direct product is equal to \[C_{\ k\ lm}^{i\ j} = A_{\ k}^iB_{\ lm}^j.\]
    \end{example}

    \newformula{Operator product}{\label{vector:operator_product}
        It is also possible to combine operators acting on different vector spaces to make them act on the tensor product space:
        \begin{gather}
            (A\otimes B)(v\otimes w) := Av\otimes Bw.
        \end{gather}
    }
    \begin{notation}[Abuse of notation]\label{vector:tensor_abuse}
        Consider an operator $A$ acting on a vector space $V_1$. When working with a tensor product space $V_1\otimes V_2$, the operator $A$ can be extended to the product as $A\otimes\mathbbm{1}$. However, it is often still denoted by $A$.
    \end{notation}

    \begin{notation}[Symmetric part]\index{symmetric!part}
        Consider a second-order tensor $T$ (here taken to be of covariant type for notational simplicity). The symmetric and antisymmetric part of $T$ are sometimes denoted by
        \begin{gather}
            T_{(ij)} = \frac{1}{2}\left(T_{ij} + T_{ji}\right)
        \end{gather}
        and
        \begin{gather}
            T_{[ij]} = \frac{1}{2}\left(T_{ij} - T_{ji}\right).
        \end{gather}
        This notation is easily generalized to other types of tensors.
    \end{notation}

    \begin{property}[Gradient of tensor products]\index{gradient}
        The gradient of an outer product is defined through the Leibniz rule:
        \begin{gather}
            \nabla\cdot(v\otimes w) := (\nabla\cdot v)w+(v\cdot\nabla)w.
        \end{gather}
    \end{property}

    \newdef{Complexification}{\index{complexification}\label{vector:complexification}
        Let $V$ be a real vector space. The complexification of $V$ is defined as the following tensor product:
        \begin{gather}
            V^{\mathbb{C}} := V\otimes\mathbb{C}.
        \end{gather}
        This space can still be considered a real vector space, but it can also be turned into a complex vector space by generalizing the scalar product as follows for all $\alpha\in\mathbb{C}$:
        \begin{gather}
            \alpha(v\otimes\beta) := v\otimes(\alpha\beta).
        \end{gather}
    }
    \begin{property}\label{vector:complexification_decomposition}
        By (multi)linearity every element $v_{\mathbb{C}}\in V^{\mathbb{C}}$ can be written as \[v_{\mathbb{C}} = (v_1\otimes1) + i(v_2\otimes 1).\] Therefore, the complexification can be (formally) decomposed as
        \begin{gather}
            V^{\mathbb{C}} \cong V\oplus iV.
        \end{gather}
    \end{property}

\section{Exterior algebra}
\subsection{Antisymmetric tensors}

    \newdef{Antisymmetric tensor}{\index{antisymmetry}
        A tensor that changes sign under the interchange of any two indices.
    }
    \begin{notation}[Symmetric tensors]
        \nomenclature[S_SnV]{$S^n(V)$}{space of symmetric rank $n$ tensors over a vector space $V$}
        The space of symmetric $(n,0)$-tensors is denoted by $S^n(V)$. The space of symmetric $(0,n)$-tensors is denoted by $S^n(V^*)$.
    \end{notation}
    \begin{notation}[Antisymmetric tensors]\label{vector:antysimmetric_space}
        \nomenclature[S_zsymLambda]{$\Lambda^n(V)$}{space of antisymmetric rank $n$ tensors over a vector space $V$}
        The space of antisymmetric $(n,0)$-tensors is denoted by $\Lambda^n(V)$. The space of antisymmetric $(0,n)$-tensors is denoted by $\Lambda^n(V^*)$.
    \end{notation}

    \begin{property}
        Let $n = \dim(V)$. The space $\Lambda^r(V)$ equals the zero space for all $r\geq n$.
    \end{property}

\subsection{Determinant}

    \newdef{Form}{\index{form}
        An $n$-form is a totally antisymmetric element of $\mathcal{T}^0_nV$.
    }
    \newdef{Volume form}{\index{volume!form}
        A form of rank $\dim(V)$ is also called a \textbf{top form} or \textbf{volume form}.
    }

    \newdef{Determinant}{\index{determinant}
        Consider a finite-dimensional vector space $V$ with basis $\{e_i\}_{i\leq n}$. Let $\varphi$ be a tensor in $\mathcal{T}^1_1V\cong\End(V)$ and let $\omega$ be a volume form on $V$. The determinant of $\varphi$ is defined as follows:
        \begin{gather}
            \det(\varphi) := \frac{\omega(\varphi(e_1),\ldots,\varphi(e_n))}{\omega(e_1,\ldots,e_n)}.
        \end{gather}
        This definition is well-defined, i.e. it is independent of the choice of volume form and basis. Furthermore, it coincides with Definition \ref{linalgebra:operator_determinant}.

        One should note that the determinant is only well-defined for $(1,1)$-tensors. Although other types of tensors can also be represented as matrices, for these the above formula would not be independent of a choice of basis anymore. A more general concept can be defined using the language principal bundles (see Section \ref{chapter:principal_bundles}).
    }

\subsection{Levi-Civita symbol}

    \newdef{Levi-Civita symbol}{\index{Levi-Civita!symbol}\label{vector:levi_civita_symbol}
        In $n$ dimensions the Levi-Civita symbol is defined as follows:
        \begin{gather}
            \varepsilon_{i_1\ldots i_n} =
            \begin{cases}
                1&\text{if }(i_1\ldots i_n)\text{ is an even permutation of }(1\ldots n)\\
                -1&\text{if }(i_1\ldots i_n)\text{ is an odd permutation of }(1\ldots n)\\
                0&\text{if any of the indices occurs more than once.}
            \end{cases}
        \end{gather}
    }
    \begin{remark}[Pseudotensor]\label{vector:levi_civita_pseudotensor}
        The Levi-Civita symbol is not a tensor, it is a \textit{pseudotensor}. This means that the sign changes under reflections or any transformation with determinant $-1$. (To turn it into a proper tensor, one should multiply it by a factor $\sqrt{g}$, where $g$ is the determinant of the metric.)
    \end{remark}

    \newformula{Cross product}{\index{cross!product}\label{vector:cross_product}
        Using the Levi-Civita symbol, one can define the $i^{th}$ component of the cross product as follows:
        \begin{gather}
            (v\times w)_i = \varepsilon_{ijk}v_jw_k.
        \end{gather}
        The previous remark implies that the cross product is in fact not a vector, instead it is a ``pseudovector''.
    }
    \begin{remark}
        It is important to note that this construction is only valid in 3 dimensions.
    \end{remark}

\subsection{Wedge product}\label{section:wedge_product}

    \begin{definition}[Antisymmetrization]\label{vector:antisymmetrization}
        Let $S_k=$ denote the permutation group \ref{group:permutation_group} on $k$ elements. The antisymmetrization operator is defined as follows:
        \begin{gather}
            \Alt(e_1\otimes\cdots\otimes e_k) := \sum_{\sigma\in S_k} \sgn(\sigma)e_{\sigma(1)}\otimes\cdots\otimes e_{\sigma(k)}.
        \end{gather}
        Note that many authors introduce a factor $1/k!$. This convention is not adopted here to keep the subsequent constructions clean. If the factor is included, Formula \ref{vector:general_wedge_product} below should be modified.
    \end{definition}

    \newdef{Wedge product}{\index{wedge!product}\label{vector:wedge_product}
        Let $\{e_i\}_{i\leq \dim(V)}$ be a basis for $V$. The wedge product of basisvectors is defined as follows:
        \begin{gather}
            e_1 \wedge\ldots\wedge e_k = \Alt(e_1\otimes\cdots\otimes e_k)
        \end{gather}
        From this definition it immediately follows that the wedge product is (totally) antisymmetric.
    }

    \begin{construct}
        Let $\{e_i\}_{i\leq \dim(V)}$ be a basis for $V$. The above definition implies that a basis for $\Lambda^r(V)$ is given by
        \[\{e_{i_1}\wedge\ldots\wedge e_{i_r}\mid\forall k: 1\leq i_k \leq \dim(V)\}.\] Accordingly, the dimension of this space is given by
        \begin{gather}
            \label{vector:wedge_dimension}
            \dim\Lambda^r(V) = \binom{n}{r}.
        \end{gather}
        For $r=0$ this construction would be vacuous, so one just defines $\Lambda^0(V) := \mathbb{R}$.
    \end{construct}

    \begin{formula}\label{vector:general_wedge_product}
        Let $v\in\Lambda^r(V)$ and $w\in\Lambda^m(V)$. The wedge product \ref{vector:wedge_product} can be generalized as follows:
        \begin{gather}
            v\wedge w = \frac{1}{r!m!}\Alt(v\otimes w).
        \end{gather}
    \end{formula}

    \begin{definition}[Blades]\index{blade}\index{pure!vector}
        Elements of $\Lambda^k(V)$ that can be written as the wedge product of $k$ vectors are known as $k$-\textbf{blades} or \textbf{pure $k$-vectors}.
    \end{definition}

    \begin{formula}\index{cross!product}
        In dimension 3 there exists an important isomorphism $J:\Lambda^2(\mathbb{R}^3)\rightarrow\mathbb{R}^3$:
        \begin{gather}
            \label{vector:wedge_to_cross}
            J(\lambda)^i = \frac{1}{2}\varepsilon^i_{\ jk}\lambda^{jk},
        \end{gather}
        where $\lambda\in\Lambda^2(\mathbb{R}^3)$. (See also the Hodge $\ast$-operator \ref{vector:hodge_star} further below.)

        Looking at the definition of the cross product \ref{vector:cross_product}, one can see that $v\times w$ is actually the same as $J(v\wedge w)$. One can thus use the wedge product to generalize the cross product to arbitrary dimensions.
    \end{formula}

\subsection{Exterior algebra}

    \newdef{Exterior power}{\index{exterior!power}\index{form}
        In the theory of tensor calculus, the space $\Lambda^k(V)$ is often called the $k^{th}$ exterior power of $V$. As mentioned before, its elements are called (exterior) \textbf{$k$-forms}.
    }
    \newdef{Exterior algebra}{\index{exterior!algebra}\index{Grassmann!algebra}\label{vector:exterior_algebra}
        One can define a graded vector space \ref{hda:graded_vector_space} as follows:
        \begin{gather}
            \Lambda^\bullet(V) := \bigoplus_{k\geq0}\Lambda^k(V).
        \end{gather}
        This graded vector space can be turned into a graded algebra by taking the wedge product as the multiplication:
        \begin{gather}
            \wedge:\Lambda^k(V)\times\Lambda^l(V)\rightarrow \Lambda^{k+l}(V).
        \end{gather}
        This algebra is called the exterior algebra or \textbf{Grassmann algebra} over $V$. Elements of the space $\otimes_{k\mathrm{ even}}\Lambda^k(V)$ are said to be \textbf{Grassmann-even} and elements of $\otimes_{k\mathrm{ odd}}\Lambda^k(V)$ are said to be \textbf{Grassmann-odd}.
    }

    \newadef{$\dag$}{\label{vector:adef_exterior_algebra}
        Let $T(V)$ be the tensor algebra \ref{vector:tensor_algebra} over the vector space $V$, i.e.
        \begin{gather}
            T(V) = \bigoplus_{k\geq0}V^{\otimes k}.
        \end{gather}
        The exterior algebra over $V$ is defined as the quotient of $T(V)$ by the two-sided ideal $I$ generated by the elements $\{v\otimes v\mid v\in V\}$.
    }

    \begin{property}[Graded-commutativity]
        The exterior algebra is both a unital associative algebra (with identity $1\in K$) and a coalgebra. Furthermore, it is also commutative in the graded sense \ref{hda:graded_commutative}.
    \end{property}

    \begin{property}[Nilpotency]
        Graded-commutativity implies that the wedge product of any odd exterior form with itself is identically 0. The wedge product of an even exterior form with itself vanishes if and only if the form can be decomposed as a product of one-forms, i.e. if it is a pure $k$-form.
    \end{property}

\subsection{Hodge star}

    Equation \eqref{vector:wedge_dimension} says that the spaces $\Lambda^k(V)$ and $\Lambda^{n-k}(V)$ have the same dimension and , hence, that there exists a linear isomorphism between them. Such an isomorphism is given by the Hodge star operator if one restricts to vector spaces equipped with a nondegenerate Hermitian form \ref{linalgebra:NDH_form}.

    When equipped with an inner product and, hence, an orthonormal basis $\{e_i\}_{i\leq\dim(V)}$, every finite-dimensional vector space admits a canonical volume form given by
    \begin{gather}
        \vol = e_1\wedge\ldots\wedge e_n.
    \end{gather}
    This convention will also be adopted in the remainder of this section.

    \newdef{Orientation}{\index{orientation}\label{vector:orientation}
        Let $\vol(V)$ be the standard volume form on the vector space $V$ as defined above. From the definition of a volume form it follows that every other $\dim(V)$-form is a scalar multiple of $\vol(V)$. Denote this number by $r$. This also implies that a choice of volume form induces an equivalence relation on top-dimensional forms. An equivalence class under this relation is called an orientation on $V$. If $r>0$, the orientation is said to be \textbf{positive} and, if $r<0$, the orientation is said to be \textbf{negative}.
    }

    \begin{formula}[Inner product]\index{inner!product}
        Let $V$ be equipped with an inner product $\langle\cdot|\cdot\rangle$. One can extend this to an inner product on $\Lambda^k(V)$ by first defining it on decomposable forms and extending it by linearity to all of $\Lambda^k(V)$:
        \begin{gather}
            \label{vector:wedge_inner_product}
            \langle v_1\wedge\ldots\wedge v_k|w_1\wedge\ldots\wedge w_k \rangle_k := \det(\langle v_i|w_j\rangle).
        \end{gather}
        For an orthogonal basis this formula factorizes as follows:
        \begin{gather}
            \langle v_1\wedge\ldots\wedge v_k|w_1\wedge\ldots\wedge w_k \rangle_k = \langle v_1|w_1 \rangle\cdots\langle v_k|w_k \rangle.
        \end{gather}
    \end{formula}
    \newdef{Hodge star}{\index{Hodge!star}\label{vector:hodge_star}
        The Hodge star $\ast:\Lambda^k(V)\rightarrow\Lambda^{n-k}(V)$ is defined as the unique isomorphism such that for all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ the following equality holds:
        \begin{gather}
            \omega\wedge\rho = \langle\ast\omega|\rho\rangle_{n-k}\vol(V),
        \end{gather}
        where $\langle\cdot|\cdot\rangle_{n-k}$ is the inner product \eqref{vector:wedge_inner_product} on $\Lambda^{n-k}(V)$. The element $\ast\omega$ is often called the \textbf{(Hodge) dual} of $\omega$.
        \begin{proof}
            Fix an element $\omega\in\Lambda^k(V)$. For every element $\rho\in\Lambda^{n-k}(V)$ one can see that $\omega\wedge\rho$ is an element of $\Lambda^n(V)$ and as such it is a scalar multiple of $\vol(V)$. This implies that it can be written as \[c_\omega(\rho)\vol(V).\] The map $c_\omega:\Lambda^{n-k}(V)\rightarrow\mathbb{R}:\rho\mapsto c_\omega(\rho)$ is a bounded (and thus continuous) linear map, so Riesz's representation theorem \ref{functional:riesz} can be applied to identify $c_\omega$ with a unique element $\ast\omega\in\Lambda^{n-k}(V)$ such that \[c_\omega(\rho) = \langle\ast\omega|\rho\rangle_{n-k}.\]
        \qed
        \end{proof}
    }

    \begin{formula}
        Let $\{e_i\}_{i\leq n}$ be a positively oriented orthonormal basis for $V$. An explicit formula for the Hodge star is given by the following construction:
        \begin{quote}
            Let $\{i_1,\ldots,i_k\}$ and $\{j_1,\ldots,j_{n-k}\}$ be two ordered, complementary index sets and consider an element $\omega = e_{i_1}\wedge\ldots\wedge e_{i_k}\in\Lambda^k(V)$.
            \begin{gather}
                \label{vector:explicit_hodge_star}
                \ast\omega =\sgn(\tau)\prod_{m = 1}^{n-k}\langle e_{j_m}|e_{j_m} \rangle e_{j_1}\wedge\ldots\wedge e_{j_{n-k}},
            \end{gather}
            where $\tau$ is the permutation that maps $e_{i_1}\wedge\ldots\wedge e_{i_k}\wedge e_{j_1}\wedge\ldots\wedge e_{j_{n-k}}$ to $\vol(V)$.
        \end{quote}
    \end{formula}
    Using this formula one can easily prove the following important property:
    \begin{property}
        Consider an inner product space $V$. The double dual is involutive up to a factor:
        \begin{gather}
            \ast\ast\omega = (-1)^{k(n-k)}\omega.
        \end{gather}
    \end{property}

    Taking the defining relation of the Hodge star operator together with the above property implies the following formula (which is often found in the literature as the defining relation):
    \begin{formula}
        For all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ the Hodge star operator satisfies the following formula:
        \begin{gather}
            \omega\wedge\ast\rho = \langle\omega|\rho\rangle\vol(V).
        \end{gather}
    \end{formula}

    \begin{result}\label{vector:hodge_star_vectorcalculus}
        Consider three vectors $u,v,w\in\mathbb{R}^3$.
        \begin{align}
            \ast(v\wedge w) &= v\times w \label{vector:cross_by_hodge_star}\\
            \ast(v\times w) &= v\wedge w\\
            \ast(u\wedge v\wedge w) &= u\cdot(v\times w)
        \end{align}
    \end{result}
    \begin{remark}
        Formula \eqref{vector:wedge_to_cross} is an explicit evaluation of the first equation \eqref{vector:cross_by_hodge_star}.
        \begin{proof}
            The signs $\sgn(\sigma)$ in the definition of wedge products can be written using the Levi-Civita symbol $\varepsilon_{ijk}$ as defined in \ref{vector:levi_civita_symbol}. The factor $\frac{1}{2}$ is introduced to correct for the double counting due to the contraction over both the indices $j$ and $k$.
        \end{proof}
    \end{remark}

    \newdef{Self-dual form}{\index{dual!self-dual}
        Let $V$ be a 4-dimensional inner product space and consider $\omega\in\Lambda^2(V)$. Then $\omega$ is said to be self-dual if $\ast\omega = \omega$. Furthermore, every $\rho\in\Lambda^2(V)$ can be uniquely decomposed as the sum of a self-dual and an anti-self-dual two-form.
    }
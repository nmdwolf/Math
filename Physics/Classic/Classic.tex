\chapter{Classical Mechanics}\label{chapter:lagrange}

    The section about the geometric framework is mainly based on \cite{arnold, palais_solitons}. For an introduction to differential geometry, see Chapter \ref{chapter:manifolds} and onwards.

\section{Newtonian mechanics}
\subsection{Linear motion}

    \begin{axiom}[Newton's second law]\index{force}\label{classic:force}
        The force acting on a system can be related to the change of momentum in the following way:
        \begin{gather}
            \vector{F}=\deriv{\vector{p}}{t}.
        \end{gather}
    \end{axiom}

    \newdef{Work}{\index{work}\label{classic:work}
        \begin{gather}
            W := \int\vector{F}\cdot d\vector{l}
        \end{gather}
    }
    \newdef{Conservative force}{\index{conservative!force}
        If the work done by a force is independent of the path taken, the force is said to be \textbf{conservative}:
        \begin{gather}
            \label{classic:conservative_force_2}
            \oint_C\vector{F}\cdot d\vector{l}=0.
        \end{gather}
        The Kelvin-Stokes theorem \ref{vector:kelvin_stokes_theorem} together with relation \eqref{vector:rotor_of_gradient} allows to rewrite the conservative force as the gradient of a scalar field:
        \begin{gather}
            \label{classic:conservative_force}
            \vector{F} = -\nabla V.
        \end{gather}
    }

    \newdef{Central force}{
        A force that only depends on the relative position of two objects:
        \begin{gather}
            \vector{F}_c\equiv F\big(\|\vector{r}_2 - \vector{r}_1\|\big)\boldsymbol{\hat{e}_r}.
        \end{gather}
    }


    \begin{formula}[Momentum of point mass]\index{momentum}
        Consider a mass $m$ with velocity $\vector{v}$. Its momentum is given by
        \begin{gather}
            \vector{p} = m\vector{v}.
        \end{gather}
        If the mass is constant along its trajectory, Newton's second law \ref{classic:force} can be rewritten as follows:
        \begin{gather}
            \vector{F} = m\deriv{\vector{v}}{t} = m\vector{a}.
        \end{gather}
    \end{formula}
    \begin{formula}[Kinetic energy]\index{energy}\label{classic:kinetic_energy}
        For a free particle with total momentum $p$, the kinetic energy is given by the following formula:
        \begin{gather}
            E_\mathrm{kin} := \frac{p^2}{2m}.
        \end{gather}
    \end{formula}

\subsection{Rotational motion}

    In this section $r$ always denotes the distance from the object's center of mass to the axis around which the object rotates.

    \newdef{Angular velocity}{\label{classic:angular_velocity}
        \begin{gather}
            \vector{\omega} := \frac{\vector{r}\times\vector{v}}{r^2}
        \end{gather}
    }
    \newdef{Angular frequency}{\label{classic:frequency}
        \begin{gather}
            \nu := \frac{\|\vector{\omega}\|}{2\pi}
        \end{gather}
    }

    \newdef{Moment of inertia}{\index{inertia}\label{classic:moment_of_inertia}
        For a (spherically) symmetric object the moment of inertia is given by
        \begin{gather}
            I := \int_V r^2\rho(r)dV,
        \end{gather}
        where $\rho$ denotes the mass density function. For a general body the moment of inertia tensor is given by
        \begin{gather}
            \label{classic:inertia_tensor}
            \mathcal{I} := \int_V\rho(\vector{r})\left(r^2\mathbbm{1} - \vector{r}\otimes\vector{r}\right)dV.
        \end{gather}
    }
    \begin{example}[Objects with azimuthal symmetry]
        Let $m,r$ denote the mass and the radius of the object, respectively.
        \begin{itemize}
            \item Solid disk: $I = \frac{1}{2}mr^2$
            \item Cylindrical shell: $I = mr^2$
            \item Hollow sphere: $I = \frac{2}{3}mr^2$
            \item Solid sphere: $I = \frac{2}{5}mr^2$
        \end{itemize}$ $ % bad cheat to get a line break !!
        \begin{proof}[Proof for the solid disk and sphere]
            \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
                The volume of a (solid) disk is given by
                \begin{gather}
                    V_\mathrm{disk} = \pi R^2d,
                \end{gather}
                where $R$ denotes the radius and $d$ denotes the thickness. The mass density is then given by
                \begin{gather}
                    \rho = \frac{M}{\pi R^2d}.
                \end{gather}
                Using cylindrical coordinates the moment of inertia becomes
                \begin{align}
                    I &= \frac{M}{\pi R^2d}\int_0^{2\pi}d\varphi\int_0^ddz\int_0^Rr^3dr\nonumber\\
                    &= \frac{M}{\pi R^2d}2\pi d\frac{R^4}{4}\nonumber\\
                    &= \frac{1}{2}MR^2.
                \end{align}

                The volume of a solid sphere is given by
                \begin{gather}
                    V_\text{sphere} = \frac{4}{3}\pi R^3,
                \end{gather}
                where $R$ denotes the radius. The mass density is then given by
                \begin{gather}
                    \rho = \frac{M}{\frac{4}{3}\pi R^3}.
                \end{gather}
                Spherical coordinates will be used to derive the moment of inertia, but one has to be careful. The $r$ in Formula \ref{classic:moment_of_inertia} is the distance between a point in the body and the axis of rotation. So it is not the same as the $r$ in spherical coordinates, which is the distance between a point and the origin. However, the relation between these two quantities is easily found using basic geometry:
                \begin{gather}
                    r = r'\sin\theta,
                \end{gather}
                where $r'$ is the spherical coordinate. Now, one can calculate the moment of inertia as follows:
                \begin{align}
                    I &= \frac{M}{\frac{4}{3}\pi R^3} \int_0^{2\pi}d\varphi\int_0^Rr'^{^4}dr'\int_0^\pi\sin^3\theta d\theta\nonumber\\
                    &= \frac{M}{\frac{4}{3}\pi R^3} 2\pi\frac{R^5}{5}\frac{4}{3}\nonumber\\
                    &= \frac{2}{5}MR^2.
                \end{align}\qed
            \end{mdframed}
        \end{proof}
    \end{example}

    \newdef{Principal axes of inertia}{\index{principal!axis}
        Let $I$ be the matrix of inertia, i.e.~the matrix associated with the inertia tensor \eqref{classic:inertia_tensor}. This is a real symmetric matrix and, by Property \ref{linalgebra:diagonalizable_hermitian}, admits an eigendecomposition of the form
        \begin{gather}
            I = Q\Lambda Q^T.
        \end{gather}
        The columns of $Q$ determine the principal axes of inertia. The eigenvalues are called the \textbf{principal moments of inertia}.
    }

    \begin{theorem}[Parallel axis theorem\footnotemark]\index{Steiner}\label{classic:theorem:parallel_axis_theorem}
        \footnotetext{Also called \textbf{Steiner's theorem}.}
        Consider a rotation about an axis $\psi$ through a point $A$ and let $\psi_\mathrm{CM}$ be a parallel axis through the center of mass. The moment of inertia about $\psi$ is related to the moment of inertia about $\psi_\mathrm{CM}$ in the following way:
        \begin{gather}
            I_A = I_\mathrm{CM} + m\|\vector{r}_A - \vector{r}_\mathrm{CM}\|^2,
        \end{gather}
        where $m$ is the mass of the rotating body.
    \end{theorem}

    \newdef{Angular momentum}{\label{classic:angular_momentum}
        \begin{gather}
            \vector{L} := \vector{r}\times\vector{p}
        \end{gather}
        Given the angular velocity vector $\vector{\omega}$ one can compute the angular momentum as follows:
        \begin{gather}
            \label{classic:angular_momentum_general}
            \vector{L} = \mathcal{I}(\vector{\omega}),
        \end{gather}
        where $\mathcal{I}$ is the inertia tensor. If $\vector{\omega}$ is parallel to a principal axis, the formula reduces to
        \begin{gather}
            \vector{L} = I\vector{\omega},
        \end{gather}
        where $I$ is the corresponding principal moment of inertia.
    }

    \begin{formula}[Torque]\index{torque}\label{classic:torque}
        For angular momenta there exists a formula analogous to Newton's second law:
        \begin{gather}
            \vector{\tau} :=\deriv{\vector{L}}{t}.
        \end{gather}
        For a constant mass this formula can be rewritten as follows:
        \begin{gather}
            \vector{\tau} = I\vector{\alpha} = \vector{r}\times\vector{F}.
        \end{gather}
    \end{formula}

    \begin{remark}
        From the previous definitions it follows that both the angular momentum and torque vectors are in fact pseudovectors and, accordingly, change sign under coordinate transformations with $\det = -1$.
    \end{remark}

    \newformula{Rotational energy}{\index{energy}\label{classic:rotational_energy}
        \begin{gather}
            E_\mathrm{rot} := \frac{1}{2}\mathcal{I}(\vector{\omega})\cdot\vector{\omega}
        \end{gather}
    }

\section{Lagrangian mechanics}
\subsection{Action}

    \begin{definition}[Generalized coordinates]\index{generalized!coordinate}
        The generalized coordinates $q_k$ are mutually independent coordinates that completely characterize the configuration of a system (relative to a reference configuration).

        When a system is characterized by $N$ parameters and $n_c$ constraints, there are $(N-n_c)$ generalized coordinates. Furthermore, every set of generalized coordinates describing the same system contains exactly $(N-n_c)$ coordinates. (In Chapter \ref{chapter:constrained_dynamics} it is explained how the relevant degrees of freedom can be extracted.)
    \end{definition}
    \begin{definition}[Generalized velocities]
        The generalized velocities $\dot{q}_k$ are the derivatives of the generalized coordinates with respect to time.
    \end{definition}
    \begin{definition}[Conjugate momentum]\index{momentum!conjugate}\label{lagrange:conjugate_momentum}
        \begin{gather}
            p_k := \pderiv{L}{\dot{q}^k}
        \end{gather}
    \end{definition}

    \begin{notation}\label{lagrange:notational_convention_1}
        Given a Lagrangian function, depending on $n$ generalized coordinates and their associated velocities, the following shorthand notation is often used:
        \begin{gather}
            L\left(q,\dot{q},t\right)\equiv L\left(q_1(t),\ldots,q_n(t),\dot{q}_1(t),\ldots,\dot{q}_n(t),t\right)
        \end{gather}
    \end{notation}
    \begin{definition}[Action]\index{action}\label{lagrange:action}
        Given a Lagrangian function $L$, the action is a functional on the space of paths in configuration space defined by integrating $L$:
        \begin{gather}
            S[q] := \int_{t_1}^{t_2}L\left(q,\dot{q},t\right)dt.
        \end{gather}
    \end{definition}

\subsection{Euler-Lagrange equations}\index{Lagrange!equations of motion}\index{Euler-Lagrange|see{Lagrange}}

    \begin{axiom}[d'Alembert's principle]\index{d'Alembert!principle}\index{virtual!displacement}\label{lagrange:dalembert_principle}
        \begin{gather}
            \sum_i(\vector{F}_i - \dot{\vector{p}}_i)\cdot\delta\vector{q}_i = 0,
        \end{gather}
        where the $\delta q^i$ denote the \textbf{virtual displacement} vectors, i.e.~the infinitesimal variations consistent with the contraints. In the spirit of calculus of variations (Section \ref{section:variational_calculus}) these are defined as follows. Consider a trajectory $\vector{q}:[0,1]\rightarrow\mathbb{R}^n$ and a variation $\vector{\gamma}$ of $\vector{q}$, i.e.~a smooth function $\vector{\gamma}:[0,1]\times[-\varepsilon,\varepsilon]\rightarrow\mathbb{R}^n$ such that $\vector{\gamma}(t,0)=\vector{q}(t)$. This function encodes how $\vector{q}$ can vary given the constraints of the system. The virtual displacement vector is then defined as the tangent vector
        \begin{gather}
            \delta\vector{q} := \left.\deriv{\vector{\gamma}}{\varepsilon}\right|_{\varepsilon=0}.
        \end{gather}
    \end{axiom}

    \begin{formula}[Euler-Lagrange equation of the first kind]\index{generalized!force}
        \begin{gather}
            \label{lagrange:first_kind}
            \deriv{}{t}\left(\pderiv{T}{\dot{q}^k}\right) - \pderiv{T}{q^k} = Q_k,
        \end{gather}
        where $T$ is the total kinetic energy and $Q_k$ are the \textbf{generalized forces}:
        \begin{gather}
            Q_k := \sum_i\vector{F}_i\cdot\pderiv{\vector{r}_i}{q^k}.
        \end{gather}
        Note that the constraint forces do not contribute to this quantity because they are always perpendicular to the motion.
    \end{formula}
    \begin{formula}[Euler-Lagrange equation of the second kind]
        \begin{gather}
            \label{lagrange:second_kind}
            \deriv{}{t}\left(\pderiv{L}{\dot{q}^k}\right) - \pderiv{L}{q^k} = 0
        \end{gather}
        \begin{proof}[Proof based on d'Alembert's principle]
            \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
                In the following derivation the mass is assumed to be constant.
                \begin{align}
                    &\sum_k\left(\vector{F}_k - \dot{\vector{p}}_k\right)\cdot\dot{\vector{r}}_k = 0\nonumber\\
                    \iff&\sum_k\left(\vector{F}_k - \dot{\vector{p}}_k\right)\cdot\left(\sum_l\pderiv{\vector{r}_k}{q_l}\dot{q_l}\right) = 0\nonumber\\
                    \iff&\sum_l\left(\sum_k\vector{F}_k\cdot\pderiv{\vector{r}_k}{q_l} - \sum_km\ddot{\vector{r}}_k\cdot\pderiv{\vector{r}_k}{q_l}\right)\dot{q_l} = 0\nonumber\\
                    \iff&\sum_l\left(Q_l - \sum_km\ddot{\vector{r}}_k\cdot\pderiv{\vector{r}_k}{q_l}\right)\dot{q_l} = 0.\label{lagrange_deriv:deriv1}
                \end{align}
                Now, consider at the following derivative:
                \begin{align}
                    &\deriv{}{t}\left(\dot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l}\right) = \ddot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l} + \dot{\vector{r}}\cdot\deriv{}{t}\left(\pderiv{\vector{r}}{q_l}\right)\nonumber\\
                    \iff&\ddot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l} = \deriv{}{t}\left(\dot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l}\right) - \dot{\vector{r}}\cdot\deriv{}{t}\left(\pderiv{\vector{r}}{q_l}\right)\nonumber\\
                    \iff&\ddot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l} = \deriv{}{t}\left(\dot{\vector{r}}\cdot{\color{red}\underbrace{\textcolor{black}{\pderiv{\vector{r}}{q_l}}}_A}\right) - \dot{\vector{r}}\cdot\left(\pderiv{\dot{\vector{r}}}{q_l}\right).\label{lagrange_deriv:deriv2}
                \end{align}
                To evaluate the factor indicated by \textcolor{red}{A}, one can consider another derivative:
                \begin{align}
                    \pderiv{\dot{\vector{r}}}{\dot{q}_l} &= \pderiv{}{\dot{q}_l}\left(\sum_k\pderiv{r}{q_k}\dot{q}_k\right)\nonumber\\
                    &=\sum_k\pderiv{r}{q_k}\delta_{kl}\nonumber\\
                    &=\pderiv{\vector{r}}{q_l}\nonumber\\
                    &=\textcolor{red}{A}.\nonumber
                \end{align}
                Substituting this in Equation \eqref{lagrange_deriv:deriv2} gives
                \begin{align}
                    \ddot{\vector{r}}\cdot\pderiv{\vector{r}}{q_l} &= \deriv{}{t}\left(\dot{\vector{r}}\cdot\pderiv{\dot{\vector{r}}}{\dot{q}_l}\right) - \dot{\vector{r}}\cdot\left(\pderiv{\dot{\vector{r}}}{q_l}\right)\nonumber\\
                    &=\deriv{}{t}\left(\frac{1}{2}\pderiv{\dot{\vector{r}}^2}{\dot{q}_l}\right) - \frac{1}{2}\pderiv{\dot{\vector{r}}^2}{q_l}.\label{lagrange_deriv:deriv3}
                \end{align}
                If one multiplies this by the mass $m$ and sums over all masses, the following expression is obtained:
                \begin{align}
                    \sum_km_k\ddot{\vector{r}}_k\cdot\pderiv{\vector{r}_k}{q_l}=\ &\deriv{}{t}\pderiv{}{\dot{q}_l}\left(\sum_k\frac{1}{2}m\dot{\vector{r}}_k^2\right) - \pderiv{}{q_l}\left(\sum_k\frac{1}{2}m\dot{\vector{r}}_k^2\right)\nonumber\\
                    =\ &\deriv{}{t}\pderiv{T}{\dot{q}_l} - \pderiv{T}{q_l}\label{lagrange_deriv:deriv4},
                \end{align}
                where the total kinetic energy is denoted by $T$ in the last line. Plugging this result into Equation \eqref{lagrange_deriv:deriv1} gives
                \begin{gather}
                    \label{lagrange_deriv:deriv5}
                    \sum_l\left(Q_l - \deriv{}{t}\pderiv{T}{\dot{q}_l} - \pderiv{T}{q_l}\right)\dot{q_l} = 0.
                \end{gather}
                Because all the coordinates $q_l$ are independent, the following relation should hold for all $l$:
                \begin{align}
                    &Q_l - \deriv{}{t}\left(\pderiv{T}{\dot{q}_l}\right) - \pderiv{T}{q_l} = 0\nonumber\\
                    \iff&\deriv{}{t}\left(\pderiv{T}{\dot{q}_l}\right) - \pderiv{T}{q_l} = Q_l.\label{lagrange_deriv:first_kind}
                \end{align}
                This last equation is known as a \textbf{Lagrange equation of the first kind}.

                If the system only contains conservative forces, the force on the $i^{th}$ mass can be written as
                \begin{gather}
                    \label{lagrange_deriv:deriv7}
                    F_i = -\nabla_iV.
                \end{gather}
                With this in mind, one can relate the partial derivatives of the potential to the generalized forces:
                \begin{gather}
                    \label{lagrange_deriv:deriv8}
                    \begin{aligned}
                        \pderiv{V}{q_l} &= \sum_i\left(\nabla_iV\right)\cdot\pderiv{\vector{r}_i}{q_l}\\
                        &=-Q_l.
                    \end{aligned}
                \end{gather}
                Furthermore, the derivative of $V$ with respect to the generalized velocities vanishes. This combined with Equation \eqref{lagrange_deriv:first_kind} gives
                \begin{align}
                    &\deriv{}{t}\left(\pderiv{T}{\dot{q}_l}\right) - \pderiv{T}{q_l} = Q_l\nonumber\\
                    \iff&\deriv{}{t}\left(\pderiv{T}{\dot{q}_l}\right) - \pderiv{T}{q_l} = -\pderiv{V}{q_l} + \pderiv{V}{\dot{q}_l}\nonumber\\
                    \iff&\deriv{}{t}\left(\pderiv{T}{\dot{q}_l} - \pderiv{V}{\dot{q}_l}\right) - \pderiv{}{q_l}\left(T - V\right) = 0.\label{lagrange:deriv9}
                \end{align}
                If one introduces a new variable $L:=T-V$, called the \textbf{Lagrangian}, one gets the \textbf{Lagrangian equation of the second kind}:
                \begin{gather}
                    \label{lagrange_deriv:second_kind}
                    \deriv{}{t}\left(\pderiv{L}{\dot{q}_l}\right) - \pderiv{L}{q_l} = 0.
                \end{gather}\qed
            \end{mdframed}
        \end{proof}

        \begin{proof}[Proof based on the principle of least action]
            \begin{mdframed}[roundcorner=10pt, linecolor=blue, linewidth=1pt]
                First, recall the definition of the \textbf{action}:
                \begin{gather}
                    \label{lagrange_deriv:action_integral}
                    I[q] := \int_{t_1}^{t_2}L\left(q(t),\dot{q}(t),t\right)dt.
                \end{gather}
                The principle of least action (\textbf{Hamilton's principle}) postulates that the action is minimal for the physically relevant path. To this end, define a family of paths
                \begin{gather}
                    \label{lagrange_deriv:family_of_paths}
                    q(t,\alpha) = q(t) + \alpha\eta(t),
                \end{gather}
                where $\eta(t)$ is an arbitrary function satisfying the following boundary conditions:
                \begin{gather}
                    \begin{cases}
                    \eta(t_1) = 0,&\\
                    \eta(t_2) = 0.&
                    \end{cases}
                \end{gather}
                If the action integral is extended to such a family, the integral \eqref{lagrange_deriv:action_integral} becomes a function of $\alpha$:
                \begin{gather}
                    \label{lagrange_deriv:action_integral_over_family}
                    I(\alpha) = \int_{t_1}^{t_2}L\left(q(t,\alpha),\dot{q}(t,\alpha),t\right)dt.
                \end{gather}
                Requiring that the action integral is stationary for the physical path $q(t)$, i.e.~$\alpha=0$, is equivalent to requiring that the derivative at $\alpha=0$ vanishes:
                \begin{gather}
                    \label{lagrange_deriv:stationary_condition}
                    \left.\deriv{I}{\alpha}\right|_{\alpha=0} = 0.
                \end{gather}
                As one evaluates this derivative at $\alpha=0$, $q(t,\alpha)$ can be replaced by $q(t)$ due to \eqref{lagrange_deriv:family_of_paths}:
                \begin{align}
                    \deriv{I}{\alpha}&=\int_{t_1}^{t_2}\left[\pderiv{L}{q}\pderiv{q}{\alpha} + \pderiv{L}{\dot{q}}\pderiv{\dot{q}}{\alpha}\right]dt\nonumber\\
                    &=\int_{t_1}^{t_2}\left[\pderiv{L}{q}\eta(t) + \pderiv{L}{\dot{q}}\dot{\eta}(t)\right]dt.
                \end{align}
                By applying integration by parts to the second term in this integral, one obtains
                \begin{align}
                    \deriv{I}{\alpha}&=\int_{t_1}^{t_2}\left[\pderiv{L}{q}(t)\eta(t) + \pderiv{L}{\dot{q}}(t)\dot{\eta}(t)\right]dt\nonumber\\
                    &=\int_{t_1}^{t_2}\left[\pderiv{L}{q}(t)\eta(t) + \pderiv{L}{\dot{q}}(t)\deriv{\eta}{t}\right]dt\nonumber\\
                    &=\int_{t_1}^{t_2}\pderiv{L}{q}(t)\eta(t)dt + \eta(t_2)\pderiv{L}{\dot{q}}(t_2) - \eta(t_1)\pderiv{L}{\dot{q}}(t_1) - \int_{t_1}^{t_2}\deriv{}{t}\left(\pderiv{L}{\dot{q}}\right)\eta(t)dt.
                \end{align}
                Due to the initial conditions \eqref{lagrange_deriv:stationary_condition} for the function $\eta$, the second and third term vanish:
                \begin{gather}
                    \label{lagrange_deriv:final_integral}
                    \deriv{I}{\alpha}=\int_{t_1}^{t_2}\left[\pderiv{L}{q} - \deriv{}{t}\left(\pderiv{L}{\dot{q}}\right)\right]\eta(t)dt.
                \end{gather}
                Furthermore, because the function $\eta$ was arbitrary, the only possible way that this derivative zero is when the integrand itself is zero:
                \begin{gather}
                    \label{lagrange_deriv:second_kind_with_hamilton}
                    \pderiv{L}{q} - \deriv{}{t}\left(\pderiv{L}{\dot{q}}\right) = 0.
                \end{gather}
                Comparing this result to Equation \eqref{lagrange_deriv:second_kind} shows that one can also obtain the \textbf{Lagrangian equations of the second kind} by starting from the principle of least action.\qed
            \end{mdframed}
        \end{proof}
    \end{formula}

    \begin{definition}[Cyclic coordinate]\index{cyclic}
        If the Lagrangian $L$ does not explicitly depend on a coordinate $q_k$, the coordinate is said to be cyclic.
    \end{definition}

    \begin{theorem}[Noether]\index{Noether}\label{lagrange:noether_cyclic}
        The conjugate momentum of a cyclic coordinate is a conserved quantity:
        \begin{gather}
            \dot{p}_k \overset{\ref{lagrange:conjugate_momentum}}{=} \deriv{}{t}\left(\pderiv{L}{\dot{q}^k}\right) \overset{{\eqref{lagrange:second_kind}}}{=}\pderiv{L}{q^k}=0.
        \end{gather}
    \end{theorem}

\subsection{Kepler problem}\index{Kepler!problem}\label{section:kepler}

    \begin{formula}[Potential for a point mass]\index{gravity}
        \begin{gather}
            \label{classic:gravitational_potential}
            V = -G\frac{M}{r},
        \end{gather}
        where $G = \num{6,67E-11}\frac{\mathrm{Nm}^2}{\mathrm{kg}^2}$ is the \textbf{gravitational constant}.
    \end{formula}

    To solve the Kepler problem it is useful to perform a coordinate transformation. At the same time one passes to the center-of-mass reference frame
    \begin{gather}
        \vector{R} := \frac{m_1\vector{r}_1+m_2\vector{r}_2}{m_1+m_2}\qquad\qquad M:=m_1+m_2
    \end{gather}
    and also introduces the ``reduced mass'':
    \begin{gather}
        \vector{r}:=\vector{r}_1-\vector{r}_2\qquad\qquad\mu:=\frac{m_1m_2}{m_1+m_2}.
    \end{gather}
    With these coordinates, the Kepler Lagrangian becomes:
    \begin{gather}
        L_\text{Kepler} = \frac{1}{2}M\dot{R}^2 + \frac{1}{2}\mu\dot{r}^2 + \frac{GM}{r}.
    \end{gather}
    The first benefit of this transformation is that the equations of motion decouple. The center of mass behaves as a free particle:
    \begin{gather}
        M\ddot{\vector{R}} = 0.
    \end{gather}
    The displacement vector and reduced mass are those of a particle influenced by a gravitational force:
    \begin{gather}
        \mu\ddot{\vector{r}} = \frac{GM}{r^2}\boldsymbol{\hat{e}_r}.
    \end{gather}

    \begin{theorem}[Bertand]\index{Bertand}
        The only central force problems with bound orbits, whose orbits are all closed, are given by the inverse potential
        \begin{gather}
            V(r)\sim\frac{1}{r^2}
        \end{gather}
        and harmonic potential
        \begin{gather}
            V(r)\sim r^2.
        \end{gather}
    \end{theorem}

    \newdef{Laplace-Runge-Lenz vector}{\index{Laplace-Runge-Lenz vector}\label{classic:lrl_vector}
        Although all conservative central force problems have some common symmetries (time translation and rotational symmetries), the central force problems with a $1/r$-potential have an extra symmetry. However, in contrast to the ordinary Euclidean symmetries, this new symmetry is harder to understand. It is a dynamical symmetry \ref{symplectic:dynamical_symmetry} instead of a kinematical one \ref{symplectic:kinematical_symmetry}, i.e.~it cannot simply be deduced from the Lagrangian description.

        Before explaining the symmetry itself, the associated Noether charge is given:
        \begin{gather}
            \vector{A} := \vector{p}\times\vector{L} - mk\vector{r},
        \end{gather}
        where $m$ denotes the mass and $k$ fixes the scale of the potential:
        \begin{gather}
            V(r) = \frac{k}{r}.
        \end{gather}
        The reason why the LRL vector cannot be obtained from a cyclic coordinate of the ordinary Lagrangian is that it is actually associated to a 4-dimensional problem. The central problem with an inverse-square force law can be reformulated as the free motion of a particle in 4 dimensions and only in this description can the Noether charge be obtained from a cyclic coordinate.

        ?? FINISH ??
    }

\section{Hamiltonian mechanics}

    \newdef{Canonical coordinates}{\index{canonical!coordinates}
        Consider the Lagrangian coordinates $(q,\dot{q},t)$. From these one can derive a new set of coordinates, called canonical coordinates, by exchanging the time-derivatives $\dot{q}^i$ in favour of the conjugate momenta $p_i$.

        This is only equivalent if the transformation $(q,\dot{q})\leftrightarrow(q,p)$ is invertible. A sufficient condition is that the Hessian of $L$ with respect to the generalized velocities is invertible.
    }

    \newdef{Hamiltonian function}{\index{Hamilton!function}\label{lagrange:hamiltonian}
        Given a Lagrangian $L$, the Hamiltonian function is defined by the following Legendre transformation \ref{calculus:legendre}:
        \begin{gather}
            H(q,p,t) := \sum_ip_i\dot{q}^i - L(q,p,t).
        \end{gather}
    }

    \newformula{Hamilton's equations}{\index{Hamilton!equations}\label{lagrange:hamilton_equations}
        Inserting the above definition in the action \ref{lagrange:action} and applying the variational principle results in the following equations:
        \begin{align}
            \dot{q}^i &= \pderiv{H}{p_i},\\
            \dot{p_i} &= -\pderiv{H}{q^i}.
        \end{align}
        Systems obeying these equations are called \textbf{Hamiltonian systems}. (See Section \ref{section:hamiltonian_dynamics} for a formal introduction).
    }

\section{Poisson brackets}

    \newdef{Poisson bracket}{\index{Poisson!bracket}
        \nomenclature[O_sympoi]{$\{\cdot,\cdot\}$}{Poisson bracket}
        To stick with the conventions of Definition \ref{symplectic:poisson}, the Poisson bracket is defined as
        \begin{gather}
            \{A,B\} = \pderiv{A}{p}\pderiv{B}{q} - \pderiv{A}{q}\pderiv{B}{p},
        \end{gather}
        where $q,p$ are the canonical coordinates in the Hamiltonian formalism.
    }
    \sremark{As noted before, some authors define the Poisson bracket with the opposite sign. One should always pay attention to which convention is used.}
    \newformula{Total time derivative}{
        Hamilton's equations imply the following expression for the total time-derivative:
        \begin{gather}
            \deriv{F}{t} = \pderiv{F}{t} + \{H,F\},
        \end{gather}
        where $H$ is the Hamiltonian \ref{lagrange:hamiltonian}.
    }

\section{Hamilton-Jacobi equation}

    For a geometrical interpretation see Section \ref{section:hamilton_jacobi}.

    \newdef{Canonical transformations}{\index{canonical!transformation}\label{lagrange:canonical_transformation}
        A canonical transformation is a transformation that leaves the Hamiltonian equations of motion unchanged. Mathematically this means that the transformations leave the action invariant up to a constant or, equivalently, they leave the Lagrangian invariant up to a complete time-derivative:
        \begin{gather}
            \sum_ip_i\dot{q}^i - H(q,p,t) = \sum_iP_i\dot{Q}^i - K(Q,P,t) - \deriv{S}{t}(Q,P,t).
        \end{gather}
        The function $S$ is called the \textbf{generating function} of the canonical transformation. The choice of generating function uniquely determines the transformation (the converse is, however, not true).
    }

    \newformula{Hamilton-Jacobi equation}{\index{Hamilton-Jacobi equation}\index{Hamilton!principal function}
        Sufficient conditions for a function $S$ to be a generating function of canonical transformations are:
        \begin{align*}
            p_i &=\pderiv{S}{q^i},\\
            Q^i &=\pderiv{S}{P_i},
        \end{align*}
        and \[K = H + \pderiv{S}{t}.\]
        Choosing the new Hamiltonian function $K$ to be 0 (this function is sometimes called the \textbf{Kamiltonian}) gives the Hamilton-Jacobi equation:
        \begin{gather}
            \label{lagrange:hamilton_jacobi_equation}
            H\left(q,\pderiv{S}{q},t\right)+\pderiv{S}{t} = 0.
        \end{gather}
        The function $S$ is called \textbf{Hamilton's principal function}.
    }

    \begin{property}
        The new coordinates $P_i$ and $Q^i$ are all constants of motion. This follows immediately from the choice $K = 0$.
    \end{property}

    \newdef{Hamilton's characteristic function}{\index{Hamilton!characteristic function}\index{energy}
        For time-independent systems the HJE can be rewritten as follows:
        \begin{gather}
            \label{lagrange:time_independent_hje}
            H\left(q,\pderiv{S}{q}\right) = -\pderiv{S}{t} =: E.
        \end{gather}
        After a redefinition of $H$ this is the same as Equation \eqref{symplectic:hamilton_jacobi}. One thus obtains the classical result that for time-independent systems the Hamiltonian function is a constant of motion (often called the \textbf{energy}\footnote{Note that this is not a general fact.}). Integration with respect to time gives the following form of the principal function:
        \begin{gather}
            S(q,p,t) = W(q,p) - Et.
        \end{gather}
        The time-independent function $W$ is called Hamilton's characteristic function.
    }

    \begin{property}[St\"ackel condition]\index{St\"ackel potential}
        The Hamilton-Jacobi equation is separable if and only if the potential is of the form
        \begin{gather}
            \label{lagrange:stackel_condition}
            V(q) = \sum_{i=1}^n\ds\frac{W_i(q^i)}{G_i^2(q)}
        \end{gather}
        whenever the Hamiltonian function can be written as
        \begin{gather}
            H(q,p) = \frac{1}{2}\sum_i\frac{p_i^2}{G^2_i(q)} + V(q).
        \end{gather}
        Potentials of this form are called \textbf{St\"ackel potentials}.
    \end{property}

\section{Analytical mechanics}\label{section:analytical_mechanics}
\subsection{Phase space}

    \newdef{Phase space}{\index{phase space}
        The set of all possible $n$-tuples $(q^i,p_i)$ of generalized coordinates and associated momenta is called the phase space of the system. Note that this also includes the tuples that are not solutions of the equations of motion.
    }

    \newdef{Libration}{\index{libration}
        A closed trajectory for which the coordinates take on only a subset of the allowed values. It is the generalization of an oscillation. Topologically it is characterized by a contractible, closed trajectory.
    }
    \newdef{Rotation}{\index{rotation}
        A closed trajectory for which at least one of the variables takes on all possible values. Topologicaly it is characterized by a noncontractible, closed trajectory.
    }

    \newdef{Separatrix}{\index{separatrix}
        When plotting different (closed) trajectories in the phase space of a system, the curve that separates regions of librations and rotations is called the separatrix.\footnote{In general the separatrix of a dynamical system is a curve that separates regions with different behaviour.}
    }

    \newdef{Lagrangian derivative\footnotemark}{\index{Lagrange!derivative|see{material derivative}}\index{material!derivative}\index{advective}\label{lagrange:lagrangian_derivative}
        \footnotetext{Also known as the \textbf{material derivative}, especially when applied to fluid mechanics.}
        Let $a(q,\dot{q},t)$ be a phase space quantity. Its Lagrangian derivative along a path $(q(t),\dot{q}(t))$ in phase space is given by
        \begin{align}
            \Deriv{a}{t} &:= \lim_{\Delta t\rightarrow0}\frac{a(q+\Delta q,\dot{q}+\Delta\dot{q},t+\Delta t) - a(q,\dot{q},t)}{\Delta t}\nonumber\\
            &= \pderiv{a}{t} + \deriv{q^i}{t}\pderiv{a}{q^i} + \deriv{\dot{q}^i}{t}\pderiv{a}{\dot{q}^i}\\
            &= \pderiv{a}{t} + \dot{q}^i\pderiv{a}{q^i} + \deriv{q^i}{t}\pderiv{a}{\dot{q}^i}\,.\nonumber
        \end{align}
        The second term $\dot{\vector{q}}\cdot\nabla a$ in this equation is called the \textbf{advective} term.
    }
    \begin{remark}\index{convective}
        In the case that $a(q,\dot{q},t)$ is a tensor field, the gradient $\nabla$ has to be replaced by the covariant derivative. The advective term is then sometimes called the \textbf{convective} term.
    \end{remark}

    \begin{result}
        For $a(q,\dot{q},t)=\vector{q}$ one obtains
        \begin{gather}
            \Deriv{\vector{q}}{t} = \dot{\vector{q}}.
        \end{gather}
    \end{result}

\subsection{Liouville's theorem}

    \begin{formula}[Liouville's lemma]
        Consider a phase space volume element $dV_0$ moving along a path $(q(t),\dot{q}(t))\equiv x(t)$. The Jacobian $J(x(t))$ associated with this motion is given by
        \begin{gather}
            J(x(t)) = \deriv{V}{V_0} = \det\left(\pderiv{\vector{x}}{\vector{x}_0}\right) = \sum_{ijklmn}\varepsilon^{ijklmn}\pderiv{x^1}{x^i_0}\pderiv{x^2}{x^j_0}\pderiv{x^3}{x^k_0}\pderiv{x^4}{x^l_0}\pderiv{x^5}{x^m_0}\pderiv{x^6}{x^n_0}.
        \end{gather}
        The Lagrangian derivative of this Jacobian is
        \begin{gather}
            \label{lagrange:jacobian_derivative}
            \Deriv{J}{t} = (\nabla\cdot\vector{x})J.
        \end{gather}
        Furthermore, using Hamilton's equations \ref{lagrange:hamilton_equations}, it is easy to prove that
        \begin{gather}
            \nabla\cdot\vector{x} = 0,
        \end{gather}
        and, hence, that the material derivative of $J$ vanishes.
    \end{formula}

    \begin{theorem}[Liouville]\index{Liouville!theorem on phase spaces}\label{lagrange:liouvilles_theorem}
        Let $V(t)$ be a moving phase space volume containing a fixed set of particles. Applying Liouville's lemma gives
        \begin{gather}
            \Deriv{V}{t} = \Deriv{}{t}\int_{\Omega(t)}d^6x = \Deriv{}{t}\int_{\Omega_0}J(\vector{x},t)d^6x_0 = 0.
        \end{gather}
        This implies that the phase space volume of a Hamiltonian system is invariant with respect to time-evolution.
    \end{theorem}
    \begin{remark}[$\clubsuit$]
        Any phase space admits a symplectic form $\omega$ such that the Hamiltonian equations of motion are encoded in a vector field $X_H$. By noting that the volume in phase space is calculated through the symplectic volume form $\vol_\omega\sim\omega^n$, this theorem easily follows from the fact that time evolution is the flow of the Hamiltonian vector field, which preserves the symplectic form.
    \end{remark}

    \newformula{Boltzmann's transport equation}{\index{Boltzmann!transport equation}\index{Vlasov equation}
        Let $F(q,\dot{q},t)$ be the mass distribution function
        \begin{gather}
            M_\mathrm{tot} = \int_{\Omega(t)}F(q,\dot{q},t)d^6x.
        \end{gather}
        From the conservation of mass one can derive the following formula:
        \begin{gather}
            \label{lagrange:boltzmann_transport_gather}
            \Deriv{F}{t} = \pderiv{F}{t} + \deriv{q^i}{t}\cdot\pderiv{F}{q^i} - \nabla V^i\pderiv{F}{\dot{q}^i} = \left[\pderiv{F}{t}\right]_\mathrm{col},
        \end{gather}
        where the right-hand side gives the change of $F$ due to collisions.\footnote{The collisionless form of this equation is sometimes called the \textbf{Vlasov equation}.} This partial differential equation in 7 variables can be solved to obtain $F$.
    }

    Consider a Hamiltonian system with a phase space $\mathcal{V}$. By Liouville's theorem, the phase flow generated by the equations of motion is a volume- or measure-preserving map $g:\mathcal{V}\rightarrow\mathcal{V}$ (Definition \ref{lebesgue:measure_preserving}). This gives rise to the following theorem:
    \begin{theorem}[Poincar\'e recurrence theorem]\index{Poincar\'e!recurrence theorem}
        Let $\mathcal{V}_0$ be the initial phase space volume of the system. For every point $x_0\in\mathcal{V}_0$ and for every neighbourhood $U$ of $x_0$, there exists a point $y\in U$ such that $g^n(y)\in U$ for every $n\in\mathbb{N}$.
    \end{theorem}

    \begin{theorem}[Strong Jeans's theorem\footnotemark]\index{Jeans}\index{isolating integrals}
        \footnotetext{Actually due to \textit{Donald Lynden-Bell}.}
        For a time-independent system, for which almost all orbits are regular, the distribution function can be expressed in terms of three integrals of motion.
    \end{theorem}
    The constants in Jeans's theorem are called the \textup{\textbf{isolating integrals}} of the system.

\subsection{Continuity equation}

    \newformula{Reynolds transport theorem}{\index{Reynolds transport theorem}\index{Leibniz!integral rule}\label{lagrange:reynolds_transport_theorem}
        Consider a quantity \[F = \int_{V(t)}f(q,\dot{q},t)dV.\] Combining Equation \eqref{lagrange:jacobian_derivative} with the divergence theorem \ref{vector:divergence_theorem} gives
        \begin{gather}
            \Deriv{F}{t} = \int_V\pderiv{f}{t}dV + \oint_{\partial V}f\dot{\vector{q}}\cdot d\vector{S}.
        \end{gather}
        This formula can be interpreted as a three-dimensional generalization of the \textit{Leibniz integral rule}.
    }
    \newformula{Continuity equations}{\index{continuity!equation}
        For a conserved quantity the equation above becomes:
        \begin{gather}
            \label{lagrange:lagrangian_continuity_gather}
            \Deriv{f}{t} + (\nabla\cdot\dot{\vector{q}})f = 0
        \end{gather}
        \begin{gather}
            \label{lagrange:eulerian_continuity_gather}
            \pderiv{f}{t} + \nabla\cdot(f\dot{\vector{q}}) = 0.
        \end{gather}
        If one sets $f=\rho$ (the mass density), the first equation is called the \textbf{Lagrangian continuity equation} and the second equation is called the \textbf{Eulerian continuity equation}. Both equations can be found by pulling the Lagrangian derivative inside the integral on the left-hand side of \ref{lagrange:reynolds_transport_theorem}.

        The difference between these two equations corresponds to the way the system is observed. In the Eulerian approach one observes a fixed point in space and measures how a given quantity at that point evolves. In the Lagrangian approach one observes a given point (or particle) in the system and measures how a given quantity evolves around the chosen point as it moves throughout space.
    }
    \begin{result}
        Combining the Reynolds transport theorem with the Lagrangian continuity equation gives the following identity for an arbitrary function $f$:
        \begin{gather}
            \label{lagrange:result1}
            \Deriv{}{t}\int_V\rho fdV = \int_V\rho\Deriv{f}{t}dV.
        \end{gather}
    \end{result}

\subsection{Fluid mechanics}

    \begin{theorem}[Cauchy's stress theorem\footnotemark]\index{Cauchy!fundamental theorem}\index{stress}
        \footnotetext{Also known as \textbf{Cauchy's fundamental theorem}.}
        Knowing the stress vectors acting on the coordinate planes through a point is sufficient to calculate the stress vector acting on an arbitrary plane passing through that point.
    \end{theorem}
    Cauchy's stress theorem is equivalent to the existence of the following tensor:
    \newdef{Cauchy stress tensor}{\index{Cauchy!stress tensor}
        The Cauchy stress tensor is a $(0,2)$-tensor $\mathbf{T}$ that gives the relation between a stress vector associated to a plane and the normal vector $\vector{n}$ to that plane:
        \begin{gather}
            \vector{t}_{(\vector{n})} = \mathbf{T}(\vector{n}).
        \end{gather}
    }
    \begin{example}
        For identical particles the stress tensor is given by
        \begin{gather}
            \mathbf{T} = -\rho\langle\vector{w}\otimes\vector{w}\rangle,
        \end{gather}
        where $\vector{w}$ is the random component of the velocity vector and $\langle\cdot\rangle$ denotes the expectation value \ref{prob:expectation_value}.
    \end{example}

    \begin{theorem}[Cauchy's lemma]\index{Cauchy!lemma}
        The stress vectors acting on opposite planes are equal in magnitude but opposite in direction:
        \begin{gather}
            \vector{t}_{(-\vector{n})} = -\vector{t}_{(\vector{n})}.
        \end{gather}
    \end{theorem}

    \newformula{Cauchy momentum equation}{\index{Cauchy!momentum equation}
        From Newton's second law \ref{classic:force} it follows that
        \begin{gather}
            \Deriv{\vector{P}}{t} = \int_V\vector{f}(x,t)dV + \oint_{\partial V}\vector{t}(x,t)dS,
        \end{gather}
        where $\vector{P}$ is the momentum density, $\vector{f}$ are the ``body'' forces and $\vector{t}$ are the surface forces (such as \textit{shear stress}). Using Cauchy's stress theorem and the divergence theorem \ref{vector:divergence_theorem} one obtains
        \begin{gather}
            \Deriv{\vector{P}}{t} = \int_V\left[\vector{f}(x,t) + \nabla\cdot\mathbf{T}(x,t)\right]dV.
        \end{gather}
        The left-hand side can be rewritten using Equation \eqref{lagrange:result1} as
        \begin{gather}
            \int_V\rho\Deriv{\vector{v}}{t}dV = \int_V\left[\vector{f}(x,t) + \nabla\cdot\mathbf{T}(x,t)\right]dV.
        \end{gather}
    }

\section{Dynamical systems}

    The following property, although seemingly innocuous, is very important for the study of mechanical systems:
    \begin{property}
        For dynamical systems governed by ODEs satisfying the Picard-Lindel\"of conditions \ref{ode:picard_lindelof}, different trajectories never intersect.
    \end{property}

    The above property has an important (but nontrivial) consequence
    \begin{theorem}[Poincar\'e-Bendixson]\index{Poincar\'e-Bendixson}
        In a phase plane, i.e.~a two-dimensional phase space, the only trajectories inside of a closed bounded subregion without fixed points are either closed orbits or trajectories spiralling into closed orbits.
    \end{theorem}
    \begin{result}
        In two-dimensional (Cartesian) phase spaces there cannot exist chaos, i.e.~no strange attractors can exist.
    \end{result}

    \newdef{Lypanuov exponents}{\index{Lyapunov exponent}
        Consider two trajectories of a system denote the distance between them at a time $t_0$ by $s_0:=s(t_0)$ (the initial time can be taken to be 0 without loss of generality). If after some time $t$ these trajectories satisfy
        \begin{gather}
            s(t)\approx e^{\lambda t}s_0,
        \end{gather}
        $\lambda$ is called the Lyapunov exponent of the system.
    }

    \newdef{Limit cycle}{\index{cycle|seealso{limit}}\index{limit!cycle}
        Consider a closed trajectory $C$. If there exist curves that asymptotically $(t\longrightarrow\pm\infty$) converge to $C$, i.e.~their \textbf{limit set} is $C$, one calls $C$ a limit cycle.
    }

    \newdef{Poincar\'e map}{\index{Poincar\'e!map}
        Consider a dynamical system determined by a phase flow $\phi$ and let $S$ be a codimension-1 hypersurface in the phase space $Q$ that is transversal to $\phi$, i.e.~all trajectories intersect $S$ at isolated points. Intuitively, the Poincar\'e map $P:S\rightarrow S$ is defined as the ``map of first return'', i.e.~the image of every point $x\in S$, if it exists, is given by $\phi_T(x)$ with $T=:\min\{t\in\mathbb{R}^+\mid\phi_t(x)\in S\}$.

        One can give a more formal definition (one that also avoids the fact that $P$ would only be partially defined). The Poincar\'e map $P$ is defined as follows:
        \begin{itemize}
            \item $P:U\rightarrow S$ is a differentiable map, where $U\subset S$ is open and connected.
            \item $P|_{P(U)}$ is a diffeomorphism.
            \item For every point $u\in U$, the positive semi-orbit of $u$ intersect $S$ for the first time at $P(u)$.
        \end{itemize}
        The usefulness of this map lies in the fact that it preserves (quasi)periodicity whilst reducing the dimensionality of the space. It is especially useful for the study of 3-dimensional spaces, where the section $S$ is 2-dimensional and, hence, easily visualized.
    }
    \begin{property}[Fixed points]
        Fixed points of the Poincar\'e map correspond to closed orbits.
    \end{property}

\section{Geometric mechanics}

    In this section the current chapter is reformulated in a differential geometric framework. The first step is to reformulate ordinary Newtonian mechanics. The general setting here is a Riemannian manifold $(M,g)$, where the metric is mainly used for defining the kinetic energy $\frac{1}{2}g(v,v)$, together with a second-order ODE in the form of a vector field on $TM$ such that $\pi_*(X_v) = v$, where $\pi$ denotes the tangent bundle projection.

    For integral curves $\gamma$ of second-order ODEs it is easy to show that they are the tangent vector fields of their projections. In particular, if $q_i(t)$ are the local coordinates of a curve $\sigma:=\pi(\gamma)$ on $M$, it can be shown that the tangent coordinates $\dot{q}^i$ of $\gamma$ are exactly the derivatives of the coordinates $q_i$:
    \begin{gather}
        \dot{q}^i(t) = \deriv{q^i}{t}(t).
    \end{gather}
    As such the abuse of notation $\dot{q}^i$ is justified. Conversely, it can be shown that a vector field on $TM$ is second-order if and only if this is true for any local chart, i.e.~if the vector field $X\in\mathfrak{X}(TM)$ can be expressed as follows:
    \begin{gather}
        X = \dot{q}^i\pderiv{}{q^i} + F^i(q,\dot{q})\pderiv{}{\dot{q}^i}.
    \end{gather}
    By writing this vector field as a system of differential equations, a second-order ODE is obtained (hence the terminology):
    \begin{gather}
        \mderiv{2}{q^i}{t} = F^i(q,\dot{q}).
    \end{gather}
    The primary example of such a second-order ODE is the vector field generating the geodesic flow on $TM$, i.e.~the integral curves are the tangent curves of geodesics on $M$. Therefore it should not be surprising that the above equation is similar to Formula \ref{diff:geodesic_equation}. By adopting the notation of Equation \eqref{riemann:autoparallel_equation} one can generalize the geodesic equation to obtain Newton's equations of motion for an arbitrary smooth ``potential'' $U:M\rightarrow\mathbb{R}$:
    \begin{formula}[Newton's equation]\index{Newton!equation}
        Let $(M,g)$ be a Riemannian manifold and let $U:M\rightarrow\mathbb{R}$ be a smooth function. Newton's equations for a curve $\sigma:[a,b]\rightarrow M$ read
        \begin{gather}
            \nabla_{\dot{\sigma}}\dot{\sigma} = -\mathrm{grad}(U)
        \end{gather}
        where $\nabla$ indicates the Levi-Civita connection and $\mathrm{grad}$ denotes the gradient operator \ref{vector:gradient} (here not denoted by $\nabla$ to avoid confusion with the covariant derivative).
    \end{formula}

\subsection{Lagrangian mechanics}

    Now it is time to turn to Noether's theorem and, in particular, the version concerning cyclic coordinates \ref{lagrange:noether_cyclic}. Any diffeomorphism of $M$ induces a diffeomorphism on $TM$ by pushforward.
    \newdef{Lagrangian symmetry}{\index{symmetry}
        A symmetry of a Lagrangian $L:TM\rightarrow\mathbb{R}$ is a diffeomorphism $\phi$ of $M$ such that $\phi^*L = L$. Infinitesimal symmetries, i.e.~the generators of symmetries, are vector fields for which the associated flow is a symmetry.
    }
    Given a complete vector field $X$, one can define the conjugate momentum $\widehat{X}:TM\rightarrow\mathbb{R}$ as follows:
    \begin{gather}
        \label{lagrange:metric_conjugate_momentum}
        \widehat{X}(v) := g(X_{\pi(v)},v).
    \end{gather}
    Using this definition Noether's theorem \ref{lagrange:noether_cyclic} can be reformulated as follows:
    \begin{theorem}[Noether's theorem]\index{Noether!theorem}
        The conjugate momentum of an infinitesimal symmetry is a constant of motion.
    \end{theorem}

    If the conjugate momenta of the coordinate-induced vector fields $\partial_i$ are denoted by $p_i$, the nondegeneracy of $g$ implies that the set $\{q^i,p_i\}_{i\leq n}$ gives well-defined coordinate functions on $T^*M$. The equivalence of the Lagrangian action principle and the Newtonian equations of motion imply that the second-order ODE associated to the potential $U$ takes the following form:
    \begin{gather}
        X_U := \dot{q}^i\pderiv{}{q^i} + \pderiv{L}{q^i}\pderiv{}{p_i}.
    \end{gather}
    After performing the Legendre transformation $E:=p_i\dot{q}^i-L$ to obtain the (Hamiltonian) energy function\footnote{This function will not be called a Hamiltonian function, because this will be reserved for functions on the cotangent bundle.}, Newton's equations can be rewritten in Hamiltonian form:
    \begin{gather}
        X_E = \pderiv{E}{p_i}\pderiv{}{q^i}-\pderiv{E}{q^i}\pderiv{}{p_i}.
    \end{gather}

\subsection{Hamiltonian mechanics}

    The procedure of mapping a (complete) vector field to its conjugate momentum can be generalized to an isomorphism $TM\rightarrow T^*M$ as follows:
    \newdef{Fibre derivative}{\index{fibre!derivative}
        Let $L:TM\rightarrow\mathbb{R}$ be a smooth Lagrangian. The fibre derivative of $L$ is defined as the directional (G\^ateaux) derivative \ref{functional:gateaux} of $L$:
        \begin{gather}
            \langle\mathbb{F}L(v),w\rangle := \left.\deriv{}{t}\right|_{t=0}L(v+tw).
        \end{gather}
        Because $\mathbb{F}L(v)\in\mathcal{L}(TM,\mathbb{R})=T^*M$ by definition of the derivative, one can see that $\mathbb{F}L$ defines a map $TM\rightarrow T^*M$. In local coordinates $(q^i,\dot{q}^i)$ the fiber derivative is given by
        \begin{gather}
            \mathbb{F}L:(q^i,\dot{q}^i)\mapsto\left(q^i,\pderiv{L}{\dot{q}^i}\right)\equiv(q^i,p_i).
        \end{gather}
        As such the classical definition \ref{lagrange:conjugate_momentum} for conjugate momenta is obtained. In the case of kinetic Lagrangians defined by a metric $g$, it is not hard to see that this boils down to Equation \eqref{lagrange:metric_conjugate_momentum} of conjugate momenta given in the previous paragraph.
    }
    \begin{remark}[Legendre transform]\index{Legendre!transformation}
        The fibre derivative $\mathbb{F}L$ is often called the Legendre transform of $L$. Although this does not exactly coincide with \ref{calculus:legendre} or \eqref{info:legendre}, the relation is simple enough. The Legendre transformation $L\mapsto L^*$ (on the tangent bundle) is implemented as $L^*(x) = \langle\mathbb{F}L(x),x\rangle - L(x)$.
    \end{remark}
    Lagrangians for which the Legendre transformation is invertible, i.e.~for which $\mathbb{F}L$ is a diffeomorphism, give rise to equivalent mechanics encoded in the cotangent bundle. Given such a Lagrangian, construct the associated energy function $E$ by a Legendre transformation and map it to a Hamiltonian $H$ on the cotangent bundle as $H:=E\circ\mathbb{F}L^{-1}$. By abuse of notation, write $L\equiv L\circ\mathbb{F}L^{-1}$.) This transformation also induces a Hamiltonian vector field $X_H:=\mathbb{F}L_*X_E$ on $T^*M$ that can alternatively be encoded as $X_E=\mathbb{F}L^*X_H$. If the cotangent coordinates $p_i(\alpha) := \alpha(\partial_i)$ are chosen, it can easily be seen that $p_i\circ\mathbb{F}L\equiv p_i$. This way the Hamiltonian equations remain virtually unchanged when transporting them to the cotangent bundle.

    For any choice of coordinates such that the symplectic form on $T^*M$ takes the standard Darboux form $\omega=dp_i\wedge dq^i$, the Newtonian equations of motion take on a Hamiltonian form. If there exists a coordinate chart in which the Hamiltonian function $H$ does not depend on any of the base coordinates $q^i$, the coordinates are called \textbf{action-angle variables} and the system is said to be \textbf{completely integrable}.\index{action-angle coordinates}\index{integrability}

\subsection{Contact structure}

    By extending the cotangent bundle of the configuration space with a time variable, one can also (re)obtain some interesting features. Consider the following one-form on the trivial line bundle $T^*M\times\mathbb{R}$:
    \begin{gather}
        \alpha := p_idq^i - Hdt \equiv \omega - Hdt.
    \end{gather}
    First of all, this endows the extended cotangent bundle with a contact structure \ref{contact:contact_structure}. Moreover, it can be shown to be a relative integral invariant \ref{bundle:integral_invariant} of the following Pfaffian system:
    \begin{gather}
        \frac{dq^i}{\smallpderiv{H}{p_i}} = dt = \frac{-dp_i}{\smallpderiv{H}{q^i}}.
    \end{gather}
    The vector fields that leave the contact form invariant are exactly those generated by the Hamiltonian vector field $X_H$. Liouville's theorem \ref{lagrange:liouvilles_theorem} is then simply a consequence of the absolute invariance of $d\alpha$.

    A canonical transformation \ref{lagrange:canonical_transformation} was originally defined as a transformation that leaves the Hamiltonian equations of motion invariant. In the contact setting it can be defined as follows:
    \newdef{Canonical transformation}{\index{canonical!transformation}\index{symplectization}
        A transformation of the trivial line bundle $T^*M\times\mathbb{R}$ that leaves both the fibres and the \textbf{symplectization}\footnote{This form turns $T^*M\times\mathbb{R}^2$ into a symplectic manifold.}
        \begin{gather}
            \Omega := d(e^\lambda\alpha)
        \end{gather}
        invariant.

        ?? CHECK THIS STATEMENT (cannot find source) ??
    }

\subsection{Symplectic structure on infinite-dimensional systems}

    Although this section could have been included in Chapter \ref{chapter:symplectic}, it better fits in here since the study of these manifolds is almost always related to the study of physical phenomena such as \textit{solitons}.

    The general definition of a symplectic manifold $(M,\omega)$ remains the same, i.e.~it is a smooth manifold $M$ equipped with a closed, nondegenerate $2$-form $\omega$. Even though the 2-plectic nature is preserved, the content of Remark \ref{symplectic:hamiltonian_forms} applies also to infinite-dimensional systems, i.e.~Hamiltonian functions do not necessarily exist. On the space of Hamiltonian functions (and vector fields) one can define a Poisson structure as follows:
    \begin{gather}
        \{F,G\} := \omega(X^F,X^G).
    \end{gather}

    ?? COMPLETE (e.g.~Palais, cursus Antwerpen) ??

\section{Astronomy}
\subsection{Ellipsoidal coordinates}\index{coordinate!ellipsoidal}

    Consider the following equation:
    \begin{gather}
        \label{astronomy:ellipsoidal_defining_function}
        f(\tau) = \frac{x^2}{\tau + \alpha} + \frac{y^2}{\tau + \beta} + \frac{z^2}{\tau + \gamma} - 1,
    \end{gather}
    where $\alpha<\beta<\gamma<0$. By multiplying by the denominators and choosing $f(\tau) = 0$, a polynomial equation of degree 3 in $\tau$ is obtained. This polynomial can be formally factorized as
    \begin{gather}
        -(\tau-\lambda)(\tau-\mu)(\tau-\nu) = 0.
    \end{gather}
    This solutions of this equation obey the following conditions:
    \begin{itemize}
        \item $\nu\in\ ]-\gamma,-\beta[$\,,
        \item $\mu\in\ ]-\beta,-\alpha[$\,, and
        \item $\lambda\in\ ]-\alpha,\infty[$.
    \end{itemize}
    From the previous two equations one can find a solution for $x^2$ by multiplying by $(\tau+\alpha)$ and taking the limit $\tau\rightarrow-\alpha$. Solutions for $y^2$ and $z^2$ can be found in a similar way:
    \begin{gather}
        \label{astronomy:ellipsoidal_coordinates}
        \begin{cases}
            x^2 = \frac{(\lambda + \alpha)(\mu + \alpha)(\nu + \alpha)}{(\beta - \alpha)(\gamma - \alpha)}&\\
            y^2 = \frac{(\lambda + \beta)(\mu + \beta)(\nu + \beta)}{(\beta - \alpha)(\beta - \gamma)}&\\
            z^2 = \frac{(\lambda + \gamma)(\mu + \gamma)(\nu + \gamma)}{(\alpha - \gamma)(\beta - \gamma)}.&
        \end{cases}
    \end{gather}
    These solutions can be divided in different families depending on the value of $\tau$.

\subsection{Ellipsoid: \texorpdfstring{$\tau=\lambda$}{tau equals lambda}}\index{ellipsoid}\index{focal!ellipse}

    The first family consists of the surfaces defined by fixing $\tau=\lambda$ in Equation \eqref{astronomy:ellipsoidal_defining_function}. By noting that all denominators are positive in this case, it can be seen that the obtained surface is an ellipsoid with the $x$-axis as the shortest axis. By taking the limit $\lambda\longrightarrow\infty$ the equation of a sphere with radius $\sqrt{\lambda}$ is obtained, whilst taking $\lambda\longrightarrow-\alpha$ results in an ellipse in the $yz$-plane. This ellipse is called the \textbf{focal ellipse}.

\subsection{One-sheet hyperboloid: \texorpdfstring{$\tau=\mu$}{tau equals mu}}\index{focal!hyperboloid}\index{hyper-!boloid}

    By fixing $\tau=\mu$ in \eqref{astronomy:ellipsoidal_defining_function} the equation of a one-sheet hyperboloid (also called a \textbf{hyperbolic hyperboloid}) around the $x$-axis is obtained. By taking the limit $\mu\longrightarrow-\alpha$ the hyperboloid collapses on the $yz$-plane and the surface outside the focal ellipse is obtained. If one takes $\mu\longrightarrow-\beta$, the hyperboloid becomes degenerate and one gets the surface inside the \textbf{focal hyperbola} defined by
    \begin{gather}
        \label{astronomy:focal_hyperbola}
        \frac{x^2}{\alpha-\beta} + \frac{z^2}{\gamma-\beta} = 1.
    \end{gather}
    This hyperbola intersects the $z$-plane in the foci of the focal ellipse.

\subsection{Two-sheet hyperboloid: \texorpdfstring{$\tau = \nu$}{}}

    By fixing $\tau=\nu$ in \eqref{astronomy:ellipsoidal_defining_function} the equation of a two-sheet hyperboloid (also called an \textbf{elliptic hyperboloid}) around the $z$-axis is obtained. By taking the limit $\nu\longrightarrow-\beta$ the hyperboloid becomes degenerate and one obtains the surface outside the focal hyperbola \eqref{astronomy:focal_hyperbola}. If $\nu\longrightarrow-\gamma$ the two sheets coincide in the $xy$-plane.

\subsection{Hamiltonian function}

    When writing out the kinetic energy in ellipsoidal coordinates and noting that mixed terms of the form $\pderiv{x^a}{\lambda^i}\pderiv{x^a}{\lambda^j}$ cancel out due to \eqref{astronomy:ellipsoidal_coordinates}, it is clear that the Hamiltonian function can be separated:
    \begin{gather}
        H = \frac{1}{2}\left(\frac{p_\lambda^2}{Q_\lambda^2} + \frac{p_\mu^2}{Q_\mu^2} + \frac{p_\nu^2}{Q_\nu^2}\right) + V,
    \end{gather}
    where $Q_j^2 = \sum_i\left(\pderiv{x^i}{\lambda^j}\right)^2$ are the metric coefficients in ellipsoidal coordinates. After a straightforward calculation these can be found to be:
    \begin{gather}
        Q_\lambda^2 = \frac{1}{4}\frac{(\lambda-\mu)(\lambda-\nu)}{(\lambda+\alpha)(\lambda+\beta)(\lambda+\gamma)},
    \end{gather}
    which is also valid for $\mu$ and $\nu$ after cyclically permutating the coordinates.

    Because of the St\"ackel conditions \eqref{lagrange:stackel_condition}, the potential must be of the form
    \begin{gather}
        V = \sum_i\frac{W_i(\lambda^i)}{Q_i^2}
    \end{gather}
    if one wants to obtain a separable Hamilton-Jacobi equation. Due to the disjoint nature of $\lambda,\mu$ and $\nu$ one can consider $W_\lambda,W_\mu$ and $W_\nu$ as three components of a single function:
    \begin{gather}
        G(\tau) := -4(\tau+\beta)W_\tau(\tau).
    \end{gather}
    The 3D potential is thus completely determined by a univariate function $G(\tau)$.

\subsection{Hamilton-Jacobi equation}

    If a time-independent system is considered, one can use the Hamilton-Jacobi equation \eqref{lagrange:time_independent_hje} as the starting point. By multiplying this equation by $(\lambda - \mu)(\lambda - \nu)(\mu - \nu)$ one obtains
    \begin{multline}
        (\mu - \nu)\left[2(\lambda + \alpha)(\lambda + \beta)(\lambda + \gamma)\left(\deriv{S^\lambda(\lambda)^2}{\lambda}\right)\right.\\ - (\lambda + \alpha)(\lambda + \gamma)G(\lambda) - \lambda^2E \bigg] + \text{cyclic permutations} = 0,
    \end{multline}
    where the multiplicative factor was rewritten in the form $a\lambda^2 + b\mu^2 + c\nu^2$ before multiplying the right-hand side of \eqref{lagrange:time_independent_hje}. This equation can also be rewritten as
    \begin{gather}
        (\mu-\nu)U(\lambda) + (\lambda-\mu)U(\nu) + (\nu-\lambda)U(\mu) = 0.
    \end{gather}
    Differentiating twice with respect to any $\lambda^i$ gives $U''(\tau) = 0$ or, equivalently,
    \begin{gather}
        U(\tau) = I_3 - I_2\tau,
    \end{gather}
    where $I_2$ and $I_3$ are two new integrals of motion.

    Using the Hamiltonian-Jacobi equation \eqref{lagrange:hamilton_jacobi_equation} one can obtain the conjugate momenta. After a lengthy calculation one obtains
    \begin{gather}
        p_\tau^2 = \left(\deriv{S^\tau}{\tau}\right)^2 = \frac{1}{2(\tau+\beta)}\left[E - V_\mathrm{eff}(\tau)\right],
    \end{gather}
    where the effective potential is given by
    \begin{gather}
        V_\mathrm{eff} = \frac{J}{\tau+\alpha} + \frac{K}{\tau+\gamma} - G(\tau).
    \end{gather}
    The two conserved quantities $J$ and $K$ are given by \[J = \frac{\alpha^2E + \alpha I_2 + I_3}{\alpha - \gamma} \qquad\text{and}\qquad K = \frac{\gamma^2E + \gamma I_2 + I_3}{\gamma - \alpha}.\] To be physically acceptable, $p_\tau^2$ should be positive. This leads to following conditions on the energy:
    \begin{gather}
        \begin{cases}
            E&\geq V_\mathrm{eff}(\lambda)\\
            E&\geq V_\mathrm{eff}(\mu)\\
            E&\leq V_\mathrm{eff}(\nu).
        \end{cases}
    \end{gather}
    The generating function $G(\tau)$ should also satisfy some conditions. Note that the St\"ackel potential $V(\lambda,\mu,\nu)$ can be rewritten as
    \begin{gather}
        \label{astronomy:potential2}
        V = -\frac{1}{\lambda - \nu}\left(\frac{F(\lambda) - F(\mu)}{\lambda - \mu} - \frac{F(\mu) - F(\nu)}{\mu - \nu}\right) \leq 0,
    \end{gather}
    where $F(\tau) = (\tau + \alpha)(\tau + \gamma)G(\tau)$. For $\lambda\longrightarrow\infty$ (or $r^2\longrightarrow\infty$) one obtains $V\approx-\frac{F(\lambda)}{\lambda^2}\approx-G(\lambda)$. Because $V\sim \lambda^{-1}$ it is clear that $G(\tau)$ cannot decay faster than $\lambda^{-1/2}$ at infity. Furthermore, one can interpret \eqref{astronomy:potential2} as an approximation of $-F''(\tau)$. It follows that $F(\tau)$ should be convex. For $\tau\longrightarrow-\gamma$ one obtains \[\begin{cases}\alpha + \tau < 0\\\tau + \gamma\longrightarrow 0.\end{cases}\] So, if $G(\tau)$ decays faster than $\frac{1}{\tau + \gamma}$, then $F(\tau)\longrightarrow-\infty$, which is not possible for a convex function.

    To fulfil these conditions assume that the generating function can be written as
    \begin{gather}
        G(\tau) = \frac{GM}{\sqrt{\gamma_0 + \tau}},
    \end{gather}
    where $G$ is the gravitational constant and $M$ is the galactic mass.

    \begin{theorem}[Kuzmin]\index{Kuzmin}
        The spatial mass density function generated by a St\"ackel potential is completely determined by a function of the form $\rho(z)$.
    \end{theorem}
    \begin{result}
        For triaxial mass models in ellipsoidal coordinates the axial ratios are inversely proportional to the axial ratios of the coordinate system.
    \end{result}
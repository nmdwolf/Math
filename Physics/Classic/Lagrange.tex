\chapter{Lagrangian and Hamiltonian Mechanics}\label{chapter:lagrange}

\section{Action}

    \begin{definition}[Generalized coordinates]\index{generalized!coordinates}
        The generalized coordinates $q_k$ are independent coordinates that completely describe the current configuration of a system relative to a reference configuration.

        When a system has $N$ degrees of freedom and $n_c$ constraints, there are $(N - n_c)$ generalized coordinates. Furthermore, every set of generalized coordinates describing the same system contains exactly $(N - n_c)$ coordinates.
    \end{definition}
    \begin{definition}[Generalized velocities]
        The generalized velocities $\dot{q}_k$ are the derivatives of the generalized coordinates with respect to time.
    \end{definition}
    \begin{notation}
        Given a Lagrangian function, depending on $n$ generalized coordinates and their associated velocities, we often introduce the following shorthand notation:
        \begin{gather}
            \label{lagrange:notational_convention_1}
            L\left(q(t),\dot{q}(t),t\right) \equiv L\left(q_1(t),\ldots,q_n(t),\dot{q}_1(t),\ldots,\dot{q}_n(t),t\right)
        \end{gather}
    \end{notation}

    \begin{definition}[Action]\index{action}\label{lagrange:action}
        \begin{gather}
            S[q] := \int_{t_1}^{t_2}L\left(\vector{q}(t),\dot{\vector{q}}(t),t\right)dt.
        \end{gather}
    \end{definition}

\section{Euler-Lagrange equations\texorpdfstring{$^\dag$}\ }\index{Lagrange!equations of motion}\index{Euler-Lagrange|see{Lagrange}}

    \begin{formula}[Euler-Lagrange equation of the first kind]
        \begin{gather}
            \label{lagrange:first_kind}
            \deriv{}{t}\left(\pderiv{T}{\dot{q}^k}\right) - \pderiv{T}{q^k} = Q_k
        \end{gather}
        where $T$ is the total kinetic energy.
    \end{formula}
    \begin{formula}[Euler-Lagrange equation of the second kind]
        \begin{gather}
            \label{lagrange:second_kind}
            \deriv{}{t}\left(\pderiv{L}{\dot{q}^k}\right) - \pderiv{L}{q^k} = 0
        \end{gather}
    \end{formula}

\section{Conservation laws and symmetry properties}

    \begin{definition}[Conjugate momentum]\index{momentum!conjugate}\label{lagrange:conjugate_momentum}
        Also called the \textbf{canonically} conjugate momentum.
        \begin{gather}
            p_k = \pderiv{L}{\dot{q}^k}
        \end{gather}
    \end{definition}
    \begin{definition}[Cyclic coordinate]
        If the Lagrangian $L$ does not explicitly depend on a coordinate $q_k$, the coordinate is said to be cyclic.
    \end{definition}

    \begin{property}[Noether's theorem]\index{Noether!theorem}\label{lagrange:noether_cyclic}
        The conjugate momentum of a cyclic coordinate is a conserved quantity:
        \begin{gather}
            \dot{p}_k \overset{\ref{lagrange:conjugate_momentum}}{=} \deriv{}{t}\left(\pderiv{L}{\dot{q}^k}\right) \overset{{\ref{lagrange:second_kind}}}{=}\pderiv{L}{q^k} \overset{\underset{\text{\tiny{coord.}}}{\text{\tiny{cyclic}}}}{=} 0.
        \end{gather}
    \end{property}

\section{Hamilton's equations}

    \newdef{Canonical coordinates}{\index{canonical!coordinates}
        Consider the generalized coordinates $(q, \dot{q}, t)$ from the Lagrangian formalism. Using these we can define a new set of coordinates, called canonical coordinates, by exchanging the time-derivatives $\dot{q}^i$ in favour of the conjugate momenta $p_i$ (see definition \ref{lagrange:conjugate_momentum}).
    }

    \newdef{Hamiltonian function}{\index{Hamilton!function}\label{hamilton:hamiltonian}
        Given a Lagrangian $L$ one defines the Hamiltonian function as follows:
        \begin{gather}
            H(q, p, t) := \sum_ip_i\dot{q}^i - L(q, p, t).
        \end{gather}
    }

    \newformula{Hamilton's equations\footnotemark}{\index{Hamilton!equations of motion}\label{lagrange:hamilton_equations}
        \footnotetext{Also known as the \textbf{canonical equations of Hamilton}.}
        \begin{align}
            \dot{q}^i &= \pderiv{H}{p_i}\\
            \dot{p_i} &= -\pderiv{H}{q^i}
        \end{align}
        Systems obeying these equations are called \textbf{Hamiltonian systems} (see section \ref{section:hamiltonian_dynamics} for a formal introduction).
    }

    The formula to obtain the Hamiltonian from the Lagrangian is an application of the following more general \textbf{Legendre transformation}:
    \newdef{Legendre transformation}{\index{Legendre!transformation}
        Consider an equation of the following form:
        \begin{gather}
            \label{lagrange:legendre1}
            df = udx + vdy
        \end{gather}
        where $u = \pderiv{f}{x}$ and $v = \pderiv{f}{y}$.

        Suppose we want to perform a coordinate transformation $(x,y)\rightarrow(u,y)$ that preserves the general form of \ref{lagrange:legendre1}. To this end we consider the function
        \begin{gather}
            \label{lagrange:legendre}
            g = f - ux.
        \end{gather}
        Differentiation gives
        \begin{gather*}
            dg = vdy - xdu
        \end{gather*}
        which has the form of \ref{lagrange:legendre1} as desired. The quantities $v$ and $x$ are now given by
        \begin{gather}
            x = -\pderiv{g}{u}\qquad\text{and}\qquad v=\pderiv{g}{y}.
        \end{gather}
        The transition $f\longrightarrow g$ defined by equations \ref{lagrange:legendre1} and \ref{lagrange:legendre} is called a Legendre transformation.
    }
    \begin{remark}
        Although the previous derivation used only 2 coordinates, the definition of Legendre transformations can easily be generalized to more coordinates.
    \end{remark}

\subsection{Poisson brackets}

    \newdef{Poisson bracket}{\index{Poisson!bracket}
        \nomenclature[O_sympoi]{$\{\cdot,\cdot\}$}{Poisson bracket}
        To tick with the conventions of definition \ref{symplectic:poisson} the Poisson bracket is defined as
        \begin{gather}
            \{A,B\} = \pderiv{A}{p}\pderiv{B}{q} - \pderiv{A}{q}\pderiv{B}{p}
        \end{gather}
        where $q,p$ are the generalized coordinates in the Hamiltonian formalism.
    }
    \remark{Some authors define the Poisson bracket with the opposite sign. One should always pay attention to which convention is used.}
    \newformula{Total time derivative}{
        Hamilton's equations imply the following expression for the total time derivative:
        \begin{gather}
            \deriv{F}{t} = \pderiv{F}{t} + \{H, F\}
        \end{gather}
        where $\{\cdot,\cdot\}$ is the Poisson bracket as defined above and $H$ is the Hamiltonian \ref{hamilton:hamiltonian}.
    }

\section{Hamilton-Jacobi equation}

    For a formal introduction see section \ref{section:hamilton_jacobi}.

\subsection{Canonical transformations}

    \newdef{Canonical transformations}{\index{canonical!transformation}
        A canonical transformation is a transformation that leaves the Hamiltonian equations of motion unchanged. Mathematically this means that the transformations leave the action invariant up to a constant, or equivalently, they leave the Lagrangian invariant up to a complete time-derivative:
        \begin{gather}
            \sum_i \dot{q}^ip_i - H(q, p, t) = \sum_i \dot{Q}^iP_i - K(Q, P, t) - \deriv{G}{t}(Q, P, t).
        \end{gather}
        The function $G$ is called the \textbf{generating function} of the canonical transformation. The choice of $G$ uniquely determines the transformation (the converse is not true).
    }

    \newformula{Hamilton-Jacobi equation}{\index{Hamilton-Jacobi}\index{Hamilton!principal function}
        Sufficient conditions for the generating function $S$ are given by:
        \begin{align*}
            P_i &=\ds\pderiv{S}{Q^i}\\
            Q^i &=\ds\pderiv{S}{P_i}
        \end{align*}
        and \[K = H + \pderiv{S}{t}.\]
        Choosing the new Hamiltonian function $K$ to be 0 gives the Hamilton-Jacobi equation:
        \begin{gather}
            \label{lagrange:hamilton_jacobi_equation}
            H\left(q, \pderiv{S}{q}, t\right)+\pderiv{S}{t} = 0.
        \end{gather}
        The function $S$ is called \textbf{Hamilton's principal function}.
    }

    \begin{property}
        The new coordinates $P_i$ and $Q^i$ are all constants of motion. This follows immediately from the choice $K = 0$.
    \end{property}

    \newdef{Hamilton's characteristic function}{\index{Hamilton!characteristic function}\index{energy}
        For time-independent systems the HJE can be rewritten as follows:
        \begin{gather}
            \label{lagrange:time_independent_hje}
            H\left(q, \pderiv{S}{q}\right) = -\pderiv{S}{t} := E.
        \end{gather}
        After a redefinition of $H$ this is the same as equation \ref{symplectic:hamilton_jacobi}. One thus obtains the classical result that for time-independent systems the Hamiltonian function is a constant of motion (the \textbf{energy}). Integration with respect to time then gives the following form of the principal function:\footnote{Note that often $E$ will be the energy, however this is not a general fact.}
        \begin{gather}
            S(q,p,t) = W(q,p) - Et.
        \end{gather}
        The time-independent function $W$ is called Hamilton's characteristic function.
    }

\subsection{St\"ackel potentials}

    \begin{remark}
        If the principal function can be separated into $n$ equations, the HJE splits up into $n$ equations of the form
        \begin{gather}
            h_i\left(q^i, \pderiv{S}{q^i}, \alpha_i\right) = 0.
        \end{gather}
        The partial differential equation for $S$ can thus be rewritten as a system of $n$ ordinary differential equations.
    \end{remark}

    \begin{property}[St\"ackel condition]\index{St\"ackel potential}
        The Hamilton-Jacobi equation is separable if and only if the potential is of the following form:
        \begin{gather}
            \label{lagrange:stackel_condition}
            V(q) = \sum_{i=1}^n\ds\frac{1}{G_i^2(q)}W_i(q^i)
        \end{gather}
        whenever the Hamiltonian function can be written as
        \begin{gather}
            H(q, p) = \frac{1}{2}\sum_i\stylefrac{p_i^2}{G^2_i(q)} + V(q).
        \end{gather}
        Potentials of this form are called \textbf{St\"ackel potentials}.
    \end{property}

\section{Analytical mechanics}
\subsection{Phase space}

    \newdef{Phase space}{\index{phase space}
        The set of all possible $n$-tuples\footnote{Not only those as given by the equations of motion.} $(q^i, p_i)$ of generalized coordinates and associated momenta is called the phase space of the system. Given a system living on a smooth manifold $Q$, its phase space is modelled by the cotangent bundle $T^*Q$.
    }

    \newdef{Libration}{\index{libration}
        A closed trajectory for which the coordinates take on only a subset of the allowed values. It is the generalization of an oscillation. Topologically it is characterized by a closed trajectory that is contractible.
    }
    \newdef{Rotation}{\index{rotation}
        A closed trajectory in which at least one of the variables takes on all possible values. Topologicaly these are characterized as those closed trajectories that are noncontractible.
    }

    \newdef{Separatrix}{\index{separatrix}
        When plotting different (closed) trajectories in the phase space of a system, the curve that separates regions of librations and rotations is called the separatrix.\footnote{In general the separatrix of a dynamical system is a curve that separates regions with different behaviour.}
    }

\subsection{Material derivative}

    \newdef{Lagrangian derivative\footnotemark}{\index{Lagrange!derivative|see{material derivative}}\index{material!derivative}\label{phasespace:lagrangian_derivative}
        \footnotetext{Also known as the \textbf{material derivative}, especially when applied to fluid mechanics.}
        Let $a(\vector{r}, \vector{v}, t)$ be a property defined at every point of the system. The Lagrangian derivative along a path $(\vector{r}(t), \vector{v}(t))$ in phase space is given by
        \begin{align}
            \ds\Deriv{a}{t} &= \lim_{\Delta t\rightarrow0}\ds\frac{a(\vector{r} + \Delta\vector{r}, \vector{v} + \Delta\vector{v}, t+\Delta t) - a(\vector{r}, \vector{v}, t)}{\Delta t}\nonumber\\
            &= \pderiv{a}{t} + \deriv{\vector{r}}{t}\cdot\pderiv{a}{\vector{r}} + \deriv{\vector{v}}{t}\cdot\pderiv{a}{\vector{v}}\nonumber\\
            &= \pderiv{a}{t} + \vector{v}\cdot\nabla a + \deriv{\vector{v}}{t}\cdot\pderiv{a}{\vector{v}}.
        \end{align}
        The second term $\vector{v}\cdot\nabla a$ in this equation is called the \textbf{advective} term.
    }
    \begin{remark}
        In the case that $a(\vector{r}, \vector{v}, t)$ is a tensor field the gradient $\nabla$ has to be replaced by the covariant derivative. The advective term is then called the \textbf{convective} term.
    \end{remark}

    \begin{result}
        If we take $a(\vector{r}, \vector{v}, t) = \vector{r}$ we obtain
        \begin{gather}
            \Deriv{\vector{r}}{t} = \vector{v}.
        \end{gather}
    \end{result}

\subsection{Liouville's theorem}

    \begin{formula}[Liouville's lemma]
        Consider a phase space volume element $dV_0$ moving along a path $(\vector{r}(t), \vector{v}(t))\equiv\vector{x}(t)$. The Jacobian $J(\vector{x}, t)$ associated with this motion is given by
        \begin{gather}
            J(\vector{x}(t)) = \deriv{V}{V_0} = \det\left(\pderiv{\vector{x}}{\vector{x}_0}\right) = \sum_{ijklmn}\varepsilon^{ijklmn}\pderiv{x^1}{x^i_0}\pderiv{x^2}{x^j_0}\pderiv{x^3}{x^k_0}\pderiv{x^4}{x^l_0}\pderiv{x^5}{x^m_0}\pderiv{x^6}{x^n_0}.
        \end{gather}
        The Lagrangian derivative of this Jacobian then becomes
        \begin{gather}
            \label{fluidum:jacobian_derivative}
            \Deriv{J}{t} = (\nabla\cdot\vector{x})J.
        \end{gather}
        Furthermore, using Hamilton's equations \ref{lagrange:hamilton_equations} it is easy to prove that
        \begin{gather}
            \nabla\cdot\vector{x} = 0
        \end{gather}
        and hence that the material derivative of $J$ vanishes.
    \end{formula}

    \begin{theorem}[Liouville]\index{Liouville!theorem on phase spaces}\label{fluidum:liouvilles_theorem}
        Let $V(t)$ be a phase space volume containing a fixed set of particles. Applying Liouville's lemma gives
        \begin{gather}
            \Deriv{V}{t} = \Deriv{}{t}\int_{\Omega(t)}d^6x = \Deriv{}{t}\int_{\Omega_0}J(\vector{x}, t)d^6x_0 = 0.
        \end{gather}
        It follows that the phase space volume of a Hamiltonian system is invariant with respect to time-evolution.
    \end{theorem}
    \begin{remark}
        Recall that any phase space carries the structure of a symplectic form $\omega$ and that the Hamiltonian equations of motion are encoded in a vector field $X_H$. By noting that the volume in phase space is calculated through the symplectic volume form $\text{Vol}_\omega\sim\omega^n$, this theorem easily follows from the fact that time evolution is the flow of the Hamiltonian, and in particular symplectic vector field, $X_H$.
    \end{remark}

    \newformula{Boltzmann's transport equation}{\index{Boltzmann!transport equation}\index{Vlasov equation}
        Let $F(\vector{r}, \vector{v}, t)$ be the mass distribution function
        \begin{gather}
            M_{tot} = \int_{\Omega(t)}F(\vector{r}, \vector{v}, t)d^6x.
        \end{gather}
        From the conservation of mass we can derive the following formula:
        \begin{gather}
            \label{fluidum:boltzmann_transport_gather}
            \Deriv{F}{t} = \pderiv{F}{t} + \deriv{\vector{r}}{t}\cdot\pderiv{F}{\vector{r}} - \nabla V\cdot\pderiv{F}{\vector{v}} = \left[\pderiv{F}{t}\right]_{col}
        \end{gather}
        where the right-hand side gives the change of $F$ due to collisions.\footnote{The collisionless form of this equation is sometimes called the \textbf{Vlasov equation}.} This partial differential equation in 7 variables can be solved to obtain $F(\vector{r}, \vector{v}, t)$.
    }

    Consider a Hamiltonian system with a phase space $\mathcal{V}$. By Liouville's theorem, the phase flow generated by the equations of motion is a measure- or volume-preserving map $g:\mathcal{V}\rightarrow\mathcal{V}$. This leads us to the following theorem:
    \begin{theorem}[Poincar\'e recurrence theorem]\index{Poincar\'e!recurrence theorem}
        Let $\mathcal{V}_0$ be the phase space volume of the system. For every point $x_0\in\mathcal{V}_0$ and for every neighbourhood $U$ of $x_0$ there exists a point $y\in U$ such that $g^n(y)\in U$ for every $n\in\mathbb{N}$.
    \end{theorem}

    \begin{theorem}[Strong Jeans's theorem\footnotemark]\index{Jeans}\index{isolating integrals}
        \footnotetext{Actually due to \textit{Donald Lynden-Bell}.}
        The distribution function $F(\vector{r}, \vector{v})$ of a time-independent system for which almost all orbits are regular can be expressed in terms of 3 integrals of motion.
    \end{theorem}
    The constants in Jeans's theorem are called the \textup{\textbf{isolating integrals}} of the system.

\subsection{Continuity equation}

    \newformula{Reynolds transport theorem\footnotemark}{\index{Reynolds!transport theorem}\index{Leibniz!integral rule}\label{fluidum:reynolds_transport_theorem}
        \footnotetext{This is a 3D extension of the \textit{Leibniz integral rule}.}
        Consider a quantity \[F = \int_{V(t)}f(\vector{r}, \vector{v}, t)dV.\] Using equation \ref{fluidum:jacobian_derivative} together with the divergence theorem \ref{vectorcalculus:divergence_theorem} gives us
        \begin{gather}
            \Deriv{F}{t} = \int_V\pderiv{f}{t}dV + \oint_Sf\vector{v}\cdot d\vector{S}.
        \end{gather}
    }
    \newformula{Continuity equations}{\index{continuity!equation}
        For a conserved quantity the equation above becomes:
        \begin{gather}
            \label{fluidum:lagrangian_continuity_gather}
            \Deriv{f}{t} + (\nabla\cdot\vector{v})f = 0
        \end{gather}
        \begin{gather}
            \label{fluidum:eulerian_continuity_gather}
            \pderiv{f}{t} + \nabla\cdot(f\vector{v}) = 0.
        \end{gather}
        If we set $f = \rho$ (the mass density) then the first equation is called the \textbf{Lagrangian continuity equation} and the second equation is called the \textbf{Eulerian continuity equation}. Both equations can be found by pulling the Lagrangian derivative inside the integral on the left-hand side of \ref{fluidum:reynolds_transport_theorem}.

        The difference between these two equations corresponds to the way we observe the system. In the Eulerian approach one observes a fixed point in space and measures how a given quantity at that point evolves. In the Lagrangian approach one observes a given point (or particle) in the system and measures how a given quantity evolves around the chosen point as it moves throughout space.
    }
    \begin{result}
        Combining the Reynolds transport theorem with the Lagrangian continuity equation gives the following identity for an arbitrary function $f$:
        \begin{gather}
            \label{fluidum:result1}
            \Deriv{}{t}\int_V\rho fdV = \int_V\rho\Deriv{f}{t}dV.
        \end{gather}
    \end{result}

\section{Dynamical systems}

    The following property, although seemingly innocuous, is rather important:
    \begin{property}
        For dynamical systems governed by ODEs satisfying the Picard-Lindel\"of conditions \ref{diffeq:picard_lindelof}, different trajectories never intersect (this follows from the uniqueness part).
    \end{property}

    The above property has an important consequence\footnote{The proof is a bit more involved than that.}
    \begin{theorem}[Poincar\'e-Bendixson]\index{Poincar\'e-Bendixson}
        In a phase plane, i.e. a 2D phase space, the only trajectories inside of a closed bounded subregion without fixed points are either closed orbits or trajectories spiralling into closed orbits.
    \end{theorem}
    \begin{result}
        In 2D (Cartesian) phase spaces there cannot exist chaos, i.e. no strange attractors can exist.
    \end{result}

    \newdef{Lypanuov exponents}{
        Consider two trajectories of a system throughout phase space. Let $s_0 := s(t_0)$ be the distance between these trajectories at a time $t_0$ (we can take this time to be 0 without loss of generality). If after some time $t$ we can write
        \begin{gather}
            s(t) \approx e^{\lambda t}s_0,
        \end{gather}
        we call $\lambda$ the Lyapunov exponent of the system.
    }

    \newdef{Limit cycle}{\index{cycle|seealso{limit}}\index{limit!cycle}
        Consider a closed trajectory $C$. If there exist curves that asymptotically $(t\rightarrow\pm\infty$) converge to $C$, i.e. their \textbf{limit set} is $C$, then we call $C$ a limit cycle.
    }

    \newdef{Poincar\'e map}{\index{Poincar\'e!map}
        Consider a dynamical system determined by a phase flow $\phi$. Let $S$ be a codimension-1 hypersurface in the phase space $Q$ that is transversal to $\phi$, i.e. all trajectories intersect $S$ at isolated points.

        Intuitively, the Poincar\'e map $P:S\rightarrow S$ is defined as the ''first return map'', i.e. for every point $x\in S$ the point $P(x)$, if it exists, is given by $\phi_T(x)$ with $T=:\min\{t\in\mathbb{R}^+:\phi_t(x)\in S\}$.

        One can give a more formal definition (one that also avoids the fact that $P$ would only be partially defined). The Poincar\'e map $P$ is defined as follows:
        \begin{enumerate}
            \item A differentiable map $P:U\rightarrow S$ where $U\subset S$ is open and connected.
            \item $P|_{P(U)}$ is a diffeomorphism.
            \item For every point $u\in U$ the positive semi-orbit of $u$ intersects $S$ for the first time at $P(u)$.
        \end{enumerate}

        The usefulness of this map lies in the fact that it preserves (quasi)periodicity whilst reducing the dimensionality of the space. Its main use lies in the study of 3D spaces where the section $S$ is 2-dimensional and hence easily visualized.
    }
    \begin{property}
        Fixed points of the Poincar\'e map correspond to closed orbits.
    \end{property}

\section{Fluid mechanics}

    \begin{theorem}[Cauchy's stress theorem\footnotemark]\index{Cauchy!fundamental theorem}
        \footnotetext{Also known as \textbf{Cauchy's fundamental theorem}.}
        Knowing the stress vectors acting on the coordinate planes through a point $A$ is sufficient to calculate the stress vector acting on an arbitrary plane passing through $A$.
    \end{theorem}

    The \textit{Cauchy stress theorem} is equivalent to the existence of the following tensor:
    \newdef{Cauchy stress tensor}{\index{Cauchy!stress tensor}
        The Cauchy stress tensor is a $(0,2)$-tensor $\mathbf{T}$ that gives the relation between a stress vector associated to a plane and the normal vector $\vector{n}$ to that plane:
        \begin{gather}
            \vector{t}_{(\vector{n})} = \mathbf{T}(\vector{n}).
        \end{gather}
    }
    \begin{example}
        For identical particles the stress tensor is given by
        \begin{gather}
            \mathbf{T} = -\rho \langle\vector{w}\otimes\vector{w}\rangle
        \end{gather}
        where $\vector{w}$ is the random component of the velocity vector and $\langle\cdot\rangle$ denotes the expectation value (see \ref{prob:expectation_value}).
    \end{example}

    \begin{theorem}[Cauchy's lemma]\index{Cauchy!lemma}
        The stress vectors acting on opposite planes are equal in magnitude but opposite in direction:
        \begin{gather}
            \vector{t}_{(-\vector{n})} = -\vector{t}_{(\vector{n})}.
        \end{gather}
    \end{theorem}

    \newformula{Cauchy momentum equation}{\index{Cauchy!momentum equation}
        From Newton's second law \ref{forces:force} it follows that
        \begin{gather}
            \Deriv{\vector{P}}{t} = \int_V\vector{f}(\vector{x}, t)dV + \oint_S\vector{t}(\vector{x}, t)dS
        \end{gather}
        where $\vector{P}$ is the momentum density, $\vector{f}$ are body forces and $\vector{t}$ are surface forces (such as \textit{shear stress}). Using Cauchy's stress theorem and the divergence theorem \ref{vectorcalculus:divergence_theorem} we get
        \begin{gather}
            \Deriv{\vector{P}}{t} = \int_V\left[\vector{f}(\vector{x}, t) + \nabla\cdot\mathbf{T}(\vector{x}, t)\right]dV.
        \end{gather}
        The left-hand side can be rewritten using \ref{fluidum:result1} as
        \begin{gather}
            \int_V\rho\Deriv{\vector{v}}{t}dV = \int_V\left[\vector{f}(\vector{x}, t) + \nabla\cdot\mathbf{T}(\vector{x}, t)\right]dV.
        \end{gather}
    }

\section{Constrained dynamics}\label{section:constrained_dynamics}

    The foundations for this subject were laid by \textit{Dirac} in \cite{constrained}. By introducing constrains, the coordinates and their momenta are not independent anymore. This implies for example that the Hamiltonian equations of motion have to be modified.

    First, recall the Lagrangian equations of motion \ref{lagrange:second_kind}. By expanding these equations, it can be shown that the accelerations $\ddot{q}$ are uniquely determined by the coordinates and velocities $(q,\dot{q})$ if and only if the Hessian of the Lagrangian is invertible. If this Hessian is not invertible, the definition of the conjugate momenta \ref{lagrange:conjugate_momentum} cannot be inverted to express velocities in terms of momenta. Alternatively, the momenta $p$ and coordinates $q$ are not independent and there must exist relations of the form
    \begin{gather}
        \phi(q,p) = 0.
    \end{gather}

    \begin{remark}
        These constraints are called \textbf{primary constraints}. They do not serve to constrain the range of the coordinates $q$. They only couple the coordinates and the momenta.
    \end{remark}
    \begin{axiom}[Regularity conditions]
        It will always be assumed that the independent constraints, i.e. the minimal subset of constraints that imply the others, satisfy the following (equivalent) conditions:
        \begin{itemize}
            \item The constraints can locally serve as the first coordinates of a new (regular) coordinate system.
            \item The gradients $d\phi_m$ are locally linearly independent.
            \item The variations $\delta\phi_m$ are of order $\varepsilon$ whenever the variations $\delta q^i,\delta p_i$ are of the order $\varepsilon$. (This is the original condition due to \textit{Dirac}.)
        \end{itemize}
    \end{axiom}

    A constrained dynamical system consists of a dynamic system $(M,\omega,H)$ together with a finite collection of constrained equations $\phi_m(q,p)=0$ for $m\in I$. These equations are called the \textbf{primary constraints}. If this sytem was derived from a Lagrangian $L(q,\dot{q})$, the calculus of variations easily extends to these constrained systems, where it gives the following modified Hamiltonian equations:
    \begin{gather}
        \dot{q}^i = \pderiv{H}{p_i} + \sum_{m\in I}u_m\pderiv{\phi_m}{p_i}\\
        \dot{p}_i = -\pderiv{H}{q^i} - \sum_{m\in I}u_m\pderiv{\phi_m}{q^i},
    \end{gather}
    where $u_m$ are functions of the coordinates and velocities that play a role similar to ordinary Lagrange multipliers.
    \begin{remark}
        The above relations follow from the general property that the general solutions to $\lambda_i\delta q^i + \mu_i\delta p^i = 0$ for variations $\delta q^i,\delta p_i$ tangent to the constraint surface are of the form
        \begin{gather}
            \begin{cases}
                \lambda_i = \sum_{m\in I}u_m\pderiv{\phi_m}{q^i}&\\
                \mu^i = \sum_{m\in I}u_m\pderiv{\phi_m}{p_i}.&
            \end{cases}
        \end{gather}
        Combining this result with the usual derivation of Hamilton's equations from a Lagrangian action principle gives the above modified equations.
    \end{remark}

    In terms of Poisson brackets the time evolution of a (time-independent) function is then given by
    \begin{gather}
        \label{lagrange:modified_poisson_evolution}
        \dot{f} = \{H,f\} + \sum_{m\in I}u_m\{\phi_m,f\}.
    \end{gather}

    \begin{method}[General Poisson brackets]
        Until now Poisson brackets were only defined for functions depending on the canonical coordinates $(q,p)$. This definition can be generalized to arbitrary functions through the Lie algebra properties (linearity, antisymmetry and the Jacobi identity) together with the Leibniz property. Furthermore, after working out the Poisson brackets one can use the constraint equations to drop all terms that are proportional to $\phi_m$.

        For example, equation \ref{lagrange:modified_poisson_evolution} can be rewritten as
        \begin{gather}
            \dot{f} = \{H + \sum_{m\in I}u_m\phi_m, f\}.
        \end{gather}
        To prove the equivalence, one can use the linearity and Leibniz properties. This involves the following equality
        \begin{gather}
            \{u_m\phi_m, f\} = \{u_m,f\}\phi_m + u_m\{\phi_m,f\}.
        \end{gather}
        The Poisson brackets in the second term only involve functions depending on $(q,p)$ and can be calculated in the usual way. The first term, however, involves a Poisson bracket of the Lagrange multiplier $u_m$. In general these do not simply depend on $q$ and $p$. The problem is solved because the term is proportional to the constraints, and as such vanishes on-shell. It is important that the constraints are only applied after the Poisson brackets have been fully worked out.
    \end{method}

    \begin{notation}[Weak equality]
        The constraints $\phi_m$ are only 0 on-shell. To distinguish between functional equalities, i.e. equalities that also hold off-shell, and on-shell equalities (also called \textbf{weak equalities}) the latter are often denoted by the $\approx$ symbol. For example, the condition $\phi_m\approx0$ is only a weak equality.
    \end{notation}
    Using the above definitions one can write an arbitrary time derivative as
    \begin{gather}
        \dot{f} \approx \{H_T,f\},
    \end{gather}
    where $H_T := H + \sum_{m\in I}u_m\phi_m$.

    \begin{remark}[Strong becomes weak]
        An important remark regarding weak equalities can be found by taking a Poisson bracket of an $f$ that is strongly 0, i.e. a function that vanishes on-shell and whose variation also vanishes. In this case $\{f,g\}\approx0$ for all functions $g$, i.e. the brackets only vanish weakly. Furthermore, if $f\approx0$, then $\{f,g\}$ does not even have to vanish at all.
    \end{remark}

    \begin{property}[Consistency conditions]
        By taking $f=\phi_n$ for any $n\in I$ in equation \ref{lagrange:modified_poisson_evolution} a set of consistency conditions is obtained:
        \begin{gather}
            \{H,\phi_n\} + \sum_{m\in I}u_m\{\phi_n,\phi_m\} \approx 0.
        \end{gather}
        It is possible that this condition reduces to an inconsistency of the type $1\approx0$. In this case the equations of motion are inconsistent and the theory is not physical. If this is not the case, multiple possibilities can arise:
        \begin{itemize}
            \item After imposing the primary constraints, a tautology $0=0$ is found. This gives no new information.
            \item The equation reduces to an equation not involving the Lagrange multipliers $u_m$. This gives an additional constraint
                \begin{gather}
                    \chi(q,p)=0.
                \end{gather}
                These are called \textbf{secondary constraints}.
            \item The last possibility is that a condition on the coefficients $u_m$ is obtained.
        \end{itemize}
        After having found a set of secondary constraints, this procedure can be iterated until no new contraints or conditions are found. Because the consistency condition are linear in the coefficients $u_m$, the general solution can be written as
        \begin{gather}
            u_m = U_m + v_aV^a _m
        \end{gather}
        where $U_m$ is a solution of the nonhomogeneous equation and the $V^a_m$ are linearly independent solutions of the homogeneous equation
        \begin{gather}
            \sum_{m\in I}u_m\{\phi_n,\phi_m\} = 0.
        \end{gather}
        The resulting coefficients $v_a$ are completely arbitrary functions of time. Therefore, the total Hamiltonian can be written in the form
        \begin{gather}
            H_T = H'(q,p) + v_a(t)\phi^a(q,p)
        \end{gather}
        where $\phi^a := \sum_{m\in I}V^a_m\phi_m$. The occurence of arbitrary functions in the Hamiltonian implies that the evolution of the phase space variables is not unique and, accordingly, that the theory has a gauge freedom.
    \end{property}

    \newdef{First- and second-class}{
        A function $f(q,p)$ is said to be first-class if its Poisson bracket with every constraint (both primary and secondary) is weakly zero or, equivalently, is strongly equal to a linear combination of constraints. The function is said to be second-class otherwise. It can be shown that both the total Hamiltonian $H_T$ and the primary constraints $\phi^a$ are first-class. The number of arbitrary coefficients $v_a$ is equal to the number of primary first-class constraints.
    }
    \begin{notation}
        To distinguish between first- and second-class constraints, one often denotes the latter by a separate symbol $\chi$.
    \end{notation}
    \begin{property}
        The Poisson bracket of two primary first-class functions is first-class, and so is the Poisson bracket of the total Hamiltonian and a first-class primary constraint.
    \end{property}

    \begin{remark}[Dirac conjecture]\index{Dirac!conjecture}
        The primary first-class constraints $\phi^a$ generate gauge transformations in the sense that variations in the coefficients $v_a$, which were arbitrary, give rise to phase space variations that leave the physical state invariant. Some secondary constraints might also generate gauge transformations and \textit{Dirac} even conjectured that this was the case for all constraints. Although by now counterexamples have been found, a common workaround is just to restrict to systems where the conjecture is true. From now on the distinction between priamry and secondary will be dropped. From this point of view it makes sense to define the extended Hamiltonian
        \begin{gather}
            H_E := H_T + v_b(t)\phi^b(q,p),
        \end{gather}
        where $b$ ranges over all secondary first-class constraints. For gauge invariant functions, i.e. those whose Poisson bracket with all first-class constraints vanishes, evolution with all three Hamiltonians $H,H_T$ and $H_E$ is identical. For general functions only $H_E$ takes into account the full gauge freedom.\footnote{Note that $H_T$ is the Hamiltonian that correspond to a Lagrangian approach. $H_E$ gives a more general theory.}
    \end{remark}
    \begin{result}
        The first-class constraints form an algebra with respect to the Poisson bracket. In the case of constant structure functions, this even becomes a Lie algebra and the associated gauge transformations define a submanifold in phase space (by Frobenius's theorem \ref{diff:frobenius}).
    \end{result}

    \begin{formula}[Degrees of freedom]
        The number of degrees of freedom is given by the following formula:
        \begin{align*}
            2\times\text{number of d.o.f.} = &\text{ number of canonical coordinates}\\
            &- \text{number of initial second-class constraints}\\
            &- 2\times\text{number of initial first-class constraints}.
        \end{align*}
    \end{formula}

    \newdef{Dirac bracket}{\index{Dirac!bracket}
        To take care of second-class constraints, \textit{Dirac} introduced a modification of the Poisson bracket:
        \begin{gather}
            \{f,g\}_D := \{f,g\} - \{f,\chi_a\}C^{ab}\{\chi_b,g\},
        \end{gather}
        where the $\chi_a$'s are the second-class constraints and the (invertible) matrix $C^{ab}$ is the inverse of the matrix $C_{ab}:=\{\chi_a,\chi_b\}$.

        The benefit of using the Dirac bracket (after the Poisson bracket has been used to separate constraints in first-class and second-class constraints) is that second-class constraints become strong equalities, i.e. they can be used even before evaluating further (Dirac) brackets. The Dirac bracket satisfies the same algebraic properties as the Poisson bracket.
    }
    Instead of splitting the constraints in first- and second-class instances and having to work with the nontrivial Dirac bracket, one can also try to remove second-class constraints in a different way. In the above formula for the degrees of freedom, the factor 2 on the right-hand side is obtained by the introduction of gauge fixing conditions. What these actually do is turning first-class constraints in second-class ones. In fact, the converse is also possible. One can obtain all second-class constraints as gauge fixed first-class constraints after enlarging the system (although this procedure is not unique). After doing this, there is no need for the Dirac bracket anymore and one can simply work with the Poisson bracket (with the added complexity that all constraints are now only weak).

    \newdef{Gauge-invariant functions}{
        Consider the algebra of smooth functions on phase space $C^\infty(M)$. In the spirit of algebraic geometry the space of functions on the constraint surface $\Sigma$ is given by the quotient algebra $C^\infty(\Sigma):=C^\infty(M)/\mathcal{N}$, where $\mathcal{N}$ is the ideal having $\Sigma$ as its zero locus, i.e. $\mathcal{N}$ is the ideal generated by the constraints $\phi,\chi$. The elements of $C^\infty(\Sigma)$ that are gauge-invariant, i.e. first-class with respect to first-class constraints (or first-class with respect to the Dirac bracket), should be considered as the \textbf{classical observables}.

        The restriction to gauge-invariant functions is also imperative if one wants to extend the bracket operation to $C^\infty(\Sigma)$. In general the ideal $\mathcal{N}$ is not an ideal of the Dirac bracket. The gauge-invariant subalgebra is in fact the maximal subalgebra of $C^\infty(\Sigma)$ for which $\mathcal{N}$ is again an ideal.
    }

    \begin{property}[Geometric characterization]
        When restricted to a first-class constraint surface, i.e. a constraint surface that involves no second-class constraints, the ''symplectic'' form becomes maximally degenerate with rank $\rk(\omega) = \dim(M) - 2\dim(\Sigma)$. This further implies that the constraint surface is co-isotropic and, as a consequence, that the Poisson bracket is ill-defined (since this would involve the inverse of the symplectic form). After passing to the \textbf{reduced phase space}, i.e. the leaf space of the Hamiltonian foliation generated by the constraints, one reobtains a well-defined Poisson bracket that coincides with the ordinary Poisson bracket without any constraints.

        The opposite situation arises for constraint surfaces that only involve second-class constraints. Here the induced symplectic form is of maximal rank $\rk(\omega) = \dim(M) - \dim(\Sigma)$, which implies that the surface is isotropic. Furthermore, the induced Poisson bracket coincides with the restriction of the Dirac bracket.
    \end{property}

\subsection{Gauge algebra}

    In this section the gauge symmetries of local action $S$, i.e. an action \[S[y] := \int L(y,\dot{y},\ldots,t) dt\] where the Lagrangian depends on the derivatives up to a finite order $k$, are considered. A \textbf{gauge transformation} of this action is a coordinate transformation that depends arbitrarily on the time variable, but leaves the action invariant. The most general form of such a transformation is
    \begin{gather}
        \label{lagrange:gauge_trasnformation}
        \delta_\varepsilon y^i = R^i_{0,j}\varepsilon^j + R^i_{1,j}\dot{\varepsilon}^j + \cdots + R^i_{l,j}\mderiv{s}{\varepsilon^j}{t},
    \end{gather}
    where the coefficients $R^i$ are arbitrary functions of time.

    Invariance of the action implies that
    \begin{gather}
        \delta_\varepsilon S = \frac{\delta S}{\delta y^i}\delta_\varepsilon y^i = \frac{\delta S}{\delta y^i}R^i_{j,k}\mderiv{j}{\varepsilon^k}{t} = 0.
    \end{gather}
    Beause this should hold for any value of the transformation parameters $\varepsilon$, one immediately obtains the variational Noether identities:
    \begin{property}[Noether identities]
        If a local action is invariant under the transformation \ref{lagrange:gauge_trasnformation}, then
        \begin{gather}
            \frac{\delta S}{\delta y^i}R^i_{j,k} = 0
        \end{gather}
        for all indices $j,k$. In contrast to Noether's theorem \ref{lagrange:noether_cyclic}, these identities do not imply conserved quantities. Instead they show that the equations of motion are not independent.
    \end{property}
    The structure of the infinitesimal gauge transformations is easily seen to be that of a (real) Lie algebra, whilst that of finite (exponentiated) transformations is a Lie group. However, the gauge algebra $\overline{\mathcal{G}}$ is very large (in fact it is infinite-dimensional) and contains a lot of physically irrelevant information. The simplest example is that of the \textit{zilch symmetries} as referred to by \textit{Freedman} and \textit{Van Proeyen} \cite{supergravity}:
    \newdef{Trivial gauge symmetry}{
        All transformations of the form
        \begin{gather}
            \delta_\varepsilon y^i = \varepsilon^{ij}\frac{\delta S}{\delta y^j},
        \end{gather}
        where $\varepsilon$ is antisymmetric, are trivially gauge transformations. These are physically irrelevant since they are not generated by constraints. All trivial gauge transformations form an ideal $\mathcal{N}$ of the gauge algebra and the physically relavent algebra is the quotient $\mathcal{G} := \overline{\mathcal{G}}/\mathcal{N}$.

        In fact one can show that any gauge transformation, satisfying suitable conditions, that vanishes on-shell is equal to some trivial transformation. ?? EXPLAIN (see HENNEAUX and TEITELBOIM) ??
    }
    A further problem with the gauge algebra is that independent transformations might lead to dependent Noether identities wich implies that there is still some redundancy. To fix this one defines the following minimal set:
    \newdef{Generating set}{\index{open!algebra}
        A generating set\footnote{Sometimes called a \textbf{complete set} of gauge symmetries.} of the gauge algebra is a set of transformations $\delta_\mu y^i = R^i_j\mu^j$ such that every gauge transformation can be written as follows:
        \begin{gather}
            \delta y^i = R_j\varepsilon^j + M^{ij}\frac{\delta S}{\delta y^j},
        \end{gather}
        where $M^{ij}=-M^{ji}$. Because all the coefficients $\varepsilon,M$ might be functions of the coordinates $y,\dot{y},\ldots$, the generating set is generally not a basis for the gauge algebra. Because the gauge algebra is a Lie algebra, there must exist structure functions $C^i_{kl}$ and $M^{ij}_{kl}$ such that
        \begin{gather}
            R^j_k\frac{\delta R^i_l}{\delta y^j} - R^i_l\frac{\delta R^j_k}{\delta y^i} = C_{kl}^mR^i_m + M_{kl}^{ij}\frac{\delta S}{\delta y^j},
        \end{gather}
        where $M^{ij}_{kl} = -M^{ji}_{kl}$. If all $M$ are 0, the algebra is said to be \textbf{closed} (even though the generating itself might not be closed as a Lie algebra because the $C$'s are functions) and otherwise it is said to be \textbf{open}. A generating set is said to be \textbf{irreducible} if there exist no nontrivial combinations of elements:
        \begin{gather}
            \mu_jR^i_j = M^{ij}\frac{\delta S}{\delta y^j}\implies\mu^j = N^{jk}\frac{\delta S}{\delta y^k}.
        \end{gather}
    }

\subsection{Fermionic systems}

    In this section the study of constrained systems with ''fermionic'' or odd statistics is considered. For an introduction to Grassmann numbers, see Section \ref{section:berezin}. In general the phase space will be assumed to be a \textit{supermanifold}.

    First one extends the ordinary Poisson bracket to Grassmann-odd coordinates as follows:
    \begin{gather}
        \{\theta^i,\theta^j\} = 0 = \{\pi_i,\pi_j\}
    \end{gather}
    and
    \begin{gather}
        \{\theta^i,\pi_j\} = \delta^i_j = \{\pi_j,\theta^i\}.
    \end{gather}
    By defining the matrix $C^{ij} := \{z^i,z^j\}$, where $z$ can be any of the $q,p,\theta$ or $\pi$, one can then succintly extend the Poisson bracket to all superfunctions as follows:
    \begin{gather}
        \{f,g\} := \Rderiv{f}{z^i}C^{ij}\pderiv{g}{z^j}.
    \end{gather}
    Note that this is virtually the same expression as the Poisson bracket where the matrix $C$ was the inverse of the symplectic matrix. Writing out all terms gives
    \begin{gather}
        \{f,g\} = \left(\pderiv{f}{p_i}\pderiv{g}{q^i}-\pderiv{f}{q^i}\pderiv{g}{p_i}\right) + (-1)^{\deg(f)}\left(\pderiv{f}{\pi_i}\pderiv{g}{\theta^i} + \pderiv{f}{\theta^i}\pderiv{g}{\pi_i}\right).
    \end{gather}
    The algebraic properties of this generalized Poisson bracket are graded generalizations of those of the ordinary one:
    \begin{align}
        \{f,g\} &= -(-1)^{\deg(f)\deg(g)}\{g,f\}\\
        0 &= \{f,\{g,h\}\} + (-1)^{[\deg(f)+\deg(g)]\deg(h)}\{h,\{f,g\}\}\nonumber\\
        &\ \phantom{= \{f,\{g,h\}\} } + (-1)^{\deg(f)[\deg(g)+\deg(h)]}\{g,\{h,f\}\}\\\nonumber\\
        \{f,gh\} &= \{f,g\}h + (-1)^{\deg(f)\deg(g)}f\{g,h\}\\
        \deg(\{f,g\}) &= \deg(f)+\deg(g).
    \end{align}
    The first two properties state that the generalized Poisson bracket gives rise to a Lie superalgebra \ref{hda:lie_superalgebra}. The third property states that it is in fact a \textit{Poisson superalgebra}, in fact this is the example that lends its name to the algebraic structure. Geometrically the matrix $C$ gives rise to a \textit{supersymplectic structures}.

\subsection{BRST symmetry}\label{section:classical_brst}

    Consider a dynamical system $(M,\omega,H)$ together with a set of first-class constraints $\{\phi_m\}_{m\in I}$. For further convenience the constraints will be assumed to be \textbf{irreducible}, i.e. their Jacobian is assumed to be of full rank on the constraint surface. As was shown before, these constraints generate an algebra under the Poisson bracket. However, more structure exists. One first enlarges the phase space by introducing for every constraint $\phi_m$ a Grassmann-odd\footnote{In fact one can generalize this section to phase spaces which already contain odd variables. In that case the ghost variables should have the opposite parity of the associated constraint.} \textbf{ghost variable} $\eta^m$ and its canonical conjugate $\overline{P}_m$:
    \begin{gather}
        \{\overline{P}_m,\eta^n\} := -\delta^n_m.
    \end{gather}

    \newdef{BRST operator}{\index{BRST}
        To any dynamical system governed by first-class constraints $\{\phi_m\}_{m\in I}$ one can associate a BRST operator $\Omega$ defined by the following conditions:
        \begin{enumerate}
            \item It has ghost number 1, i.e. it is of cohomological degree 1:
                \begin{gather}
                    \text{gh}(\Omega) = 1.
                \end{gather}
            \item It is nilpotent with respect to the Poisson bracket:
                \begin{gather}
                    \{\Omega,\Omega\} = 0.
                \end{gather}
            \item It is Hermitian:
                \begin{gather}
                    \Omega^* = \Omega.
                \end{gather}
            \item It is proportional to the constraints:
                \begin{gather}
                    \Omega = \eta^m\phi_m + \text{terms in ghost momenta}.
                \end{gather}
        \end{enumerate}
    }

    \begin{remark}
        It should be noted that the nilpotency of the BRST operator holds off-shell. In this sense the ghost momenta appearing in its definition are the fields necessary to close the algebra outside the constraint surface. In fact it is important to work in the Hamiltonian formalism if one wants to achieve this off-shell nilpotency. It has been shown that in the Lagrangian formalism this property cannot hold for gauge transformations that only close on-shell. (This latter property is related to the fact that the structure coefficients are generally functions of the canonical variables. Only when they are constants, does the algebra of canonical transformations generated by the constraints close off-shell.)
    \end{remark}

    Because of the algebraic properties above, the BRST operator defines a cohomology theory:
    \begin{property}[BRST cohomology]
        For all functions $f$ on the extended phase space, one has the following equality:
        \begin{gather}
            \{\{f,\Omega\},\Omega\} = 0.
        \end{gather}
        In view of this structure one defines a BRST closed function as a function $f$ satisfying
        \begin{gather}
            \{f,\Omega\} = 0.
        \end{gather}
        A function $f$ is said to be BRST exact if it can be written as
        \begin{gather}
            f = \{g,\Omega\}
        \end{gather}
        for some function $g$. It is clear that BRST cohomology is gauge invariant, since $\Omega$ is gauge invariant. It can also be shown that the BRST operator only depends on the constraint surface and not on choice of a local description. This follows from the fact that two BRST operators associated to the same constraint surface are related by a canonical transformation in extended phase space.
    \end{property}

    For negative ghost numbers it can be shown that BRST cohomology vanishes. In degree 0 the BRST cohomology is characterized as follows:
    \begin{property}[Gauge invariant functions]\label{lagrange:brst_0}
        $H^0(\Omega)$ is isomorphic to the set of equivalence classes of gauge invariant functions $f(q,p)$ under the identification $f\sim g\iff f\approx g$.
    \end{property}

    To give a geometric interpretation of higher cohomology groups, it is useful to define a special exterior derivative on the phase space. Although the gauge algebra spanned by the constraints $\phi_m$ does not necessarily generate a closed gauge group on the full phase space, they do when restricted to the constraint surface. The $|I|$-dimensional leaves of the foliation generated by the constraints are called the \textbf{gauge orbits}. The \textbf{reduced phase space} is defined as the leaf space of this foliation.

    The Hamiltonian vector fields associated to the first-class constraints are tangent to the gauge orbits. Furthermore, by the irreducibility of the constraints, the vector fields form a frame field for the tangent bundle of the gauge orbits.
    \newdef{Longitudinal vector fields}{
        Vector fields that are tangent to the gauge orbits. These are also sometimes said to be \textbf{vertical} in this context (not to be confused with the vertical vectors from Section \ref{section:connections}). Longitudinal forms are defined as the multilinear duals to the longitudinal vector fields.
    }
    The exterior derivative $\mathbf{d}$ of longitudinal one-forms is defined as follows:
    \begin{align}
        \mathbf{d}f &:= df = \partial_ifd\omega^i\\
        \mathbf{d}\omega^i &:= \frac{1}{2}C^i_{jk}(q,p)\omega^j\wedge\omega^k,
    \end{align}
    where the one-forms $\omega^i$ are dual to the basis vector fields $X_i$ and $C^i_{jk}(q,p)$ are the structure functions of the constraint algebra. It is extended to higher forms through the graded Leibniz rule. This differential operator differs from the usual exterior derivative in that it only acts along the gauge orbits. The coefficients of the differential forms
    \begin{gather}
        A = A_{i_1\ldots i_k}(q,p)\omega^{i_1}\wedge\cdots\wedge\omega^{i_k},
    \end{gather}
    are equivalence classes of functions in $C^\infty(M)$, where two functions are identifiied if they coincide on the constraint surface.
    \begin{property}[Cohomology]
        The operator $\mathbf{d}$ defines a coboundary operator on the constraint surface, i.e. modulo the constraints. This implies that one can define cohomology groups $H^k(\mathbf{d})$. It can be shown that these are isomorphic to the BRST cohomology groups for all $k\geq0$ where the one-forms are identified with the corresponding ghost variables.\footnote{In fact, for $k=0$ this is exactly the content of Property \ref{lagrange:brst_0}.}
    \end{property}

\section{Geometric description}

    In this section we reformulate the current chapter in a differential geometric framework (for an introduction to differential geometry see chapter \ref{chapter:manifolds} and onwards). This section is based on \cite{palais_solitons}.

    We first begin by reformulating ordinary Newtonian mechanics. The general setting here is a Riemannian manifold\footnote{The metric is mainly for defining the kinetic term $\frac{1}{2}g(v, v)$.} $(M, g)$ together with a second-order ODE in the form of a vector field on $TM$ such that $\pi_*(X_v) = v$ (where $\pi$ denotes the tangent bundle projection). For integral curves $\gamma$ of second-order ODEs it is easy to show that they are the tangent vector fields of their projections. If $q_i(t)$ are the local coordinates of the base curve $\sigma:=\pi(\gamma)$, then it can be shown that the tangent coordinates $\dot{q}^i$ of $\gamma$ are exactly the derivatives of the coordinates $q_i$:
    \begin{gather}
        \dot{q}^i(t) = \deriv{q^i}{t}(t)
    \end{gather}
    As such the abuse of notation $\dot{q}^i$ is justified. Furthermore, it can be shown that a vector field on $TM$ is second-order if and only if this is true for any local chart, i.e. if the vector field $X\in\mathfrak{X}(TM)$ can be expressed as follows:
    \begin{gather}
        X = \dot{q}^i\pderiv{}{q^i} + F^i(q,\dot{q})\pderiv{}{\dot{q}^i}.
    \end{gather}
    By writing this vector field as a system of differential equations we get the second-order ODE (hence the terminology)
    \begin{gather}
        \mderiv{2}{q^i}{t} = F^i(q,\dot{q}).
    \end{gather}
    The prime example of such a second-order ODE is the vector field generating the geodesic flow on $TM$, i.e. the integral curves are the tangent curves of geodesics on $M$. The similarity between the above equation and equation \ref{diff:geodesic_equation} is therefore striking. By adopting the notation of equation \ref{diff:geodesic} one can generalize the geodesic equation to obtain Newton's equation for an arbitrary smooth ''potential'' $U:M\rightarrow\mathbb{R}$:
    \begin{formula}[Newton's equation]\index{Newton!equation}
        Let $(M, g)$ be a Riemannian manifold and let $U:M\rightarrow\mathbb{R}$ be a smooth function. Newton's equation for a curve $\sigma:[a,b]\rightarrow M$ reads
        \begin{gather}
            \nabla_{\dot{\sigma}}\dot{\sigma} = -\text{grad}(U)
        \end{gather}
        where $\nabla$ indicates the Levi-Civita connection and $\text{grad}$ denotes the gradient operator.
    \end{formula}

\subsection{Lagrangian formalism}

    We now turn to Noether's theorem and in particular the version concerning cyclic coordinates \ref{lagrange:noether_cyclic}. Any diffeomorphism of $M$ induces a diffeomorphism on $TM$ by pushforward. A symmetry of the Lagrangian function $L:TM\rightarrow\mathbb{R}$ is a diffeomorphism $\phi$ of $M$ such that $\phi^*L = L$. Infinitesimal symmetries (or infinitesimal symmetry generators) are then the vector fields for which the flow is a symmetry. Given a complete vector field $X$, one can define the conjugate momentum $\widehat{X}:TM\rightarrow\mathbb{R}$ as follows:
    \begin{gather}
        \label{lagrange:metric_conjugate_momentum}
        \widehat{X}(v) := g(X_{\pi(v)}, v).
    \end{gather}
    Using this definition we can reformulate Noether's theorem \ref{lagrange:noether_cyclic} as follows:
    \begin{theorem}[Noether's theorem]\index{Noether!theorem}
        The conjugate momentum of an infinitesimal symmetry is a constant of motion.
    \end{theorem}

    If one denotes the conjugate momenta of the coordinate-induced vector fields $\partial_i$ by $P_i$, the nondegeneracy of $g$ implies that the set $\{q^i, P_i\}_{i\leq n}$ gives well-defined coordinate functions on $T^*M$. The equivalence of the Lagrangian action principle and the Newtonian equations of motion imply that the second-order ODE associated to the potential $U$ takes the following form:
    \begin{gather}
        X^U := \dot{q}^i\pderiv{}{q^i} + \pderiv{L}{q^i}\pderiv{}{P_i}.
    \end{gather}
    After performing the Legendre transformation $E:=P_i\dot{q}^i-L$ to obtain the (Hamiltonian) energy function\footnote{We refrain from calling the Hamiltonian function, as we reserve this terminology for objects on the cotangent bundle.}, we can rewrite Newton's equations in the Hamiltonian form:
    \begin{gather}
        X^E = \pderiv{E}{P_i}\pderiv{}{q^i}-\pderiv{E}{q^i}\pderiv{}{P_i}.
    \end{gather}

\subsection{Hamiltonian formalism}

    The procedure of mapping a (complete) vector field to its conjugate momentum can be generalized to an isomorphism $TM\rightarrow T^*M$ as follows:
    \newdef{Fibre derivative}{\index{fibre!derivative}
        Let $L:TM\rightarrow\mathbb{R}$ be a smooth Lagrangian. The fiber derivative of $L$ is defined as the Fr\'echet derivative
        \begin{gather}
            \langle FL(v), w\rangle := \left.\deriv{}{t}\right|_{t=0}L(v + tw).
        \end{gather}
        Because $FL(v)\in\mathcal{L}(TM, \mathbb{R})\equiv T^*M$ by definition of the derivative, we see that $FL$ defines a map $TM\rightarrow T^*M$. In local coordinates $(q^i, \dot{q}^i)$ the fiber derivative is given by
        \begin{gather}
            FL:(q^i, \dot{q}^i)\mapsto\left(q^i, \pderiv{L}{\dot{q}^i}\right)\equiv(q^i, p_i).
        \end{gather}
        As such we obtain the classical definition \ref{lagrange:conjugate_momentum} for conjugate momenta. In the case of kinetic Lagrangians defined by a metric $g$, it is not hard to see that this boils down to equation \ref{lagrange:metric_conjugate_momentum} of conjugate momenta given in the previous paragraph.
    }
    \begin{remark}[Legendre transform]\index{Legendre!transformation}
        The fibre derivative $FL$ is often called the Legendre transformation of $L$. Although this does not exactly coincide with definitions \ref{info:legendre} or \ref{lagrange:legendre}, the relation is simple enough. The Legendre transformation $L\mapsto E$ (on the tangent bundle) is implemented as $E(X) = \langle FL(X), X\rangle - L(X)$.
    \end{remark}
    Lagrangians for which the Legendre transformation is invertible, i.e. for which $FL$ is a diffeomorphism, give rise to mechanics on the cotangent bundle: Given such a Lagrangian one constructs the associated energy function $E$ by a Legendre transformation and maps it to a Hamiltonian $H$ on the cotangent bundle as $H:=E\circ FL^{-1}$. (By abuse of notation we set $L\equiv L\circ FL^{-1}$.) The transformation also induces a Hamiltonian vector field on $T^*M$ by $X^H:=FL_*X^E$ or alternatively by $X^E=FL^*X^H$. If we choose cotangent coordinates $p_i(\alpha) := \alpha(\partial_i)$, then we easily see that $p_i\circ FL=P_i$. This way the Hamiltonian equations remain virtually unchanged when transporting them to the cotangent bundle.

    For any choice of coordinates such that the symplectic form on $T^*M$ takes the standard Darboux form $\omega=dp_i\wedge dq^i$, the Newtonian equations of motion take on a Hamiltonian form. If there exists a coordinate chart in which the Hamiltonian function $H$ does not depend on any of the base coordinates $q^i$, then we call the coordinates \textbf{action-angle} variables and the system is said to be \textbf{completely integrable}.\index{action-angle coordinates}

\subsection{Symplectic structure on infinite-dimensional systems}

    Although we could have put this section in chapter \ref{chapter:symplectic}, we thought it better fit here, since the study of these manifolds is almost always related to the study of physical phenomena such as \textit{solitons}.

    The general definition of a symplectic manifold $(M, \omega)$ remains the same, i.e. it is a smooth manifold $M$ equipped with a closed, nondegenerate $2$-form $\omega$. Even though we remain in the 2-plectic setting, the content of remark \ref{symplectic:hamiltonian_forms} applies also to infinite-dimensional systems. If we restrict to the space of Hamiltonian functions (and vector fields) we can define a Poisson structure as follows:
    \begin{gather}
        \{F, G\} := \omega(X^G, X^F).
    \end{gather}

    ?? COMPLETE (e.g. Palais, cursus Antwerpen)??
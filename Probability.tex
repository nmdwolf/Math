\chapter{Probability}\label{chapter:probability}   

\section{Probability}

	\newdef{Axioms of probability}{$ $\index{probability}
	    	\begin{itemize}
			\item $P(E) \geq 0$
        		\item $P(E_1\text{ or }E_2) = P(E_1) + P(E_2)$ if $E_1$ and $E_2$ are exclusive.
		        \item $\displaystyle\sum_SP(E_i) = 1$ where the summation runs over all exclusive events.
		\end{itemize}
	}
	\begin{remark}
    		The second axiom can be defined more generally by saying that the probability $P$ should be $\sigma$-additive. Together with the first axiom and the consequence that $P(\emptyset) = 0$ means that the probability is a measure \ref{lebesgue:measure}.
	\end{remark}
    
	\newdef{Sample space}{\index{sample space}
	    	Let $X$ be a random variable. The set of all possible outcomes of $X$ is called the sample space. The sample space is often denoted by $\Omega$.
	}
	\newdef{Probability space}{\index{probability!space}
	    	Let $(\Omega,\Sigma,P)$ be a measure space\footnotemark. This measure space is called a probability space if $P(X)=1$. Furthermore, the measure $P$ is called a probability measure or simply probability.
	    	\footnotetext{See definition \ref{lebesgue:measure_space}.}
	}

	\newdef{Event}{\index{event}
	    	Let $(\Omega,\Sigma,P)$ be a probability space. An element $S$ of the $\sigma$-algebra $\Sigma$ is called an event.
	}
	
	\begin{remark*}
		From the definition of an event it is clear that a single possible outcome of a measurement can be a part of multiple events. So although only one outcome can occur at the same time, multiple event can occur simultaneously.
	\end{remark*}
	\begin{remark*}
		When working with measure-theoretic probability spaces it is more convenient to use the $\sigma$-algebra (see \ref{set:sigma_algebra}) of events instead of the power set (see \ref{set:power_set}) of all events. Intuitively this seems to mean that some possible outcomes are not treated as events. However we can make sure that the $\sigma$-algebra still contains all 'useful' events by using a 'nice' definition of the used probability space. Further information concerning probability spaces can be found in chapter \ref{chapter:lebesgue}.
	\end{remark*}
    
	\begin{formula}
		Let $A, B$ be two events.
	        \begin{equation}
			\label{prop:union}
        		P(A\cup B) = P(A) + P(B) + P(A\cap B)
		\end{equation}
	\end{formula}

	\newdef{Disjoint events}{
	    	Two events $A$ and $B$ are said to be disjoint if they cannot happen at the same time:
	        \begin{equation}
	        	P(A\cap B) = 0
	        \end{equation}
	}
	\result{If $A$ and $B$ are disjoint, the probability that both $A$ and $B$ are true is just the sum of their individual propabilities.}
    
	\newformula{Complement}{\index{complement}
	    	Let $A$ be an event. The probability of $A$ being false is denoted as $P\left(\overline{A}\right)$ and is given by:
	        \begin{equation}
			\label{prop:complement}
		        P\left(\overline{A}\right) = 1 - P(A)
		\end{equation}
	}
	\begin{result}
		From the previous equation and de Morgan's laws (equations \ref{set:de_morgan_union} and \ref{set:de_morgan_intersection}) we derive the following formula\footnote{Switching the union and intersection has no impact on the validity of the formula.}:
	    	\begin{equation}
			P\left(\overline{A}\cap\overline{B}\right) = 1 - P(A\cup B)
		\end{equation}
    	\end{result}

\section{Conditional probability}
	
	\newdef{Conditional probability}{\index{probability!conditional}
	    	Let $A, B$ be two events. The probability of $A$ given that $B$ is true is denoted as $P(A|B)$.
	        \begin{equation}
	        	\label{prop:conditional_probability}
		        \boxed{P(A|B) = \stylefrac{P(A\cap B)}{P(B)}}
		\end{equation}
	}
	\begin{result}
    		By interchanging $A$ and $B$ in previous equation and by remarking that this has no effect on the quantity $P(A\cap B)$ the following rsult can be deducued:
    		\begin{equation}
			P(A|B)P(B) = P(B|A)P(A)
		\end{equation}
	\end{result}

	\begin{theorem}[Bayes' theorem]\index{Bayes!theorem}
		Let $A, B$ be two events. From the conditional probability \ref{prop:conditional_probability} it is possible to derive following important theorem: 
		\begin{equation}
			\label{prop:theorem:bayes}
		        \boxed{P(A|B) = \stylefrac{P(B|A)P(A)}{P(B)}}
		\end{equation}
	\end{theorem}

	\newdef{Independent events}{\index{independence}
	    	Let $A, B$ be two events. $A$ and $B$ are said to be independent if they satisfy the following relation:
	        \begin{equation}
			P(A\cap B) = P(A)P(B)
		\end{equation}
	}
	\begin{result}
		If $A$ and $B$ are two independent events, then equation \ref{prop:theorem:bayes} simplifies to:
	    	\begin{equation}
			P(A|B) = P(A)
		\end{equation}
	\end{result}

	\begin{property}
		The events $A_1,...,A_n$ are independent if for all $k\leq n$ for each choice of $k$ events the probability of their intersection is equal to the product of their indivudal propabilities.
	\end{property}
	\begin{property}
		The $\sigma$-algebras $\mathcal{F}_1,...,\mathcal{F}_n$ defined on probability space $(\Omega,\mathcal{F},P)$ are independent if for all choices of distinct indices $i_1,...,i_k$ from $\{1,...,n\}$ and for all choices of sets $F_{i_n}\in\mathcal{F}_{i_n}$ the following equation holds:
	        \begin{equation}
	        	\label{prop:independent_sigma_algebras}
			P(F_{i_1}\cap...\cap F_{i_k}) = P(F_{i_1})...P(F_{i_k})
		\end{equation}
	\end{property}
    
	\begin{formula}
		Let $(B_i)_{i\in\mathbb{N}}$ be a sequence of pairwise disjoint events. If $\bigcup_{i=1}^{+\infty}B_i = \Omega$ then the total probability of a given event $A$ can be calculated as follows:
	        \begin{equation}
			\label{probability:total_probability_conditional}
		        P(A) = \sum_{i=1}^{+\infty}P(A|B_i)P(B_i)
		\end{equation}
	\end{formula}

\section{Random variables}

	\newdef{Random variable}{\index{random variable}
	    	Let $(\Omega,\Sigma,P)$ be a probability space. A function $X:\Omega\rightarrow\mathbb{R}$ is called a random variable if $\forall a\in\mathbb{R}:X^{-1}\big([a,+\infty[\big)\in\Sigma$.\footnote{$X^{-1}\big([a,+\infty[\big) = \{\omega\in\Omega:X(\omega)\geq a\}$.}
	}
    
	\newdef{$\sigma$-algebra of a random variable}{\index{$\sigma$-algebra}
	    	Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following family of sets is a $\sigma$-algebra:
	        \begin{equation}
			\label{prop:sigma_algebra_generated_random_variable}
		        X^{-1}(\mathcal{B}) = \{S\in\Sigma : S = X^{-1}(B\in\mathcal{B})\}
		\end{equation}
	}
	
	\begin{notation}
		The $\sigma$-algebra generated by the random variable $X$ is often denoted by $\mathcal{F}_X$, analogous to notation \ref{set:notation:generated_sigma_algebra}.
	\end{notation}
    
	\begin{theorem}
		Let $X,Y$ be two random variables. $X$ and $Y$ are independent if the $\sigma$-algebras generated by them are independent\footnote{See equation \ref{prop:independent_sigma_algebras}.}.
	\end{theorem}

\section{Probability distribution}
	
	\newdef{Probability distribution}{\index{probability}
    		Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following function is a measure on the $\sigma$-algebra of Borel sets:
        	\begin{equation}
        		\label{prop:probability_distribution}
			\boxed{P_X(B) = P(X^{-1}(B))}
		\end{equation}
		This measure is called the probability distribution of $X$.
	}

	\begin{formula}[Change of variable]
		Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$.
	        \begin{equation}
			\label{prop:change_of_variable}
		        \int_\Omega g(X(\omega))dP(\omega) = \int_\mathbb{R}g(x)dP_X(x)
		\end{equation}
	\end{formula}
    
	\newdef{Density}{\index{density}
	    	Let $f$ be a non-negative integrable function and recall theorem \ref{lebesgue:theorem:measure_by_integral}. The function $f$ is called the \textbf{density} of $P$ with respect to the Lebesgue measure $m$.
    	
	        For $P$ to be a probability, $f$ should satisfy the following condition:
	        \begin{equation}
			\int fdm = 1
		\end{equation}
	}
	\newdef{Cumulative distribution function}{\index{cumulative distribution function}
	    	Let $f$ be a density. The c.d.f. corresponding to $f$ is given by:
	        \begin{equation}
			\label{prop:cdf}
		        F(y) = \int_{-\infty}^yf(x)dx
		\end{equation}
	}
    
	\begin{theorem}[Skorokhod's representation theorem]\index{Skorokhod}
		Let $F:\mathbb{R}\rightarrow[0,1]$ be a function that satisfies following 3 properties:
	        \begin{itemize}
			\item $F(x)$ is non-decreasing.
		        \item $\ds\lim_{x\rightarrow-\infty}F(x) = 0$ and $\ds\lim_{x\rightarrow+\infty}F(x) = 1$
	        	\item $F(x)$ is right-continuous: $y\geq y_0, y\rightarrow y_0\implies F(y)\rightarrow F(y_0)$.
		\end{itemize}
	        There exists a random variable $X:[0,1]\rightarrow\mathbb{R}$ defined on the probability space $([0,1],\mathcal{B},m_{[0,1]})$ such that $F = F_X$.
	\end{theorem}
    
	\begin{formula}
		Let the absolutely continuous probability $P_X$ be defined on the product space $\mathbb{R}^n$. Let $f_X$ be the density associated with $P_X$. Let $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be integrable with respect to $P_X$.
	        \begin{equation}
			\int_{\mathbb{R^n}}g(x)dP_X(x) = \int_{\mathbb{R}^n}f_X(x)g(x)dx
		\end{equation}
	\end{formula}
	\begin{result}
	    	Previous formula together with formula \ref{prop:change_of_variable} gives rise to:
    		\begin{equation}
        		\label{prop:omega_int_to_real_int}
			\int_\Omega g(X)dP = \int_\mathbb{R^n}f_X(x)g(x)dx
		\end{equation}
	\end{result}
    
\section{Moments}
\subsection{Expectation value}

	\newdef{Expectation value}{\index{expectation}
    		Let $X$ be random variable defined on a probability space $(\Omega,\Sigma,P)$.
	    	\begin{equation}
        		\label{prop:expectation_value}
			\boxed{E(X) = \int_\Omega XdP}
		\end{equation}
	}
	\begin{notation}
		Other often used notations are $\langle X \rangle$ and $\mu$.
	\end{notation}
    
    \newdef{Moment of order \texorpdfstring{$r$}\ }{\index{moment}
    	The moment of order $r$ is defined as the expectation value of the $r^{th}$ power of $X$ and by equation \ref{prop:omega_int_to_real_int} this becomes:
        \begin{equation}
        	\label{prop:moment}
			E(X^r) = \int x^rf_X(x)dx
		\end{equation}
    }
    \newdef{Central moment of order \texorpdfstring{$r$}\ }{\index{central!moment}
    	\begin{equation}
        	\label{prop:central_moment}
			E((X-\mu)^r) = \int(x-\mu)^rf_X(x)dx
		\end{equation}
    }
    \newdef{Variance}{\index{variance}
    	The central moment of order 2 is called the variance: $V(X) = E((X-\mu)^2)$.
    }
    
	\begin{property}
		If $E(X^n)$ are finite for $n>0$ then for all $k\leq n$, $E(X^k)$ are also finite. If $E(X^n)$ is infinite then for all $k\geq n$, $E(X^k)$ are also infinite.
	\end{property}
	\begin{property}
		Moments of order $n$ are determined by central moments of order $k\leq n$ and central moments of order $n$ are determined by moments of order $k\leq n$.
	\end{property}

	\newdef{Moment generating function}{\index{moment!generating function}
		\begin{equation}
			\label{statistics:moment_generating_function}
		        M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty}e^{tX}P(X)dX
		\end{equation}
	}
	\begin{theorem}
		If the above function exists we can derive the following useful result\footnotemark\ by using the series expansion of the exponential function:
        	\begin{equation}
			\label{statistics:theorem:moment_generating_function}
        		E[X^n] = \left.\mderiv{n}{M_X(t)}{t}\right|_{t=0}
		\end{equation}
		\footnotetext{This property is the reason why \ref{statistics:moment_generating_function} is called the moment generating function.}
	\end{theorem}

	\newdef{Characteristic function}{\index{characteristic!function}
    		Let $X$ be a random variable. The characteristic function of $X$ is defined as follows:
        	\begin{equation}
			\label{prop:characteristic_function}
        		\varphi_X(t) = E(e^{itX})
		\end{equation}
	}
	\begin{property}\label{statistics:characteristic_function_properties}
		The characteristic function has the following properties:
	        \begin{itemize}
        		\item $\varphi_X(0) = 1$
        		\item $|\varphi_X(t)| \leq 1$
		        \item $\varphi_{aX+b}(t) = e^{itb}\varphi_X(at)$
		\end{itemize}
	\end{property}
    
    \begin{formula}
    	If $\varphi_X(t)$ is $k$ times continuously differentiable then $X$ has finite $k^{th}$ moment and
        \begin{equation}
        	\label{prop:characteristic_function_as_moment_generator}
			E(X^k) = \stylefrac{1}{i^k}\mderiv{k}{}{t}\varphi_X(0)
		\end{equation}
        Conversely, if $X$ has finite $k^{th}$ moment then $\varphi_X(t)$ is $k$ times continuously differentiable and the above formula holds.
    \end{formula}
    
    \newformula{Inversion formula}{\index{inversion!formula}
    	Let $X$ be a random varibale. If the c.d.f. of $X$ is continuous at $a, b\in\mathbb{R}$ then
        \begin{equation}
			\label{prop:inversion_formula}
            F_X(b) - F_X(a) = \lim_{c\rightarrow+\infty}\stylefrac{1}{2\pi}\int_{-c}^c\stylefrac{e^{-ita} - e^{-itb}}{it}\varphi_X(t)dt
		\end{equation}
    }
    \begin{formula}
    	If $\varphi_X(t)$ is integrable then the c.d.f. is given by:
		\begin{equation}
			f_X(x) = \stylefrac{1}{2\pi}\int_{-\infty}^{+\infty}e^{-itx}\varphi_X(t)dt
		\end{equation}
	\end{formula}
    \remark{From previous formula it is clear that the density function and the characteristic function are Fourier transformed quantities.}
    

\subsection{Correlation}
    \begin{theorem}\index{independence}
    	\label{prop:independence_expectation_values}
		Let $X, Y$ be two random variables. They are independent if and only if $E(f(X)g(Y)) = E(f(X))E(g(Y))$ holds for all Borel measurable\footnotemark\ bounded functions $f,g$.
	\end{theorem}
    \footnotetext{See definition \ref{lebesgue:borel_measurable_function}.}
    
	\noindent The value $E(XY)$ is equal to the inner product $\langle X|Y \rangle$ as defined in \ref{lebesgue:L2_inner_product}. It follows that independence of random variables implies orthogonality. To generalize this concept, we introduce following notions.
    
    \newdef{Centred random variable}{\index{random variable}
    	Let $X$ be a random variable with finite expectation value $E(X)$. The centred random variable $X_c$ is defined as $X_c = X - E(X)$.
    }
    \newdef{Covariance}{\index{covariance}
    	Let $X,Y$ be two random variables. The covariance of $X$ and $Y$ is defined as follows:
        \begin{equation}
        	\label{prop:covariance}
			\text{cov}(X,Y) = \langle X_c|Y_c \rangle = E\big((X-E(X))(Y-E(Y))\big)
		\end{equation}
        Some basic math gives:
        \begin{equation}
			\text{cov}(X,Y) = E(XY) - E(X)E(Y)
		\end{equation}
    }
    \newdef{Correlation}{\index{correlation}
    	Let $X,Y$ be two random variables. The correlation is defined as the cosine of the angle between $X_c$ and $Y_c$:
        \begin{equation}
			\label{prop:correlation}
            \rho_{XY} = \stylefrac{\text{cov}(X,Y)}{||X||_2||Y||_2}
		\end{equation}
    }
    \result{From theorem \ref{prop:independence_expectation_values} it follows that independent random variables are also uncorrelated.}
    \result{Uncorrelated $X$ and $Y$ satisfy the following equality: $E(XY) = E(X)E(Y)$.}
    
    \begin{property}
		Let $(X_i)_{i\in\mathbb{N}}$ be a sequence of independent random variables. Their variances satisfy the following equation:
        \begin{equation}
			\label{prop:variance_of_sum}
            V\left(\sum_{i=1}^{+\infty}X_i\right) = \sum_{i=1}^{+\infty}V(X_i)
		\end{equation}
	\end{property}
    
\subsection{Conditional expectation}

	Let $(\Omega,\Sigma,P)$ be a probability space. Consider a random variable $X\in L^2(\Omega,\Sigma,P)$ and a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. We know that the spaces $L^2(\Sigma)$ and $L^2(\mathcal{G})$ are complete\footnote{See property \ref{lebesgue:L2_hilbert_space}.} and hence the projection theorem \ref{linalgebra:theorem:projection_theorem} can be applied: for every $X\in L^2(\Sigma)$ there exists a random variable $Y\in L^2(\mathcal{G})$ such that $X-Y$ is orthogonal to $L^2(\mathcal{G})$. This has the following result:
	\begin{equation}
		\forall Z\in L^2(\mathcal{G}):\langle X-Y|Z \rangle = \int_\Omega(X-Y)ZdP = 0
	\end{equation}
	Since $\mathbbm{1}_G\in L^2(\mathcal{G})$ for every $G\in\mathcal{G}$ we find by applying \ref{lebesgue:interchanging_domains_with_indicator_function}:
	\begin{equation}
	    	\label{prop:conditional_expectation_condition}
		\int_G XdP = \int_G YdP
	\end{equation}
	This leads us to introducing the following notion of conditional expectations:
	\newdef{Conditional expectation}{\index{expectation!conditional}
	    	Let $(\Omega,\Sigma,P)$ be a probability space and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\Sigma$. For every $\Sigma$-measurable random variable $X\in L^2(\Sigma)$ there exists a unique (up to a null set) random variable $Y\in L^2(\mathcal{G})$ that satisfies equation \ref{prop:conditional_expectation_condition} for every $G\in\mathcal{G}$. This $Y$ is called the conditional expectation of $X$ given $\mathcal{G}$ and it is denoted by $Y := E(X|\mathcal{G})$:
	        \begin{equation}
			\label{prop:conditional_expectation}
		        \boxed{\int_GE(X|\mathcal{G})dP = \int_GXdP}
		\end{equation}
	}
	\remark{Although our derivation was based on random variables from the $L^2$ class, it is also possible to construct (unique) conditional expectations for random variables from the $L^1$ class by using method \ref{lebesgue:method:linear_proofs}.}


\section{Joint distributions}

	\newdef{Joint distribution}{\index{joint distribution}
	    	Let $X, Y$ be two random variables defined on the same probability space $(\Omega,\Sigma,P)$. Consider the vector $(X, Y):\Omega\rightarrow\mathbb{R}^2$. The distribution of $(X, Y)$ is defined on the Borel sets of the plane $\mathbb{R}^2$ and it is given by the following measure:
	        \begin{equation}
	        	P_{(X, Y)}(B) = P((X, Y)\in B)
		\end{equation}
	}
	\newdef{Joint density}{
	    	If the probability measure from previous definition can be written as
	        \begin{equation}
			P_{(X, Y)}(B) = \int_Bf_{(X, Y)}(x, y)dm_2(x, y)
		\end{equation}
	        for some integrable $f_{(X, Y)}$ it is said that $X$ and $Y$ have a joint density.
	}
    
	\newdef{Marginal distribution}{\index{marginal distribution}
		The distributions of one-dimensional random variables is determined by the joint distribution:
	        \begin{equation}
			P_X(A) = P_{(X, Y)}(A\times\mathbb{R})
		\end{equation}
	        \begin{equation}
			P_Y(A) = P_{(X, Y)}(\mathbb{R}\times A)
		\end{equation}
	}
	
	\begin{result}
		If the joint density exists then the marginal distributions are absolutely continuous and given by
	        \begin{equation}
			f_X(x) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dy
		\end{equation}
	        \begin{equation}
			f_Y(y) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dx
		\end{equation}
	        The converse however is not always true. The one-dimensional densities can be absolutely continuous without the existence of the joint density.
	\end{result}
    
\subsection{Independence}\index{independence}
    
	\begin{theorem}
		Let $X, Y$ be two random variables with joint distribution $P_{(X, Y)}$. $X$ and $Y$ are independent if and only if the joint distribution coincides with the product measure:
	        \begin{equation}
	        	P_{(X, Y)} = P_X\times P_Y
	        \end{equation}
	\end{theorem}
	\remark{\label{prop:remark_independence}
	    	If $X$ and $Y$ are absolutely continuous then the previous theorem also applies with the densities instead of the distributions.
	}
    
    
\subsection{Conditional probability}

	\newformula{Conditional density}{\index{conditional density}
    		Let $X, Y$ be two random variables with joint density $f_{(X, Y)}$. The conditional density of $Y$ given $X\in A$ is:
        	\begin{equation}
			\label{prop:conditional_distribution}
        		h(y|X\in A) = \stylefrac{\int_A f_{(X, Y)}(x, y)dx}{\int_A f_X(x)dx}
		\end{equation}
        	For $X=\{a\}$ this equation fails as the denominator would become 0. However it is possible to avoid this problem by formally putting
        	\begin{equation}
        		\label{prop:formal_conditional}
        		h(y|A=a) = \frac{f_{(X, Y)}(a, y)}{f_X(a)}
		\end{equation}
        	with $f_X(a)\neq0$ which is non-restrictive\marginpar{\dbend} because the probability of having a measurement $(X, Y)\in\{(x,y):f_X(x) = 0\}$ is 0. We can thus define the conditional probability of $Y$ given $X=a$:
        	\begin{equation}
			P(Y\in B|X=a) = \int_B h(y|X=a)dy
		\end{equation}
	}
    
	\newformula{Conditional expectation}{\index{expectation!conditional}
	    	\begin{equation}
			E(Y|X)(\omega) = \int_\mathbb{R}yh(y|X(\omega))dy
		\end{equation}
	        Furthermore, let $\mathcal{F}_X$ denote the $\sigma$-algebra generated by the random variable $X$. Using Fubini's theorem we can prove that for all sets $A\in\mathcal{F}_X$ the following equality, which should be compared with equation \ref{prop:conditional_expectation}, holds:
	        \begin{equation}
			\int_AE(Y|X)dP = \int_AYdP
		\end{equation}
	}
	\remark{Following from previous two equations we can say that the conditional expectation $E(Y|X)$ is the best representation of the random variable $Y$ as a function of $X$ (i.e. measurable with respect to $\mathcal{F}_X$).}
    
	\begin{property}
	    	As mentioned above, applying Fubini's theorem gives:
	    	\begin{equation}
			\boxed{E(E(Y|X)) = E(Y)}
		\end{equation}
	\end{property}

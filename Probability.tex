\chapter{Probability}\label{chapter:probability}

\section{Probability}

    \newdef{Axioms of probability}{\index{probability}
        The following list of axioms (introduced by Kolmogorov) states when a measure space $(\Omega, \Sigma, P)$ defines a space that supports probability theory:
        \begin{enumerate}
            \item $P(E) \geq 0$
            \item $P\left(\bigcup_{i\in I}E_i\right)=\sum_{i\in I}P(E_i)$ if all $E_i$ are mutually exclusive.\footnote{Some people require $I$ to be finite. However, this is not necessary if we use $\sigma$-algebras (as we always do for measure spaces).}
            \item $P(\Omega) = 1$.
        \end{enumerate}
    }
    \begin{remark}
        The second axiom is exactly the same as saying that the probability $P$ should be a $\sigma$-additive function. Together with the first axiom and the consequence that $P(\emptyset) = 0$ this means that the probability function $P$ is a measure \ref{lebesgue:measure}.
    \end{remark}

    \newdef{Probability space}{\index{probability!space}
        Let $(\Omega,\Sigma,P)$ be a measure space\footnote{See definition \ref{lebesgue:measure_space}.}. This measure space is called a probability space if $P(X)=1$. Furthermore, the measure $P$ is called a probability measure or simply probability.
    }

    \newdef{Random variable}{\index{random variable}
        Let $(\Omega,\Sigma,P)$ be a probability space. A function $X:\Omega\rightarrow\mathbb{R}$ is called a random variable if $\forall a\in\mathbb{R}:X^{-1}\big([a,+\infty[\big)\in\Sigma$.\footnote{$X^{-1}\big([a,+\infty[\big) = \{\omega\in\Omega:X(\omega)\geq a\}$.}
    }

    \newdef{$\sigma$-algebra of a random variable}{\index{$\sigma$-algebra}
        Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following family of sets is a $\sigma$-algebra:
        \begin{gather}
            \label{prob:sigma_algebra_generated_random_variable}
            X^{-1}(\mathcal{B}) = \{S\in\Sigma : S = X^{-1}(B\in\mathcal{B})\}
        \end{gather}
    }
    \begin{notation}
        The $\sigma$-algebra generated by the random variable $X$ is often denoted by $\mathcal{F}_X$, analogous to notation \ref{set:notation:generated_sigma_algebra}.
    \end{notation}

    \newdef{Sample space}{\index{sample space}
        Let $X$ be a random variable. The set of all possible outcomes of $X$ is called the sample space. The sample space is often denoted by $\Omega$.
    }

    \newdef{Event}{\index{event}
        Let $(\Omega,\Sigma,P)$ be a probability space. An element $S$ of the $\sigma$-algebra $\Sigma$ is called an event.
    }

    \begin{remark*}
        From the definition of an event it is clear that a single possible outcome of a measurement can be a part of multiple events. So although only one outcome can occur at the same time, multiple events can occur simultaneously.
    \end{remark*}
    \begin{remark*}
        When working with measure-theoretic probability spaces it is more convenient to use the $\sigma$-algebra (see \ref{set:sigma_algebra}) of events instead of the power set (see \ref{set:power_set}) of all events. Intuitively this seems to mean that some possible outcomes are not treated as events. However we can make sure that the $\sigma$-algebra still contains all ''useful'' events by using a ''nice'' definition of the used probability space. Further information concerning probability spaces can be found in chapter \ref{chapter:lebesgue}.
    \end{remark*}

    \begin{formula}[Union]\label{prob:union}
        Let $A, B$ be two events. The probability that at least one of them occurs is given by the following formula:
        \begin{gather}
            P(A\cup B) = P(A) + P(B) + P(A\cap B).
        \end{gather}
    \end{formula}

    \newdef{Disjoint events}{
        Two events $A$ and $B$ are said to be disjoint if they cannot happen at the same time:
        \begin{gather}
            P(A\cap B) = 0.
        \end{gather}
    }
    \result{If $A$ and $B$ are disjoint, the probability that both $A$ and $B$ occur is just the sum of their individual probabilities.}

    \newformula{Complement}{\index{complement}
        Let $A$ be an event. The probability of $A$ being false is denoted as $P\left(\overline{A}\right)$ and is given by
        \begin{gather}
            \label{prob:complement}
            P\left(\overline{A}\right) = 1 - P(A).
        \end{gather}
    }
    \begin{result}
        From the previous equation and de Morgan's laws (equations \ref{set:de_morgan_union} and \ref{set:de_morgan_intersection}) we derive the following formula\footnote{Switching the union and intersection has no impact on the validity of the formula.}:
        \begin{gather}
            P\left(\overline{A}\cap\overline{B}\right) = 1 - P(A\cup B).
        \end{gather}
    \end{result}

\section{Conditional probability}

    \newdef{Conditional probability}{\index{probability!conditional}
        Let $A, B$ be two events. The probability of $A$ given that $B$ is true is denoted as $P(A|B)$:
        \begin{gather}
            \label{prob:conditional_probability}
            P(A|B) = \stylefrac{P(A\cap B)}{P(B)}
        \end{gather}
    }
    By interchanging $A$ and $B$ in previous equation and by observing that this has no effect on the quantity $P(A\cap B)$ the following important result can be deduced:
    \begin{theorem}[Bayes]\index{Bayes}\label{prob:theorem:bayes}
        Let $A, B$ be two events. From the definition conditional probability \ref{prob:conditional_probability} it is possible to derive the following important statement:
        \begin{gather}
            P(A|B) = \stylefrac{P(B|A)P(A)}{P(B)}.
        \end{gather}
    \end{theorem}

    \begin{formula}
        Let $(B_i)_{i\in\mathbb{N}}$ be a sequence of pairwise disjoint events. If $\bigsqcup_{i=1}^{+\infty}B_i = \Omega$ then the total probability of a given event $A$ can be calculated as follows:
        \begin{gather}
            \label{probability:total_probability_conditional}
            P(A) = \sum_{i=1}^{+\infty}P(A|B_i)P(B_i)
        \end{gather}
    \end{formula}

    \newdef{Independent events}{\index{independence}
        Let $A, B$ be two events. $A$ and $B$ are said to be independent if they satisfy the following relation:
        \begin{gather}
            P(A\cap B) = P(A)P(B).
        \end{gather}
    }
    \begin{result}
        If $A$ and $B$ are two independent events then Bayes' theorem simplifies to
        \begin{gather}
            P(A|B) = P(A).
        \end{gather}
    \end{result}
    The above definition can be generalized to multiple events:
    \begin{definition}
        The events $A_1,...,A_n$ are independent if for each choice of $k$ events the probability of their intersection is equal to the product of their individual probabilities.
    \end{definition}
    This definition can be stated in terms of $\sigma$-algebras:
    \begin{definition}
        The $\sigma$-algebras $\mathcal{F}_1,...,\mathcal{F}_n$ defined on a probability space $(\Omega,\mathcal{F},P)$ are independent if for all choices of distinct indices $i_1,...,i_k$ and for all choices of sets $F_{i_n}\in\mathcal{F}_{i_n}$ the following equation holds:
        \begin{gather}
            \label{prob:independent_sigma_algebras}
            P(F_{i_1}\cap...\cap F_{i_k}) = P(F_{i_1})...P(F_{i_k})
        \end{gather}
    \end{definition}
    \begin{result}
        Let $X,Y$ be two random variables. $X$ and $Y$ are independent if the $\sigma$-algebras generated by them are independent.
    \end{result}

\section{Probability distribution}

    \newdef{Probability distribution}{\index{probability!distribution}\label{prob:probability_distribution}
        Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following function is a measure on the Borel $\sigma$-algebra:
        \begin{gather}
            P_X(B) = P(X^{-1}(B)).
        \end{gather}
        This measure is called the probability distribution of $X$.
    }

    \begin{formula}[Change of variables]
        Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. By substituting $\omega=X^{-1}(B)$ and applying previous definition we obtain
        \begin{gather}
            \label{prob:change_of_variable}
            \int_\Omega g(X(\omega))dP(\omega) = \int_\mathbb{R}g(x)dP_X(x)
        \end{gather}
    \end{formula}

    \newdef{Density}{\index{density}
        Let $f$ be a non-negative integrable function and recall theorem \ref{lebesgue:theorem:measure_by_integral}. The function $f$ is called the \textbf{density} of the measure $P:=\int fdm$ (with respect to the Lebesgue measure $m$).

        For $P$ to be a probability, $f$ should satisfy the following condition:
        \begin{gather}
            \int_\Omega fdx = 1
        \end{gather}
    }
    \newdef{Cumulative distribution function}{\index{cumulative distribution function}
        \nomenclature[A_CDF]{CDF}{Cumulative distribution function}
        Let $f$ be a density. The cumulative distribution function (CDF) corresponding to $f$ is defined by the following formula:
        \begin{gather}
            \label{prob:cdf}
            F(y) = \int_{-\infty}^yf(x)dx.
        \end{gather}
    }

    \begin{theorem}[Skorokhod's representation theorem]\index{Skorokhod}
        Let $F:\mathbb{R}\rightarrow[0,1]$ be a function that satisfies the following 3 properties:
        \begin{itemize}
            \item $F(x)$ is non-decreasing.
            \item $\ds\lim_{x\rightarrow-\infty}F(x) = 0$ and $\ds\lim_{x\rightarrow+\infty}F(x) = 1$
            \item $F(x)$ is right-continuous: $y\rightarrow^+ y_0\implies F(y)\rightarrow F(y_0)$.
        \end{itemize}
        There exists a random variable $X:[0,1]\rightarrow\mathbb{R}$ defined on the probability space $([0,1],\mathcal{B},m_{[0,1]})$ such that $F = F_X$.
    \end{theorem}

    The Radon-Nikodym theorem\footnote{See section \ref{lebesgue:section:Radon-Nikodym}.} implies the following formula:
    \begin{formula}
        Consider an absolutely continuous probability function $P_X$ defined on the product space $\mathbb{R}^n$. Let $f_X$ be the density associated with $P_X$ and let $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be integrable with respect to $P_X$.
        \begin{gather}
            \int_{\mathbb{R}^n}g(x)dP_X(x) = \int_{\mathbb{R}^n}f_X(x)g(x)dx
        \end{gather}
    \end{formula}
    \begin{result}
        Previous formula together with formula \ref{prob:change_of_variable} gives rise to
        \begin{gather}
            \label{prob:omega_int_to_real_int}
            \int_\Omega g(X)dP = \int_{\mathbb{R}^n}f_X(x)g(x)dx.
        \end{gather}
    \end{result}

    \newdef{Convergence in distribution}{
        A sequence of random variables $X_n$ is said to convergence in distribution to a random variable $Y$ if $\lim_{n\rightarrow\infty}F_n(x)=F(x)$ for all $x\in\mathbb{R}$ where $F_n$ is the cumulative distribution function of $X_n$ and $F$ is the cumulative distribution function of $Y$.
    }
    \begin{notation}
        If a sequence $X_n$ converges in distribution to a random variable $Y$ then this is often denoted by $X_n\xrightarrow{\ \ d\ \ }Y$. Sometimes the $d$ (for \textit{distribution}) is replaced by the $\mathcal{L}$ (for \textit{law}).
    \end{notation}

\section{Moments}
\subsection{Expectation value}

    \newdef{Expectation value}{\index{expectation}\label{prob:expectation_value}
        Let $X$ be random variable defined on a probability space $(\Omega,\Sigma,P)$.
        \begin{gather}
            E[X] := \int_\Omega XdP
        \end{gather}
    }
    \begin{notation}
        Other notations which are often used in the literature are $\langle X \rangle$ and $\mu_X$.
    \end{notation}

    \newdef{Moment of order \texorpdfstring{$r$}\ }{\index{moment}\label{prob:moment}
        The moment of order $r$ is defined as the expectation value of the $r^{th}$ power of $X$ and by equation \ref{prob:omega_int_to_real_int} this becomes
        \begin{gather}
            E[X^r] = \int x^rf_X(x)dx.
        \end{gather}
    }
    \newdef{Central moment of order \texorpdfstring{$r$}\ }{\index{central!moment}\label{prob:central_moment}
        \begin{gather}
            E[(X-\mu)^r] = \int(x-\mu)^rf_X(x)dx
        \end{gather}
    }
    \begin{remark}
        Moments of order $n$ are determined by central moments of order $k\leq n$ and central moments of order $n$ are determined by moments of order $k\leq n$.
    \end{remark}
    \newdef{Variance}{\index{variance}
        The central moment of order 2 is called the variance:
        \begin{gather}
            V[X] := E[(X-\mu)^2].
        \end{gather}
    }
    \newdef{Standard deviation}{\index{standard!deviation}
        \begin{gather}
            \sigma_X := \sqrt{V[X]}
        \end{gather}
    }

    \begin{property}
        If $E[|X|^n]$ is finite for $n>0$ then for all $k\leq n$: $E[X^k]$ exist and are also finite.
    \end{property}

    \newdef{Moment generating function}{\index{moment!generating function}\label{statistics:moment_generating_function}
        \begin{gather}
            M_X(t) := E[e^{tX}] = \int_{-\infty}^{\infty}e^{tX}P(X)dX
        \end{gather}
    }
    \begin{property}
        If the moment generating function exists we can express the moments $E[X^n]$ in terms of $M_X$ (using the series expansion of the exponential function):
        \begin{gather}
            \label{statistics:theorem:moment_generating_function}
            E[X^n] = \left.\mderiv{n}{M_X(t)}{t}\right|_{t=0}.
        \end{gather}
    \end{property}

    \newdef{Characteristic function}{\index{characteristic!function}\label{prob:characteristic_function}
        Let $X$ be a random variable. The characteristic function of $X$ is defined as follows:
        \begin{gather}
            \varphi_X(t) := E\left[e^{itX}\right].
        \end{gather}
    }
    \begin{property}\label{statistics:characteristic_function_properties}
        The characteristic function has the following properties:
        \begin{itemize}
            \item $\varphi_X(0) = 1$
            \item $|\varphi_X(t)| \leq 1$
            \item $\varphi_{aX+b}(t) = e^{itb}\varphi_X(at)$
        \end{itemize}
    \end{property}

    \begin{formula}
        If $\varphi_X(t)$ is $k$ times continuously differentiable then $X$ has a finite $k^{th}$ moment and
        \begin{gather}
            \label{prob:characteristic_function_as_moment_generator}
            E[X^k] = \stylefrac{1}{i^k}\mderiv{k}{}{t}\varphi_X(0).
        \end{gather}
        Conversely, if $X$ has a finite $k^{th}$ moment then $\varphi_X(t)$ is $k$ times continuously differentiable and the above formula holds.
    \end{formula}

    \newformula{Inversion formula}{\index{inversion!formula}
        Let $X$ be a random variable. If the CDF of $X$ is continuous at $a, b\in\mathbb{R}$ then
        \begin{gather}
            \label{prob:inversion_formula}
            F_X(b) - F_X(a) = \lim_{c\rightarrow+\infty}\stylefrac{1}{2\pi}\int_{-c}^c\stylefrac{e^{-ita} - e^{-itb}}{it}\varphi_X(t)dt.
        \end{gather}
    }
    \begin{formula}
        If $\varphi_X(t)$ is integrable then the CDF is given by:
        \begin{gather}
            f_X(x) = \stylefrac{1}{2\pi}\int_{-\infty}^{+\infty}e^{-itx}\varphi_X(t)dt.
        \end{gather}
    \end{formula}
    \remark{From previous formula it is clear that the density function and the characteristic function are Fourier transformed quantities.}

\subsection{Correlation}

    \begin{property}\index{independence}\label{prob:independence_expectation_values}
        Let $X, Y$ be two random variables. They are independent if and only if $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$ holds for all Borel measurable\footnote{See definition \ref{lebesgue:borel_measurable_function}.} bounded functions $f,g$.
    \end{property}

    The value $E[XY]$ is equal to the inner product $\langle X|Y \rangle$ as defined in \ref{lebesgue:L2_inner_product}. It follows that independence of random variables implies orthogonality. To generalize this concept, we introduce following notions.

    \newdef{Centred random variable}{\index{random variable}
        Let $X$ be a random variable with finite expectation value $E[X]$. The centred random variable $X_c$ is defined as $X_c = X - E[X]$.
    }
    \newdef{Covariance}{\index{covariance}
        Let $X,Y$ be two random variables. The covariance of $X$ and $Y$ is defined as follows:
        \begin{gather}
            \label{prob:covariance}
            \text{cov}(X,Y) := \langle X_c|Y_c \rangle = E\big[(X-E[X])(Y-E[Y])\big].
        \end{gather}
        Some basic math gives
        \begin{gather}
            \text{cov}(X,Y) = E[XY] - E[X]E[Y].
        \end{gather}
    }
    \newdef{Correlation}{\index{correlation}\label{prob:correlation}
        Let $X,Y$ be two random variables. The correlation is defined as the cosine of the angle between $X_c$ and $Y_c$:
        \begin{gather}
            \rho_{XY} := \stylefrac{\text{cov}(X,Y)}{\sigma_X\sigma_Y}.
        \end{gather}
    }
    \result{From theorem \ref{prob:independence_expectation_values} it follows that independent random variables are also uncorrelated.}
    \result{If the random variables $X$ and $Y$ are uncorrelated then they satisfy $E[XY] = E[X]E[Y]$.}

    \begin{property}
        Let $\seq{X}$ be a sequence of independent random variables. Their variances satisfy the following equation:
        \begin{gather}
            \label{prob:variance_of_sum}
            V\left[\sum_{i=1}^{+\infty}X_i\right] = \sum_{i=1}^{+\infty}V[X_i].
        \end{gather}
    \end{property}

\subsection{Conditional expectation}

    Let $(\Omega,\Sigma,P)$ be a probability space. Consider a random variable $X\in L^2(\Omega,\Sigma,P)$ and a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. We know that the spaces $L^2(\Sigma)$ and $L^2(\mathcal{G})$ are complete\footnote{See property \ref{lebesgue:L2_hilbert_space}.} and hence the projection theorem \ref{linalgebra:theorem:projection_theorem} can be applied: for every $X\in L^2(\Sigma)$ there exists a random variable $Y\in L^2(\mathcal{G})$ such that $X-Y$ is orthogonal to $L^2(\mathcal{G})$. This has the following result:
    \begin{gather}
        \forall Z\in L^2(\mathcal{G}):\langle X-Y|Z \rangle = \int_\Omega(X-Y)ZdP = 0.
    \end{gather}
    Since $\mathbbm{1}_G\in L^2(\mathcal{G})$ for every $G\in\mathcal{G}$ we find by applying \ref{lebesgue:interchanging_domains_with_indicator_function}:
    \begin{gather}
        \label{prob:conditional_expectation_condition}
        \int_G XdP = \int_G YdP
    \end{gather}
    for all $G\in\mathcal{G}$. This leads us to introducing the following notion of conditional expectations:
    \newdef{Conditional expectation}{\index{expectation!conditional}\label{prob:conditional_expectation}
        Let $(\Omega,\Sigma,P)$ be a probability space and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\Sigma$. For every $\Sigma$-measurable random variable $X\in L^2(\Sigma)$ there exists a unique (up to a null set) random variable $Y\in L^2(\mathcal{G})$ that satisfies equation \ref{prob:conditional_expectation_condition} for every $G\in\mathcal{G}$. This variable $Y$ is called the conditional expectation of $X$ given $\mathcal{G}$ and it is denoted by $E[X|\mathcal{G}]$:
        \begin{gather}
            \int_GE[X|\mathcal{G}]dP = \int_GXdP.
        \end{gather}
    }
    \remark{Although this construction was based on orthogonal projections (this benefits the intuition) we could as well have used the (signed) Radon-Nikodym theorem \ref{lebesgue:signed_radon_nikodym} since $G\mapsto\int_GXdP$ is absolutely continuous.}

    \begin{property}\label{prob:conditional_expectation_props}
        Let $(\Omega, \Sigma, P)$ be a probability space and consider a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. If the random variable $X$ is $\mathcal{G}$-measurable then
        \begin{gather}
            E[X|\mathcal{G}] = X.
        \end{gather}
        On the other hand, if $X$ is independent of $\mathcal{G}$, then
        \begin{gather}
            E[X|\mathcal{G}] = E[X].
        \end{gather}
    \end{property}

\section{Joint distributions}

    \newdef{Joint distribution}{\index{joint distribution}
        Let $X, Y$ be two random variables defined on the same probability space $(\Omega,\Sigma,P)$. Consider the vector $(X, Y):\Omega\rightarrow\mathbb{R}^2$. The distribution of $(X, Y)$ is defined on the Borel sets of the plane $\mathbb{R}^2$ and it is given by the following measure:
        \begin{gather}
            P_{(X, Y)}(B) = P((X, Y)\in B)
        \end{gather}
    }
    \newdef{Joint density}{
        If the probability measure from previous definition can be written as
        \begin{gather}
            P_{(X, Y)}(B) = \int_Bf_{(X, Y)}(x, y)dm_2(x, y)
        \end{gather}
        for some integrable $f_{(X, Y)}$ it is said that $X$ and $Y$ have a joint density.
    }

    \newdef{Marginal distribution}{\index{marginal distribution}
        The distributions of one-dimensional random variables is determined by the joint distribution:
        \begin{gather}
            P_X(A) = P_{(X, Y)}(A\times\mathbb{R})\\
            P_Y(A) = P_{(X, Y)}(\mathbb{R}\times A)
        \end{gather}
    }

    \begin{result}
        If the joint density exists then the marginal distributions are absolutely continuous and the associated density functions are given by
        \begin{gather}
            f_X(x) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dy\\
            f_Y(y) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dx
        \end{gather}
        The converse however is not always true. The one-dimensional distributions can be absolutely continuous without the existence of the joint density.
    \end{result}

    \begin{property}
        Let $X, Y$ be two random variables with joint distribution $P_{(X, Y)}$. $X$ and $Y$ are independent if and only if the joint distribution coincides with the product measure:
        \begin{gather}
            P_{(X, Y)} = P_X\times P_Y.
        \end{gather}
    \end{property}
    \remark{\label{prob:remark_independence}
        If $X$ and $Y$ are absolutely continuous then the previous theorem also applies with the densities instead of the distributions.
    }

\subsection{Conditional probability}

    \newformula{Conditional density}{\index{conditional density}
        Let $X, Y$ be two random variables with joint density $f_{(X, Y)}$. The conditional density of $Y$ given $X\in A$ is
        \begin{gather}
            \label{prob:conditional_distribution}
            h(y|X\in A) = \stylefrac{\int_A f_{(X, Y)}(x, y)dx}{\int_A f_X(x)dx}.
        \end{gather}
        For $X=\{a\}$ this equation fails as the denominator would become 0. However it is possible to avoid this problem by formally setting
        \begin{gather}
            \label{prob:formal_conditional}
            h(y|A=a) := \frac{f_{(X, Y)}(a, y)}{f_X(a)}
        \end{gather}
        where $f_X(a)\neq0$. This last condition is non-restrictive\marginpar{\dbend} because the probability of having a measurement $(X, Y)\in\{(x,y):f_X(x) = 0\}$ is 0. We can thus define the conditional probability of $Y$ given $X=a$ as follows:
        \begin{gather}
            P(Y\in B|X=a) = \int_B h(y|X=a)dy.
        \end{gather}
    }

    \newformula{Conditional expectation}{\index{expectation!conditional}
        \begin{gather}
            E[Y|X](\omega) = \int_\mathbb{R}yh(y|X(\omega))dy
        \end{gather}
        Let $\mathcal{F}_X$ denote the $\sigma$-algebra generated by the random variable $X$ as before. Using Fubini's theorem we can prove that for all sets $A\in\mathcal{F}_X$ the following equality, which should be compared with equation \ref{prob:conditional_expectation}, holds:
        \begin{gather}
            \int_AE[Y|X]dP = \int_AYdP.
        \end{gather}
        This implies that the conditional expectation $E[Y|X]$ on $\mathcal{F}_X$ coincides with definition \ref{prob:conditional_expectation}. Furthermore, applying property \ref{prob:conditional_expectation_props} to the case $\mathcal{G}=\mathcal{F}_X$ gives us the law of total expectation:
    }
    \begin{property}[Law of total expectation]
        \begin{gather}
            E[E[Y|X]] = E[Y]
        \end{gather}
    \end{property}

\section{Stochastic calculus}

    \newdef{Stochastic process}{\index{stochastic!process}
        A sequence of random variables $\seq{X}$.
    }

    \newdef{Filtered probability space}{\index{probability!space}
        Consider a probability space $(\Omega, \Sigma, P)$ together with a filtration\footnote{See definition \ref{set:filtration}.} of $\Sigma$, i.e. a collection of $\sigma$-algebras $\mathbb{F}=\seq{\mathbb{F}}$ such that $i\leq j\implies\mathbb{F}_i\subseteq\mathbb{F}_j$. The quadruple $(\Omega, \Sigma, \mathbb{F}, P)$ is called a filtered probability space.

        Often we require the filtration to be exhaustive and separated (where we replace $\emptyset$ by $\mathbb{F}_0=\{\emptyset, \Omega\}$ since any $\sigma$-algebra has to contain the total space).
    }

    \newdef{Adapted process}{\index{adapted!process}
        Consider a stochastic process $\seq{X}$ on a filtered probability space $(\Omega, \Sigma, \mathbb{F}, P)$ is said to be adapted to the filtration $\mathbb{F}$ is $X_n$ is $\mathbb{F}_n$-measurable for all $n\geq0$.
    }
    \newdef{Predictable process}{\index{predictable}
        A stochastic process $\seq{X}$ on a filtered probability space $(\Omega, \Sigma, \mathbb{F}, P)$ is said to be predictable if $X_{n+1}$ is $\mathbb{F}_n$-measurable for all $n\geq0$.
    }

    \newdef{Stopping time}{\index{stopping time}
        Consider a random variable $\tau$ on filtered probability space $(\Omega, \Sigma, \mathbb{F}, P)$ where the codomain of $\tau$ coincides with the index set of $\mathbb{F}$. This variable is called a stopping time for $\mathbb{F}$ if
        \begin{gather}
            \{\tau\leq t\}\in\mathbb{F}_t
        \end{gather}
        for all $t$. Hence the stopping time is some kind of time indicator that only depends on the knowledge of the process up to time $t$.
    }

\subsection{Martingales}

    \newdef{Martingale}{\index{martingale}
        Consider a filtered probability space $(\Omega, \Sigma, \mathbb{F}, P)$. A stochastic process $\seq{X}$ is said to be a martingale relative to $\mathbb{F}$ if it satisfies the following conditions:
        \begin{enumerate}
            \item $\seq{X}$ is adapted to $\mathbb{F}$.
            \item Each random variable $X_n$ is integrable, i.e. $X_n\in L^1(P)$ for all $n\geq0$.
            \item For all $n\geq0: E[X_{n+1}|\mathcal{F}_n]=X_n$.
        \end{enumerate}
        If the equality in the last condition is replace by the inequality $\leq$ (resp. $\geq$) then we call the stochastic process a \textbf{supermartingale} (resp. \textbf{submartingale}).
    }

    \begin{theorem}[Doob decomposition]\index{Doob}
        Any integrable adapted process $\seq{X}$ can be decomposed as $X_n=X_0+M_n+A_n$ where $\seq{M}$ is a martingale and $\seq{A}$ is a predictable process. These two processes are constructed iteratively as follows:
        \begin{align}
            A_0 = 0\qquad&\qquad M_0 = 0\\
            \Delta A_n = E[\Delta X_n|\mathbb{F}_{n-1}]\qquad&\qquad\Delta M_n = \Delta X_n - \Delta A_n
        \end{align}
        Furthermore, $\seq{X}$ is a submartingale if and only if $\seq{A}$ is (almost surely) increasing.
    \end{theorem}
    \begin{result}\index{variation!quadratic}
        If we consider the special case $X=Y^2$ for some martingale $Y$ then we can show the following property:
        \begin{gather}
            E[(\Delta Y_n)^2|\mathbb{F}_{n-1}] = \Delta A_n.
        \end{gather}
        The process $\seq{A}$ is often called the \textbf{quadratic variation process} of $\seq{X}$ and is denoted by $\seq{[X]}$.
    \end{result}

    \newdef{Discrete stochastic integral\footnotemark}{\index{integral!stochastic}\index{martingale!transform|see {integral, stochastic}}
        \footnotetext{Sometimes called the \textbf{martingale transform}.}
        Let $\seq{M}$ be a martingale on a filtered probability space $(\Omega, \Sigma, \mathbb{F}, P)$ and let $\seq{X}$ be a predictable stochastic process (with respect to $\mathbb{F}$). The (discrete) stochastic integral of $X$ with respect to $M$ is defined as follows:
        \begin{gather}
            (X\cdot M)_t(\omega) = \sum_{i=1}^tX(\omega)_i\Delta M_i(\omega)
        \end{gather}
        where $\omega\in\Omega$. For t$=0$ we take the convention $(X\cdot M)_0=0$.
    }
    \begin{property}
        If the process $\seq{X}$ is bounded then the stochastic integral itself defines a martingale.
    \end{property}

    \begin{property}[It\^o isometry]\index{It\^o!isometry}
        Consider a martingale $\seq{M}$ and a predictable process $\seq{X}$. Using the Doob decomposition theorem we can show the following equality:
        \begin{gather}
            E\left[\left(X\cdot M\right)_n^2\right] = E\left[(X^2\cdot[M])_n\right]
        \end{gather}
        for all $n\geq0$.
    \end{property}

    It is this property that will allow us to define integrals with respect to continuous martingales since although the martingales are not in general of bounded variation (and hence do not induce a well-defined Lebesgue-Stieltjes integral) their quadratic variations are, e.g. the Wiener process.

\section{Information theory}

    \newdef{Self-information}{\index{information}
        The self-information of an event $x$ described by a distribution $P$ is defined as follows:
        \begin{gather}
            I(x) := -\ln P(x).
        \end{gather}
        This definition is modeled on the following reasonable requirements:
        \begin{itemize}
            \item Events that are almost surely going to happen, i.e. events $x$ such that $P(x)=1$, contain only little information $I(x)=0$.\footnote{And by extension if $P(x)\approx1$ then also $I(x)\approx0$.}
            \item Events that are very rare contain a lot of information.
            \item Independent events contribute additively to the information.
        \end{itemize}
    }
    \newdef{Shannon entropy}{\index{entropy!Shannon}
        The amount of uncertainty in a distribution $P$ is characterized by its (Shannon) entropy
        \begin{gather}
            H(P) := E[I(X)].
        \end{gather}
    }

    \newdef{Kullback-Leibler divergence\footnotemark}{\index{Kullback-Leibler}\index{entropy!relative}
        \footnotetext{Sometimes called the \textbf{relative entropy}.}
        Let $p,q$ be two probability distributions. The Kullback-Leibler divergence of $p$ with respect to $q$ is defined as follows:
        \begin{gather}
            D(p||q) := \sum_ip_i\log\left(\frac{p_i}{q_i}\right)
        \end{gather}
        where the sum should be replaced by an integral if one works with continuous distributions. This quantity can be interpreted as the information gained when using the distribution $p$ instead of $q$.
    }

    \begin{property}[Gibbs' inequality]
        By noting that the logarithm is a concave function and applying Jensen's equality \ref{linalgebra:jensen_inequality} one can prove that the Kullback-Leibler divergence is nonnegative:
        \begin{gather}
            D(p||q)\geq0.
        \end{gather}
        Furthermore, the Kullback-Leibler divergence is zero if and only if $p$ and $q$ are equal almost everywhere.
    \end{property}

\section{Extreme value theory}

    \newdef{Conditional excess}{
        Consider a random variable $X$ with cumulative distribution $F$. The conditional probability that $X$ is larger than a given threshold is given by the conditional excess distribution:
        \begin{gather}
        F_u(y) = P(X-u\leq y|X>u) = \frac{F(u+y)-F(u)}{1-F(u)}.
        \end{gather}
    }

    \newdef{Extreme value distribution}{
        The cdf of the extreme value distribution is given by the following formula:
        \begin{gather}
        F(x; \xi) = \exp\left(-(1+x\xi)^{-1/\xi}\right).
        \end{gather}
        In the case that $\xi=0$ one can use the definition of the Euler number $e$ to rewrite the definition as \begin{gather}
        F(x;0)=\exp(-e^{-x}).
        \end{gather}
        The number $\xi$ is called the \textbf{extreme value index}.
    }

    \newdef{Maximum-domain of attraction}{
        The maximum-domain of attraction of a distribution function $H$ consist of all distribution functions $F$ for which there exist sequences $\{a_n>0\}$ and $\{b_n\}$ such that $F^n(a_nx+b_n)\longrightarrow H(x)$ when $n\longrightarrow\infty$.
    }

    \begin{theorem}[Fischer, Tippett \& Gnedenko]
        Consider a sequence of i.i.d. random variables with distribution function $F$. If $F$ lies in the domain of attraction of $G$ then $G$ has the form of an extreme value distribution.
    \end{theorem}

    \begin{theorem}[Pickands, Balkema \& de Haan]
        Consider a sequence of i.i.d. random variables with conditional excess distribution $F_u$. If the cumulative distribution function $F$ lies in the domain of attraction of the extreme value distribution then the conditional excess distribution $F_u$ converges to the generalised Pareto distribution when $u\longrightarrow\infty$.
    \end{theorem}

\section{Copulas}

    \begin{property}
        Consider a continuous random variable $X$. Let $U$ be the result of the probability integral transformation, i.e. $U = F_X(X)$. This transformed random variable has a uniform cumulative distribution, i.e. $F_U(u) = u$.
    \end{property}

    \newdef{Copula}{\index{copula}
        The joint cumulative distribution function of a random variable with uniform marginal distributions.
    }

    The following alternative definition is more analytic in nature:
    \newadef{Copula}{
        A function $C: [0,1]^d\rightarrow[0,1]$ satisfying the following properties:
        \begin{enumerate}
            \item $C(x_1, \ldots, x_d)=0$ if any one of the $x_i$ is zero.
            \item Uniformity: $C(1, 1, \ldots, x_i, 1, \ldots)=x_i$ for all $1\leq i\leq d$.
            \item $d$-nondecreasing: For every box $B=\prod_{1\leq i\leq d}[a_i, b_i]\subseteq[0,1]^d$ the $C$-volume is nonnegative:
            \begin{gather}
            \int_B dC(x) = \sum_{\mathbf{z}\in\prod_i\{a_i,b_i\}}(-1)^{N_b(\mathbf{z})}C(\mathbf{z})\geq0
            \end{gather}
            where $N_B(\mathbf{z}) = \text{Card}(\{i:a_i=z_i\})$.
        \end{enumerate}
    }

    \begin{theorem}[Sklar]
        For every joint cumulative distribution function $H$ with marginal distributions $F_i$ there exists a unique copula $C$ such that
        \begin{gather}
        H(x_1, \ldots, x_d) = C(F_1(x_1), \ldots, F_d(x_d)).
        \end{gather}
    \end{theorem}

    \begin{property}[Fr\'echet-Hoeffding bounds]
        Every copula $C:[0,1]^d\rightarrow[0,1]$ is bounded in the following way:
        \begin{gather}
        \max\left(\sum_{i=1}^du_i-d+1, 0\right)\leq C(u_1, \ldots, u_d)\leq \min_iu_i
        \end{gather}
        for all $(u_1, \ldots, u_d)\in[0,1]^d$. Furthermore, the upper bound is sharp, i.e. $\min_iu_i$ is itself a copula.\footnote{The lower bound is only a copula for $d=2$. In general this bound is only pointwise sharp.}
    \end{property}

    \newdef{Extreme value copula}{
        A copula $C$ for which there exists a copula $\tilde{C}$ such that
        \begin{gather}
        \tilde{C}(u_1^{1/n}, \ldots, u_d^{1/n})^n\longrightarrow C(u_1, .., u_d)
        \end{gather}
        when $n\rightarrow\infty$ for all $(u_1, \ldots, u_d)\in[0,1]^d$.
    }
    \begin{property}
        A copula $C$ is an extreme value copula if and only if it is stable in the following sense:
        \begin{gather}
        C(u_1, \ldots, u_d) = C(u_1^{1/n}, \ldots, u_d^{1/n})^n
        \end{gather}
        for all $n\geq1$.
    \end{property}
\chapter{Probability}\label{chapter:probability}   

\section{Probability}

	\newdef{Axioms of probability}{\index{probability}The following list of axioms (defined by Kolmogorov) states when a measure space $(\Omega, \Sigma, P)$ defines a space that supports probability theory:
	    	\begin{enumerate}
			\item $P(E) \geq 0$
        		\item $P\left(\bigcup_{i\in I}E_i\right)=\sum_{i\in I}P(E_i)$ if all $E_i$ are mutually exclusive.\footnote{Some people require $I$ to be finite. However, this is not necessary if we use $\sigma$-algebras (as we always do for measure spaces).}
		        \item $P(\Omega) = 1$
		\end{enumerate}
	}
	\begin{remark}
    		The second axiom is exactly the same as saying that the probability $P$ should be a $\sigma$-additive function. Together with the first axiom and the consequence that $P(\emptyset) = 0$ this means that the probability is a measure \ref{lebesgue:measure}.
	\end{remark}

	\newdef{Probability space}{\index{probability!space}
	    	Let $(\Omega,\Sigma,P)$ be a measure space\footnote{See definition \ref{lebesgue:measure_space}.}. This measure space is called a probability space if $P(X)=1$. Furthermore, the measure $P$ is called a probability measure or simply probability.
	}

	\newdef{Random variable}{\index{random variable}
	    	Let $(\Omega,\Sigma,P)$ be a probability space. A function $X:\Omega\rightarrow\mathbb{R}$ is called a random variable if $\forall a\in\mathbb{R}:X^{-1}\big([a,+\infty[\big)\in\Sigma$.\footnote{$X^{-1}\big([a,+\infty[\big) = \{\omega\in\Omega:X(\omega)\geq a\}$.}
	}

	\newdef{$\sigma$-algebra of a random variable}{\index{$\sigma$-algebra}
	    	Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following family of sets is a $\sigma$-algebra:
	        \begin{gather}
			\label{prob:sigma_algebra_generated_random_variable}
		        X^{-1}(\mathcal{B}) = \{S\in\Sigma : S = X^{-1}(B\in\mathcal{B})\}
		\end{gather}
	}
	\begin{notation}
		The $\sigma$-algebra generated by the random variable $X$ is often denoted by $\mathcal{F}_X$, analogous to notation \ref{set:notation:generated_sigma_algebra}.
	\end{notation}

	\newdef{Sample space}{\index{sample space}
	    	Let $X$ be a random variable. The set of all possible outcomes of $X$ is called the sample space. The sample space is often denoted by $\Omega$.
	}

	\newdef{Event}{\index{event}
	    	Let $(\Omega,\Sigma,P)$ be a probability space. An element $S$ of the $\sigma$-algebra $\Sigma$ is called an event.
	}

	\begin{remark*}
		From the definition of an event it is clear that a single possible outcome of a measurement can be a part of multiple events. So although only one outcome can occur at the same time, multiple events can occur simultaneously.
	\end{remark*}
	\begin{remark*}
		When working with measure-theoretic probability spaces it is more convenient to use the $\sigma$-algebra (see \ref{set:sigma_algebra}) of events instead of the power set (see \ref{set:power_set}) of all events. Intuitively this seems to mean that some possible outcomes are not treated as events. However we can make sure that the $\sigma$-algebra still contains all ''useful'' events by using a ''nice'' definition of the used probability space. Further information concerning probability spaces can be found in chapter \ref{chapter:lebesgue}.
	\end{remark*}

	\begin{formula}[Union]\label{prob:union}
		Let $A, B$ be two events. The probability that at least one of them occurs is given by the following formula:
	        \begin{gather}
        		P(A\cup B) = P(A) + P(B) + P(A\cap B).
		\end{gather}
	\end{formula}

	\newdef{Disjoint events}{
	    	Two events $A$ and $B$ are said to be disjoint if they cannot happen at the same time:
	        \begin{gather}
	        	P(A\cap B) = 0.
	        \end{gather}
	}
	\result{If $A$ and $B$ are disjoint, the probability that both $A$ and $B$ occur is just the sum of their individual probabilities.}

	\newformula{Complement}{\index{complement}
	    	Let $A$ be an event. The probability of $A$ being false is denoted as $P\left(\overline{A}\right)$ and is given by
	        \begin{gather}
			\label{prob:complement}
		        P\left(\overline{A}\right) = 1 - P(A).
		\end{gather}
	}
	\begin{result}
		From the previous equation and de Morgan's laws (equations \ref{set:de_morgan_union} and \ref{set:de_morgan_intersection}) we derive the following formula\footnote{Switching the union and intersection has no impact on the validity of the formula.}:
	    	\begin{gather}
			P\left(\overline{A}\cap\overline{B}\right) = 1 - P(A\cup B).
		\end{gather}
    	\end{result}

\section{Conditional probability}

	\newdef{Conditional probability}{\index{probability!conditional}
	    	Let $A, B$ be two events. The probability of $A$ given that $B$ is true is denoted as $P(A|B)$:
	        \begin{gather}
	        	\label{prob:conditional_probability}
		        P(A|B) = \stylefrac{P(A\cap B)}{P(B)}
		\end{gather}
	}
	By interchanging $A$ and $B$ in previous equation and by observing that this has no effect on the quantity $P(A\cap B)$ the following important result can be deduced:
	\begin{theorem}[Bayes]\index{Bayes}\label{prob:theorem:bayes}
		Let $A, B$ be two events. From the definition conditional probability \ref{prob:conditional_probability} it is possible to derive the following important statement:
		\begin{gather}
		        P(A|B) = \stylefrac{P(B|A)P(A)}{P(B)}.
		\end{gather}
	\end{theorem}

	\begin{formula}
		Let $(B_i)_{i\in\mathbb{N}}$ be a sequence of pairwise disjoint events. If $\bigsqcup_{i=1}^{+\infty}B_i = \Omega$ then the total probability of a given event $A$ can be calculated as follows:
	        \begin{gather}
			\label{probability:total_probability_conditional}
		        P(A) = \sum_{i=1}^{+\infty}P(A|B_i)P(B_i)
		\end{gather}
	\end{formula}

	\newdef{Independent events}{\index{independence}
	    	Let $A, B$ be two events. $A$ and $B$ are said to be independent if they satisfy the following relation:
	        \begin{gather}
			P(A\cap B) = P(A)P(B).
		\end{gather}
	}
	\begin{result}
		If $A$ and $B$ are two independent events then Bayes' theorem simplifies to
	    	\begin{gather}
			P(A|B) = P(A).
		\end{gather}
	\end{result}
	The above definition can be generalized to multiple events:
	\begin{definition}
		The events $A_1,...,A_n$ are independent if for each choice of $k$ events the probability of their intersection is equal to the product of their individual probabilities.
	\end{definition}
	This definition can be stated in terms of $\sigma$-algebras:
	\begin{definition}
		The $\sigma$-algebras $\mathcal{F}_1,...,\mathcal{F}_n$ defined on a probability space $(\Omega,\mathcal{F},P)$ are independent if for all choices of distinct indices $i_1,...,i_k$ and for all choices of sets $F_{i_n}\in\mathcal{F}_{i_n}$ the following equation holds:
	        \begin{gather}
	        	\label{prob:independent_sigma_algebras}
			P(F_{i_1}\cap...\cap F_{i_k}) = P(F_{i_1})...P(F_{i_k})
		\end{gather}
	\end{definition}
	\begin{result}
		Let $X,Y$ be two random variables. $X$ and $Y$ are independent if the $\sigma$-algebras generated by them are independent.
	\end{result}

\section{Probability distribution}

	\newdef{Probability distribution}{\index{probability!distribution}\label{prob:probability_distribution}
    		Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. The following function is a measure on the Borel $\sigma$-algebra:
        	\begin{gather}
			\boxed{P_X(B) = P(X^{-1}(B))}
		\end{gather}
		This measure is called the probability distribution of $X$.
	}

	\begin{formula}[Change of variables]
		Let $X$ be a random variable defined on a probability space $(\Omega,\Sigma,P)$. By substituting $\omega=X^{-1}(B)$ and applying previous definition we obtain
	        \begin{gather}
			\label{prob:change_of_variable}
		        \int_\Omega g(X(\omega))dP(\omega) = \int_\mathbb{R}g(x)dP_X(x)
		\end{gather}
	\end{formula}
    
	\newdef{Density}{\index{density}
	    	Let $f$ be a non-negative integrable function and recall theorem \ref{lebesgue:theorem:measure_by_integral}. The function $f$ is called the \textbf{density} of the measure $P:=\int_{-}fdm$ with respect to the Lebesgue measure $m$.
    	
	        For $P$ to be a probability, $f$ should satisfy the following condition:
	        \begin{gather}
			\int fdx = 1
		\end{gather}
	}
	\newdef{Cumulative distribution function}{\index{cumulative distribution function}
	    	Let $f$ be a density. The cumulative distribution function (cdf) corresponding to $f$ is defined by the following formula:
	        \begin{gather}
			\label{prob:cdf}
		        F(y) = \int_{-\infty}^yf(x)dx.
		\end{gather}
	}

	\begin{theorem}[Skorokhod's representation theorem]\index{Skorokhod}
		Let $F:\mathbb{R}\rightarrow[0,1]$ be a function that satisfies the following 3 properties:
	        \begin{itemize}
			\item $F(x)$ is non-decreasing.
		        \item $\ds\lim_{x\rightarrow-\infty}F(x) = 0$ and $\ds\lim_{x\rightarrow+\infty}F(x) = 1$
	        	\item $F(x)$ is right-continuous: $y\rightarrow^+ y_0\implies F(y)\rightarrow F(y_0)$.
		\end{itemize}
	        There exists a random variable $X:[0,1]\rightarrow\mathbb{R}$ defined on the probability space $([0,1],\mathcal{B},m_{[0,1]})$ such that $F = F_X$.
	\end{theorem}

	The Radon-Nikodym theorem\footnote{See section \ref{lebesgue:section:Radon-Nikodym}.} implies the following formula:
	\begin{formula}
		Consider an absolutely continuous probability function $P_X$ defined on the product space $\mathbb{R}^n$. Let $f_X$ be the density associated with $P_X$ and let $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be integrable with respect to $P_X$.
	        \begin{gather}
			\int_{\mathbb{R}^n}g(x)dP_X(x) = \int_{\mathbb{R}^n}f_X(x)g(x)dx
		\end{gather}
	\end{formula}
	\begin{result}
	    	Previous formula together with formula \ref{prob:change_of_variable} gives rise to
    		\begin{gather}
        		\label{prob:omega_int_to_real_int}
			\int_\Omega g(X)dP = \int_{\mathbb{R}^n}f_X(x)g(x)dx.
		\end{gather}
	\end{result}

\section{Moments}
\subsection{Expectation value}

	\newdef{Expectation value}{\index{expectation}\label{prob:expectation_value}
    		Let $X$ be random variable defined on a probability space $(\Omega,\Sigma,P)$.
	    	\begin{gather}
			E[X] := \int_\Omega XdP
		\end{gather}
	}
	\begin{notation}
		Other notations which are often used in the literature are $\langle X \rangle$ and $\mu_X$.
	\end{notation}

	\newdef{Moment of order \texorpdfstring{$r$}\ }{\index{moment}\label{prob:moment}
    		The moment of order $r$ is defined as the expectation value of the $r^{th}$ power of $X$ and by equation \ref{prob:omega_int_to_real_int} this becomes:
	        \begin{gather}
			E[X^r] = \int x^rf_X(x)dx
		\end{gather}
	}
	\newdef{Central moment of order \texorpdfstring{$r$}\ }{\index{central!moment}
    		\begin{gather}
        		\label{prob:central_moment}
			E[(X-\mu)^r] = \int(x-\mu)^rf_X(x)dx
		\end{gather}
	}
	\begin{remark}
		Moments of order $n$ are determined by central moments of order $k\leq n$ and central moments of order $n$ are determined by moments of order $k\leq n$.
	\end{remark}
	\newdef{Variance}{\index{variance}
    		The central moment of order 2 is called the variance:
    		\begin{gather}
    			V[X] = E[(X-\mu)^2].
		\end{gather}
	}

	\begin{property}
		If $E[|X|^n]$ is finite for $n>0$ then for all $k\leq n$: $E[X^k]$ exist and are also finite.
	\end{property}

	\newdef{Moment generating function}{\index{moment!generating function}\label{statistics:moment_generating_function}
		\begin{gather}
		        M_X(t) := E[e^{tX}] = \int_{-\infty}^{\infty}e^{tX}P(X)dX
		\end{gather}
	}
	\begin{property}
		If the moment generating function exists we can express the moments $E[X^n]$ in terms of $M_X$ (using the series expansion of the exponential function):
        	\begin{gather}
			\label{statistics:theorem:moment_generating_function}
        		E[X^n] = \left.\mderiv{n}{M_X(t)}{t}\right|_{t=0}
		\end{gather}
	\end{property}

	\newdef{Characteristic function}{\index{characteristic!function}\label{prob:characteristic_function}
    		Let $X$ be a random variable. The characteristic function of $X$ is defined as follows:
        	\begin{gather}
        		\varphi_X(t) := E[e^{itX}]
		\end{gather}
	}
	\begin{property}\label{statistics:characteristic_function_properties}
		The characteristic function has the following properties:
	        \begin{itemize}
        		\item $\varphi_X(0) = 1$
        		\item $|\varphi_X(t)| \leq 1$
		        \item $\varphi_{aX+b}(t) = e^{itb}\varphi_X(at)$
		\end{itemize}
	\end{property}

	\begin{formula}
    		If $\varphi_X(t)$ is $k$ times continuously differentiable then $X$ has finite $k^{th}$ moment and
	        \begin{gather}
        		\label{prob:characteristic_function_as_moment_generator}
			E[X^k] = \stylefrac{1}{i^k}\mderiv{k}{}{t}\varphi_X(0).
		\end{gather}
	        Conversely, if $X$ has finite $k^{th}$ moment then $\varphi_X(t)$ is $k$ times continuously differentiable and the above formula holds.
	\end{formula}

	\newformula{Inversion formula}{\index{inversion!formula}
    		Let $X$ be a random variable. If the cdf of $X$ is continuous at $a, b\in\mathbb{R}$ then
	        \begin{gather}
			\label{prob:inversion_formula}
			F_X(b) - F_X(a) = \lim_{c\rightarrow+\infty}\stylefrac{1}{2\pi}\int_{-c}^c\stylefrac{e^{-ita} - e^{-itb}}{it}\varphi_X(t)dt.
		\end{gather}
	}
	\begin{formula}
    		If $\varphi_X(t)$ is integrable then the cdf is given by:
		\begin{gather}
			f_X(x) = \stylefrac{1}{2\pi}\int_{-\infty}^{+\infty}e^{-itx}\varphi_X(t)dt.
		\end{gather}
	\end{formula}
	\remark{From previous formula it is clear that the density function and the characteristic function are Fourier transformed quantities.}

\subsection{Correlation}

	\begin{property}\index{independence}\label{prob:independence_expectation_values}
		Let $X, Y$ be two random variables. They are independent if and only if $E(f(X)g(Y)) = E(f(X))E(g(Y))$ holds for all Borel measurable\footnote{See definition \ref{lebesgue:borel_measurable_function}.} bounded functions $f,g$.
	\end{property}

	The value $E(XY)$ is equal to the inner product $\langle X|Y \rangle$ as defined in \ref{lebesgue:L2_inner_product}. It follows that independence of random variables implies orthogonality. To generalize this concept, we introduce following notions.

	\newdef{Centred random variable}{\index{random variable}
    		Let $X$ be a random variable with finite expectation value $E(X)$. The centred random variable $X_c$ is defined as $X_c = X - E(X)$.
	}
	\newdef{Covariance}{\index{covariance}
    		Let $X,Y$ be two random variables. The covariance of $X$ and $Y$ is defined as follows:
	        \begin{gather}
        		\label{prob:covariance}
			\text{cov}(X,Y) = \langle X_c|Y_c \rangle = E\big((X-E(X))(Y-E(Y))\big)
		\end{gather}
	        Some basic math gives:
        	\begin{gather}
			\text{cov}(X,Y) = E(XY) - E(X)E(Y)
		\end{gather}
	}
	\newdef{Correlation}{\index{correlation}
    		Let $X,Y$ be two random variables. The correlation is defined as the cosine of the angle between $X_c$ and $Y_c$:
	        \begin{gather}
			\label{prob:correlation}
			\rho_{XY} = \stylefrac{\text{cov}(X,Y)}{||X||_2||Y||_2}
		\end{gather}
	}
	\result{From theorem \ref{prob:independence_expectation_values} it follows that independent random variables are also uncorrelated.}
	\result{Uncorrelated $X$ and $Y$ satisfy the following equality: $E(XY) = E(X)E(Y)$.}

	\begin{property}
		Let $(X_i)_{i\in\mathbb{N}}$ be a sequence of independent random variables. Their variances satisfy the following equation:
        	\begin{gather}
			\label{prob:variance_of_sum}
			V\left(\sum_{i=1}^{+\infty}X_i\right) = \sum_{i=1}^{+\infty}V(X_i)
		\end{gather}
	\end{property}

\subsection{Conditional expectation}

	Let $(\Omega,\Sigma,P)$ be a probability space. Consider a random variable $X\in L^2(\Omega,\Sigma,P)$ and a sub-$\sigma$-algebra $\mathcal{G}\subset\Sigma$. We know that the spaces $L^2(\Sigma)$ and $L^2(\mathcal{G})$ are complete\footnote{See property \ref{lebesgue:L2_hilbert_space}.} and hence the projection theorem \ref{linalgebra:theorem:projection_theorem} can be applied: for every $X\in L^2(\Sigma)$ there exists a random variable $Y\in L^2(\mathcal{G})$ such that $X-Y$ is orthogonal to $L^2(\mathcal{G})$. This has the following result:
	\begin{gather}
		\forall Z\in L^2(\mathcal{G}):\langle X-Y|Z \rangle = \int_\Omega(X-Y)ZdP = 0
	\end{gather}
	Since $\mathbbm{1}_G\in L^2(\mathcal{G})$ for every $G\in\mathcal{G}$ we find by applying \ref{lebesgue:interchanging_domains_with_indicator_function}:
	\begin{gather}
	    	\label{prob:conditional_expectation_condition}
		\int_G XdP = \int_G YdP
	\end{gather}
	This leads us to introducing the following notion of conditional expectations:
	\newdef{Conditional expectation}{\index{expectation!conditional}
	    	Let $(\Omega,\Sigma,P)$ be a probability space and let $\mathcal{G}$ be a sub-$\sigma$-algebra of $\Sigma$. For every $\Sigma$-measurable random variable $X\in L^2(\Sigma)$ there exists a unique (up to a null set) random variable $Y\in L^2(\mathcal{G})$ that satisfies equation \ref{prob:conditional_expectation_condition} for every $G\in\mathcal{G}$. This $Y$ is called the conditional expectation of $X$ given $\mathcal{G}$ and it is denoted by $Y := E(X|\mathcal{G})$:
	        \begin{gather}
			\label{prob:conditional_expectation}
		        \boxed{\int_GE(X|\mathcal{G})dP = \int_GXdP}
		\end{gather}
	}
	\remark{Although our derivation was based on random variables from the $L^2$ class, it is also possible to construct (unique) conditional expectations for random variables from the $L^1$ class by using method \ref{lebesgue:method:linear_proofs}.}

\section{Joint distributions}

	\newdef{Joint distribution}{\index{joint distribution}
	    	Let $X, Y$ be two random variables defined on the same probability space $(\Omega,\Sigma,P)$. Consider the vector $(X, Y):\Omega\rightarrow\mathbb{R}^2$. The distribution of $(X, Y)$ is defined on the Borel sets of the plane $\mathbb{R}^2$ and it is given by the following measure:
	        \begin{gather}
	        	P_{(X, Y)}(B) = P((X, Y)\in B)
		\end{gather}
	}
	\newdef{Joint density}{
	    	If the probability measure from previous definition can be written as
	        \begin{gather}
			P_{(X, Y)}(B) = \int_Bf_{(X, Y)}(x, y)dm_2(x, y)
		\end{gather}
	        for some integrable $f_{(X, Y)}$ it is said that $X$ and $Y$ have a joint density.
	}

	\newdef{Marginal distribution}{\index{marginal distribution}
		The distributions of one-dimensional random variables is determined by the joint distribution:
	        \begin{gather}
			P_X(A) = P_{(X, Y)}(A\times\mathbb{R})\\
			P_Y(A) = P_{(X, Y)}(\mathbb{R}\times A)
		\end{gather}
	}

	\begin{result}
		If the joint density exists then the marginal distributions are absolutely continuous and given by
	        \begin{gather}
			f_X(x) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dy\\
			f_Y(y) = \int_{\mathbb{R}}f_{(X, Y)}(x, y)dx
		\end{gather}
	        The converse however is not always true. The one-dimensional densities can be absolutely continuous without the existence of the joint density.
	\end{result}

\subsection{Independence}\index{independence}

	\begin{theorem}
		Let $X, Y$ be two random variables with joint distribution $P_{(X, Y)}$. $X$ and $Y$ are independent if and only if the joint distribution coincides with the product measure:
	        \begin{gather}
	        	P_{(X, Y)} = P_X\times P_Y
	        \end{gather}
	\end{theorem}
	\remark{\label{prob:remark_independence}
	    	If $X$ and $Y$ are absolutely continuous then the previous theorem also applies with the densities instead of the distributions.
	}

\subsection{Conditional probability}

	\newformula{Conditional density}{\index{conditional density}
    		Let $X, Y$ be two random variables with joint density $f_{(X, Y)}$. The conditional density of $Y$ given $X\in A$ is:
        	\begin{gather}
			\label{prob:conditional_distribution}
        		h(y|X\in A) = \stylefrac{\int_A f_{(X, Y)}(x, y)dx}{\int_A f_X(x)dx}
		\end{gather}
        	For $X=\{a\}$ this equation fails as the denominator would become 0. However it is possible to avoid this problem by formally putting
        	\begin{gather}
        		\label{prob:formal_conditional}
        		h(y|A=a) = \frac{f_{(X, Y)}(a, y)}{f_X(a)}
		\end{gather}
        	with $f_X(a)\neq0$ which is non-restrictive\marginpar{\dbend} because the probability of having a measurement $(X, Y)\in\{(x,y):f_X(x) = 0\}$ is 0. We can thus define the conditional probability of $Y$ given $X=a$:
        	\begin{gather}
			P(Y\in B|X=a) = \int_B h(y|X=a)dy
		\end{gather}
	}

	\newformula{Conditional expectation}{\index{expectation!conditional}
	    	\begin{gather}
			E(Y|X)(\omega) = \int_\mathbb{R}yh(y|X(\omega))dy
		\end{gather}
	        Furthermore, let $\mathcal{F}_X$ denote the $\sigma$-algebra generated by the random variable $X$. Using Fubini's theorem we can prove that for all sets $A\in\mathcal{F}_X$ the following equality, which should be compared with equation \ref{prob:conditional_expectation}, holds:
	        \begin{gather}
			\int_AE(Y|X)dP = \int_AYdP
		\end{gather}
	}
	\remark{The previous two equations imply that the conditional expectation $E(Y|X)$ is the best representation of the random variable $Y$ as a function of $X$ (i.e. measurable with respect to $\mathcal{F}_X$).}

	\begin{property}[Law of total expectation]
	    	As mentioned above, applying Fubini's theorem gives:
	    	\begin{gather}
			\boxed{E(E(Y|X)) = E(Y)}
		\end{gather}
	\end{property}
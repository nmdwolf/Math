\chapter{Statistics}\label{chapter:statistics}

    In this chapter, most definitions and formulas will be based on either a standard calculus approach or a data-driven approach. For a measure-theoretic approach, see Chapter \ref{chapter:probability}. For some sections the language of information geometry will be used as introduced in the previous chapter \ref{chapter:info}.

\section{Data samples}
\subsection{Moment estimators}

    \newformula{$r^{th}$ sample  moment}{\index{moment}\label{statistics:sample_moment}
        \begin{gather}
            \overline{x^r} := \frac{1}{N}\sum_{i=1}^Nx_i^r
        \end{gather}
    }
    \begin{example}[Arithmetic mean]\index{mean}\label{statistics:arithmetic_mean}
        The arithmetic mean is used to average out differences between measurements. It is defined as the first sample moment:
        \begin{gather}
            \overline{x} := \frac{1}{N}\sum_{i=1}^Nx_i.
        \end{gather}
    \end{example}
    \newformula{$r^{th}$ central sample moment}{\label{statistics:central_sample_moment}
        \begin{gather}
            m_r := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^r
        \end{gather}
    }

    \newdef{Weighted mean}{\label{statistics:weighted_mean}
        Let $f:\mathbb{R}\rightarrow\mathbb{R}^+$ be a weight function. The weighted mean is given by:
        \begin{gather}
            \overline{x} := \frac{\sum_if(x_i)x_i}{\sum_if(x_i)}.
        \end{gather}
    }
    \begin{example}[Binned mean]\label{statistics:binned_arithmetic_mean}
        If the data has been grouped in bins, the weight function is given by the number of elements in each bin.
        \begin{gather}
            \overline{x} = \frac{1}{N}\sum_{i=1}n_ix_i.
        \end{gather}
    \end{example}
    \begin{remark}
        In the above definitions, the measurements $x_i$ can be replaced by function values $f(x_i)$ to calculate the mean of the function $f(x)$. This follows from Theorem \ref{prob:unconscious_statistician}. However, it is also important to keep in mind that $\overline{f(x)} \neq f(\overline{x})$. The equality only holds for linear functions.
    \end{remark}

    \newdef{Geometric mean}{\label{statistics:geometric_mean}
        Let $\{x_i\}$ be a data set taking values in either $\mathbb{R}_+$ or $\mathbb{R}_-$. The geometric mean is used to average out \textit{normalised} measurements, i.e. ratios with respect to a reference value.
        \begin{gather}
            g := \left(\prod_{i=1}^Nx_i\right)^{1/N}
        \end{gather}
        The following relation exists between the arithmetic and geometic mean:
        \begin{gather}
            \ln g = \overline{\ln x}.
        \end{gather}
    }

    \newdef{Harmonic mean}{\label{statistics:harmonic_mean}
        \begin{gather}
            h := \left(\frac{1}{N}\sum_{i=1}^Nx_i^{-1}\right)^{-1}
        \end{gather}
        The following relation exists between the arithmetic and harmonic mean:
        \begin{gather}
            \frac{1}{h} = \overline{x^{-1}}.
        \end{gather}
    }

    \begin{property}
        Let $\{x_i\}$ be a data set taking values in $\mathbb{R}_+$.
        \begin{gather}
            h\leq g\leq\overline{x}
        \end{gather}
        The equalities only hold when all $x_i$ are equal.
    \end{property}

    \newdef{Mode}{\index{mode}
        The most occurring value in a data set.
    }
    \newdef{Median}{\index{median}
        The element $x_i$ in a data set such that half of the values is greater than $x_i$ and half of the values is smaller than $x_i$.
    }

\subsection{Dispersion}

    \newdef{Range}{\index{range}
        The simplest indicator for statistical dispersion:
        \begin{gather}
            R := x_{\max} - x_{\min}.
        \end{gather}
        However, it is very sensitive for outliers.
    }

    \newdef{Mean absolute difference}{
        \begin{gather}
            \mathrm{MD} := \frac{1}{N}\sum_{i=1}^N|x_i - \overline{x}|
        \end{gather}
    }

    \newdef{Sample variance}{\index{variance}\label{statistics:sample_variance}
        \begin{gather}
            \mathrm{Var}(x) := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^2
        \end{gather}
    }
    \begin{formula}
        The variance can also be rewritten in the following way:
        \begin{gather}
            \label{statistics:variance_without_sum}
            \mathrm{Var}(x) = \overline{x^2} - \overline{x}^2.
        \end{gather}
    \end{formula}
    \begin{remark}[Bessel corection]\index{Bessel!correction}
        A better estimator for the variance of a sample is given by the following formula:
        \begin{gather}
            \label{statistics:bessel_correction}
            \hat{s} := \frac{1}{N-1}\sum_{i=1}^N(x_i - \overline{x})^2.
        \end{gather}
        See Remark \ref{statistics:variance_bessel_correction} for more information.
    \end{remark}

    \newdef{Skewness}{\index{skewness}\label{statistics:skewness}
        The skewness $\gamma$ describes the asymmetry of a distribution. It is defined as the proportionality constant relating the third central moment $m_3$ and the standard deviation $\sigma$:
        \begin{gather}
            m_3 = \gamma\sigma^3.
        \end{gather}
        A positive skewness indicates a tail to the right or alternatively a median smaller than $\overline{x}$. A negative skewness indicates a median larger than $\overline{x}$.
    }
    \newdef{Pearson's mode skewness}{\index{Pearson!skewness|see{skewness}}\label{statistics:pearsons_skewness}
        \begin{gather}
            \gamma_P := \frac{\overline{x} - \mathrm{mode}}{\sigma}
        \end{gather}
    }

    \newdef{Kurtosis}{\index{kurtosis}\label{statistics:kurtosis}
        The kurtosis $c$ is an indicator for the ``tailedness''. It is defined as the proportionality constant relating the fourth central moment $m_4$ and the standard deviation $\sigma$:
        \begin{gather}
            m_4 = c\sigma^4.
        \end{gather}
    }
    \newdef{Excess kurtosis}{
        The excess kurtosis is defined as $c-3$. This fixes the excess kurtosis of all univariate normal distributions at 0. A positive excess is an indicator for long ``fat'' tails, a negative excess indicates short ``thin'' tails.
    }

    \newdef{Percentile}{\index{percentile}
        The $p$-percentile $c_p$ is defined as the value that is larger than $p\%$ of the measurements. The median is the 50-percentile.
    }

    \newdef{Interquartile range}{
        The difference between the upper and lower quartile (75- and 25-percentiles respectively).
    }

    \newdef{Full Width at Half Maximum}{\index{FWHM}
        \nomenclature[A_FWHM]{FWHM}{full width at half maximum}
        The difference between the two values of the independent variable where the dependent variable is half of its maximum. This quantity is often denoted by the abbreviation \textbf{FWHM}.
    }
    \begin{property}
        For Gaussian distributions the following relation exists between the FWHM and the standard deviation $\sigma$:
        \begin{gather}
            \mathrm{FWHM} = 2.35\sigma.
        \end{gather}
    \end{property}

\subsection{Multivariate data sets}

    When working with bivariate (or even multivariate) distributions it is useful to describe the relationship between the different random variables.

    \newdef{Covariance}{\index{covariance}\label{statistics:covariance}
        The covariance of two data sequences is defined as follows:
        \begin{gather}
            \mathrm{cov}(x,y) := \frac{1}{N}\sum_{i=1}^N(x_i-\overline{x})(y_i - \overline{y}) = \overline{xy} - \overline{x}\ \overline{y}.
        \end{gather}
        The covariance is also often denoted by $\sigma_{xy}$.
    }
    \begin{formula}
        The covariance and standard deviation are related by the following equality:
        \begin{gather}
            \sigma_x^2 = \sigma_{xx}.
        \end{gather}
    \end{formula}

    \newdef{Correlation coefficent}{\index{correlation}\label{statistics:correlation_coefficient}
        \begin{gather}
            \rho_{xy} := \frac{\mathrm{cov}(x,y)}{\sigma_x\sigma_y}
        \end{gather}
        The correlation coefficient is bounded to the interval $[-1,1]$. It should be noted that its magnitude is only an indicator for the linear dependence.
    }
    \begin{remark}
        For multivariate distributions the above definitions can be generalized using matrices:
        \begin{align}
            \label{statistics:covariance_matrix}
            V_{ij} &= \mathrm{cov}(x_{(i)},x_{(j)})\\
            \label{statistics:correlation_matrix}
            \rho_{ij} &= \rho_{(i)(j)},
        \end{align}
        where $\mathrm{cov}(x_{(i)},x_{(j)})$ and $\rho_{(i)(j)}$ are defined as in Formulas \ref{statistics:covariance} and \ref{statistics:correlation_coefficient}.
    \end{remark}

\section{Probability distributions}

    In the following sections and subsections, all distributions will be taken to be continuous. The formulas can be generalized to discrete distributions by replacing the integral with a summation.

    \begin{definition}[Percentile]
        The $p$-percentile $c_p$ of a distribution $F$ is defined as:
        \begin{gather}
            c_p = F^{-1}(p).
        \end{gather}
    \end{definition}

    \newdef{Parametric family}{\index{parametric family}
        A family of probability densities indexed by one or more parameters $\theta$.
    }

    \begin{example}[Mixture family]\index{mixture}
        Consider a collection of distributions $\mathcal{P}=\{P_i\}_{i\leq n}$. The mixture family generated by $\mathcal{P}$ consist of all convex combintations of elements in $\mathcal{P}$:
        \begin{gather}
            \left\{\sum_{i=1}^nw_iP_i\,\middle\vert\,w_i\geq0,\sum_{i=1}^nw_i = 1\right\}.
        \end{gather}
        Every element of this family is called a \textbf{mixture distribution}.
    \end{example}

\subsection{Empirical distribution}

    \newdef{Empirical distribution function}{\index{distribution!empirical}\label{statistics:empirical_distribution}
        The (discrete) empirical probability distribution function is defined as the uniform mixture distribution with Dirac measures at the observations:
        \begin{gather}
            F_n := \frac{1}{n}\sum_{i=1}^n\delta_{x_i}.
        \end{gather}
    }

    \begin{theorem}[Borel's law of large numbers]\index{law of large numbers}\label{statistics:large_numbers}
        If the sample size approaches infinity, the observed frequencies approach the theoretical propabilities.
    \end{theorem}
    \begin{result}[Frequentist probability\footnotemark]
        \footnotetext{Also called the \textbf{empirical probability}.}
        \begin{gather}
            \label{statistics:frequentist_probability}
            \mathrm{Pr}(x) := \lim_{n\rightarrow\infty}\frac{f_n(x)}{n}
        \end{gather}
    \end{result}

    The law of large numbers can also be phrased in terms of the empirical distribution function:
    \begin{theorem}[Glivenko-Cantelli]\index{Glivenko-Cantelli}
        Consider a cumulative distribution function $F$ on a probability space $\Omega$. Denote the empirical distribution function of $n$ random variables on $\Omega$ by $F_n$. If the random variables are i.i.d. according to $F$, then
        \begin{gather}
            \sup_{x\in\Omega}|F(x)-F_n(x)|\overset{\text{a.s.}}{\longrightarrow}0.
        \end{gather}
    \end{theorem}
    \begin{remark}
        The law of the large numbers implies pointwise convergence of the empirical distribution function, while the Glivenko-Cantelli theorem strengthens this to uniform convergence.
    \end{remark}

    The quantity in the Glivenko-Cantelli theorem is important enough to get its own name:
    \newdef{Kolmogorov-Smirnov statistic}{\label{statistics:kolmogorov_smirnov_statistic}
        Let $F$ be a given cumulative distribution function. The $n^{th}$ Kolmogorov-Smirnov statistic is defined as follows:
        \begin{gather}
            D_n := \sup_x|F_n(x) - F(x)|.
        \end{gather}
    }

    \newdef{Kolmogorov distribution}{\index{distribution!Kolmogorov}
        \begin{gather}
            \label{statistics:kolmogorov_distribution_cumulative}
            F_\mathrm{Kol}(x) := 1 - 2\sum_{i=1}^\infty(-1)^{i-1}e^{-2i^2x^2} = \frac{\sqrt{2\pi}}{x}\sum_{i=1}^\infty e^{-(2i-1)^2\pi^2/(8x^2)}
        \end{gather}
    }

    \newprop{Kolmogorov-Smirnov test}{\index{Kolmogorov-Smirnov test}
        Let the null hypothesis $H_0$ state that a given data sample is described by a distribution function $F$. The null hypothesis is rejected at significance level $\alpha$ if
        \begin{gather}
            \sqrt{n}D_n > K_{\alpha},
        \end{gather}
        where $K_\alpha$ is defined by the Kolmogorov distribution: $F_\mathrm{Kol}(K_\alpha) = 1-\alpha$.
    }

\subsection{Common distributions}

    \newformula{Uniform distribution}{\index{distribution!uniform}\label{statistics:uniform_distr}
        \begin{align}
            f(x;a,b) &:=
            \begin{cases}
                \frac{1}{b-a}&a\leq x\leq b\\
                0&\text{elsewhere}
            \end{cases}\\\nonumber\\
            \expect{x} &= \frac{a+b}{2}\\\nonumber\\
            \variance{x} &= \frac{(b-a)^2}{12}
        \end{align}
    }

    \begin{formula}[Gaussian distribution]\index{distribution!normal}\index{Gauss!distribution}\label{statistics:normal_distr}
        \begin{gather}
            \mathcal{G}(x;\mu, \sigma) := \stylefrac{1}{\sqrt{2\pi}\sigma}e^{-\stylefrac{(x-\mu)^2}{2\sigma^2}}
        \end{gather}
        This distribution is also called a (univariate) \textbf{normal distribution}.
    \end{formula}

    \newformula{Standard normal distribution}{\index{error!function}\label{statistics:standard_normal_distr}
        \begin{gather}
            \mathcal{N}(z) := \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
        \end{gather}
        The cumulative distribution of $\mathcal{N}$ is given by the \textit{error function}.
    }
    \begin{remark}\index{standardization}
        Every Gaussian distribution can be transformed into a standard normal distribution by passing to the random variable $Z = \frac{X-\mu}{\sigma}$. This transformation is often called \textbf{standardization}.
    \end{remark}

    \begin{theorem}[Central limit theorem]\index{central limit theorem}\label{statistics:CLT}
        A sum of $n$ i.i.d. random variables $X_i$ distributed according to a distribution with mean $\mu$ and variance $\sigma^2$ satisfies the following property:
        \begin{gather}
            \sqrt{n}\left(\sum_{i=1}^nX_i - \mu\right)\overset{d}{\longrightarrow}\mathcal{N}(0,\sigma^2).
        \end{gather}
    \end{theorem}
    \begin{remark}
        If the random variables are not independent, property 2 will not be fulfilled. However, a generalization to distributions that are not identical exists. These are the \textit{Lyapunov} and \textit{Lindeberg} CLTs. (This generalization does require additional conditions on the higher moments.)
    \end{remark}
    \remark{The sum of Gaussians is always Gaussian.}

    \begin{formula}[Exponential distribution]\index{distribution!exponential}\label{statistics:exponential_distr}
        \begin{align}
            f(x;\tau) &:= \frac{1}{\tau}e^{-\frac{x}{\tau}}\\\nonumber\\
            \expect{x} &= \tau\\\nonumber\\
            \variance{x} &= \tau^2.
        \end{align}
    \end{formula}
    \begin{property}\index{memory}\label{statistics:memoryless_exponential_distribution}
        The exponential distribution is \textbf{memoryless}:
        \begin{gather}
            \mathrm{Pr}(X>x_1+x_2|X>x_2) = \mathrm{Pr}(X>x_1).
        \end{gather}
    \end{property}

    \newformula{Bernoulli distribution}{\index{distribution!Bernoulli}\index{Bernoulli|seealso{distribution}}\label{statistics:bernoulli_distr}
        A random variable that can only take 2 possible values is described by a Bernoulli distribution. When the possible values are 0 and 1, with respective chances $\rho$ and $1-\rho$, the distribution is given by
        \begin{align}
            p(k;\rho) &:= \rho^k(1-\rho)^{1-k}\\\nonumber\\
            \expect{k} &= \rho\\\nonumber\\
            \variance{k} &= \rho(1-\rho).
        \end{align}
    }

    \newformula{Binomial distribution}{\index{distribution!binomial}\label{statistics:binomial_distr}
        A process with $n$ i.i.d. Bernoulli trials with probability $\rho$, is described by a binomial distribution:
        \begin{align}
            p(k;\rho,n) &:= \binom{n}{k}\rho^k(1-\rho)^{n-k}\\\nonumber\\
            \expect{k} &= n\rho\\\nonumber\\
            \variance{k} &= n\rho(1-\rho).
        \end{align}
    }

    \begin{formula}[Poisson distribution]\index{distribution!Poisson}\label{statistics:poisson_distr}
        A process with known possible outcomes but an unknown number of events is described by a Poisson distribution with average expected number of events $\lambda$.
        \begin{align}
            p(r;\lambda) &:= \frac{e^{-\lambda}\lambda^r}{r!}\\\nonumber\\
            \expect{r} &= \variance{r} = \lambda.
        \end{align}
    \end{formula}
    \begin{property}
        If two Poisson processes, with expectations $\lambda_a$ and $\lambda_b$ respectively, occur simultaneously, the probability of $r$ events is also described by a Poisson distribution with average $\lambda_a+\lambda_b$. The number of events coming from the process described by $\lambda_a$ is given by a binomial distribution $p(r_a;\Lambda_a, r)$ with $\Lambda_a = \frac{\lambda_a}{\lambda_a + \lambda_b}$.
    \end{property}
    \begin{remark}
        For $\lambda\longrightarrow\infty$, the Poisson distribution $p(r;\lambda)$ can be approximated by a Gaussian distribution $\mathcal{G}(x;\lambda,\sqrt{\lambda})$.
    \end{remark}

    \begin{formula}[$\chi^2$-distribution]\index{distribution!$\chi^2$}\label{statistics:chi_squared_distr}
        The sum of $k$ squared, independent (standard), normally distributed random variables $Y_i$ defines the random variable:
        \begin{gather}
            \chi^2_k := \sum_{i=1}^kY_i^2,
        \end{gather}
        where $k$ is said to be the number of \textbf{degrees of freedom}. The associated density is
        \begin{gather}
            f(\chi^2;n) := \frac{\chi^{n-2}e^{-\frac{\chi^2}{2}}}{2^{\frac{n}{2}}\Gamma\left(\frac{n}{2}\right)}.
        \end{gather}
    \end{formula}
    \remark{Due to the CLT \ref{statistics:CLT} the $\chi^2$-distribution approximates a Gaussian distribution for large $k$: $f(\chi^2;k)\overset{k>30}{\longrightarrow}\mathcal{G}(\sqrt{2\chi^2};\sqrt{2k-1},1)$.

    \newformula{Student-$t$ distribution}{\index{distribution!Student-$t$}\label{statistics:student_t_distr}
        The Student-$t$ distribution describes the difference between the true mean and a sample average with estimated standard deviation $\hat{\sigma}$:
        \begin{gather}
            f(t;n) := \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\ \Gamma\left(\frac{n}{2}\right)\left(1 + \frac{t^2}{n}\right)^{\frac{n+1}{2}}},
        \end{gather}
        where
        \begin{gather}
            t := \frac{(x-\mu)/\sigma}{\hat{\sigma}/\sigma} = \frac{z}{\sqrt{\chi^2/n}}.
        \end{gather}
    }

    \newformula{Cauchy distribution\footnotemark}{\index{Cauchy!distribution}\index{Breit-Wigner|see{Cauchy distribution}}\label{stat:cauchy_distribution}
        \footnotetext{Also known (especially in particle physics) as the \textbf{Breit-Wigner} distribution.}
        The general density $f(x;x_0,\gamma)$ is given by
        \begin{gather}
            f(x;x_0,\gamma) := \frac{1}{\pi}\frac{\gamma}{(x - x_0)^2 + \gamma^2}.
        \end{gather}
        The associated characteristic function is given by
        \begin{gather}
            \expect{e^{itx}} = e^{ix_0t - \gamma|t|}.
        \end{gather}
    }
    \begin{property}
        Both the mean and variance of the Cauchy distribution are undefined.
    \end{property}

\section{Errors}

    \newdef{Systematic error}{\index{error}
        Errors that always have the same effect independent of the measurements itself, i.e. they shift all values in the same way and cannot be directly inferred from the measurements. Note that they are not necessarily independent of each other.
    }

    \begin{formula}[Inverse-variance averaging]
        When performing a sequence of measurements $x_i$ with different variances $\sigma_i^2$, it is impossible to use the arithmetic mean \ref{statistics:arithmetic_mean} in a meaningful way because the measurements are not of the same type. Therefore it is also impossible to apply the CLT \ref{statistics:CLT}.

        These problems can be resolved by the using the weighted mean \ref{statistics:weighted_mean}:
        \begin{gather}
            \overline{x} := \frac{\sum_i\frac{x_i}{\sigma_i^2}}{\sum_i\frac{1}{\sigma_i^2}}.
        \end{gather}
        The variation of the weighted mean is given by
        \begin{gather}
            \label{statistics:weighted_mean_variance}
            \mathrm{Var}(\overline{x}) := \frac{1}{\sum_i\sigma_i^{-2}}.
        \end{gather}
    \end{formula}

\subsection{Propagation of errors}

    \begin{formula}\index{variance}
        Let $X$ be random variable with variance $\variance{X}$. The variance of a linear function $f(X) = aX + b$ is given by
        \begin{gather}
            \label{statistics:variance_linear_function}
            \variance{f} = a^2\variance{X}.
        \end{gather}
    \end{formula}
    \begin{formula}
        Let $X$ be random variable with a \underline{small} variance $\variance{X}$. The variance of a general function $f(X)$ is given by
        \begin{gather}
            \label{statistics:variance_general_function}
            \variance{f}\approx\left(\deriv{f}{x}\right)^2\variance{x}.
        \end{gather}
    \end{formula}
    \begin{result}
        The correlation coefficient $\rho$ (\ref{statistics:correlation_coefficient}) of a random variable $X$ and a \textbf{linear} function of $X$ is independent of $\sigma_x$ and is always equal to $\pm1$.
    \end{result}

    \newformula{Law of error propagation}{\index{propagation}
        Let $\mathbf{X}$ be a vector random variable with \underline{small} variance. The variance of a general function $f(\mathbf{X})$ is given by
        \begin{gather}
            \label{statistics:error_propagation}
            \variance{f} = \sum_p\left(\pderiv{f}{X_{(p)}}\right)^2\variance{X_{(p)}} + \sum_{p\neq q}\left(\pderiv{f}{X_{(p)}}\right)\left(\pderiv{f}{X_{(q)}}\right)\text{cov}[X_{(p)},X_{(q)}].
        \end{gather}
    }

    \newdef{Fractional error}{\index{fractional error}
        Let $X,Y$ be two independent random variables. The standard deviation of $f(X,Y) = XY$ is given by the fractional error:
        \begin{gather}
            \label{statistics:fractional_error}
            \left(\frac{\sigma_f}{f}\right)^2 = \left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2.
        \end{gather}
        The fractional error of a variable is equal to the fractional error of the reciprocal of that variable.
    }

    \begin{property}[Logarithm]
        Let $X$ be a random variable. The error of the logarithm of $X$ is equal to the fractional error of $X$.
    \end{property}

    \newformula{Covariance of functions}{\index{covariance}
        \begin{gather}
            \label{statistics:covariance_functions}
            \mathrm{cov}[f,g] = \sum_{p,q}\left(\pderiv{f}{X_{(p)}}\right)\left(\pderiv{g}{X_{(q)}}\right)\mathrm{cov}[X_{(p)},X_{(q)}]
        \end{gather}
    }
    \begin{result}
        Let $\mathbf{f} = (f_1,\ldots,f_k)$ be a vector-valued function. The covariance matrix $\variance{\mathbf{f}}$ is given by
        \begin{gather}
            \variance{\mathbf{f}} = J\variance{\mathbf{X}}J^T,
        \end{gather}
        where $J$ is the Jacobian matrix of $\mathbf{f}$.
    \end{result}

\section{Parameter estimation}
\subsection{General properties}

    \newdef{Consistency}{\index{consistency}\label{statistics:consistency}
        An estimator $\hat{a}$ is said to be consistent if it is asymptotically equal to the true parameter:
        \begin{gather}
            \lim_{N\rightarrow\infty}\hat{a} = a.
        \end{gather}
    }
    \newdef{Unbiased estimator}{\label{statistics:unbiased_estimator}
        An estimator $\hat{a}$ is said to be unbiased if its expectation value is equal to the true parameter:
        \begin{gather}
            \langle\hat{a}\rangle = a.
        \end{gather}
        Note that neither consistency, nor unbiasedness implies the other.
    }

    \newdef{Bias}{\index{bias}\label{statistics:bias}
        \begin{gather}
            B(\hat{a}) := |\langle\hat{a}\rangle - a|.
        \end{gather}
    }

    \newdef{Mean squared error}{\label{statistics:mean_squared_error}
        \begin{gather}
            \mathrm{MSE}(\hat{a}) := B(\hat{a})^2 + \mathrm{Var}(\hat{a}).
        \end{gather}
    }
    \remark{If an estimator is unbiased, the MSE is equal to the variance of the estimator.}

\subsection{Common estimators}

    \begin{property}[Unbiased mean]
        The CLT \ref{statistics:CLT} implies that the sample mean \ref{statistics:arithmetic_mean} is a consistent and unbiased estimator of the population mean.
    \end{property}
    \begin{formula}[Standard error of the mean]\index{standard!error}
        Using the Bienaym\'e formula \ref{prob:bienayme} one can show that the standard error of the mean, i.e. the standard deviation of the sample mean, is given by the following formula:
        \begin{gather}
            \label{statistics:standard_error}
            \mathrm{Var}(\overline{x}) = \frac{\sigma^2}{N}.
        \end{gather}
    \end{formula}

    \newformula{Variance estimator for known mean}{\index{variance!estimator}
        If the true mean $\mu$ is known, a consistent and unbiased estimator for the variance is given by
        \begin{gather}
            \widehat{\variance{X}} = \frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2.
        \end{gather}
    }
    \newformula{Variance estimator for unknown mean}{\index{Bessel!correction}\label{statistics:variance_bessel_correction}
        If the true mean is unknown and the sample mean has been used to estimate it, a consistent and unbiased estimator is given by
        \begin{gather}
            s^2 = \frac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x})^2.
        \end{gather}
        The modified factor $\frac{1}{N-1}$ is called the \textbf{Bessel correction}. It corrects the bias of the estimator given by the sample variance \ref{statistics:sample_variance}. The consistency is guaranteed by the CLT.
    }

\subsection{Estimation error}

    \newformula{Variance of the estimator of the variance}{
        \begin{gather}
            \mathrm{Var}\left(\widehat{\variance{X}}\right) =  \frac{(N-1)^2}{N^3}\langle(x - \langle x \rangle)^4\rangle - \frac{(N-1)(N-3)}{N^3}\langle(x - \langle x \rangle)^2\rangle^2
        \end{gather}
    }
    \newformula{Variance of the estimator of the standard deviation}{
        \begin{gather}
            \mathrm{Var}(\widehat{\sigma}) = \frac{1}{4\sigma^2}\mathrm{Var}\left(\widehat{\variance{X}}\right)
        \end{gather}
    }
    \begin{remark}
        The previous result is a little odd, as one has to know the true standard deviation to compute the variance of the estimator. This problem can be solved in two ways. Either a value (hopefully close to the real one) inferred from the sample is used as an estimator, or a guess is used in the design phase of an experiment to see what the possible outcomes are.
    \end{remark}

\subsection{Likelihood function}

    \newdef{Likelihood}{\index{likelihood}\label{statistics:likelihood}
        The likelihood $\mathcal{L}(a;\vec{x})$ is the probability to find a set of measurements $\vector{x} = \{x_1,\ldots,x_N\}$ given a density $f(X;a)$:
        \begin{gather}
            \mathcal{L}(a;\vector{x}) = \prod_{i=1}^Nf(x_i;a).
        \end{gather}
    }
    \newdef{Log-likelihood}{\label{statistics:log_likelihood}
        \begin{gather}
            \log\mathcal{L}(a;\vector{x}) = \sum_i\ln f(x_i;a)
        \end{gather}
    }

    \begin{property}
        The expectation value of an estimator $\hat{a}$ is given by
        \begin{gather}
            \langle\hat{a}\rangle = \int\hat{a}\mathcal{L}(\hat{a};x)dx.
        \end{gather}
    \end{property}

    \begin{theorem}[Cramer-Rao bound]\index{Cramer-Rao}\index{minimum!variance bound}\label{statistics:minimum_variance_bound}
        The variance of an \textbf{unbiased} estimator has a lower bound called the Cramer-Rao bound or \textbf{minimum variance bound (MVB)}:
        \begin{gather}
            \mathrm{Var}(\hat{a})\geq\frac{1}{\left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle}.
        \end{gather}
        For a biased estimator with bias $b$, the MVB takes on the following form:
        \begin{gather}
            \label{statistics:biased_minimum_variance_bound}
            \mathrm{Var}(\hat{a})\geq\frac{\left(1+\deriv{b}{a}\right)^2}{\left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle}.
        \end{gather}
    \end{theorem}
    \begin{remark}
        \begin{gather}
            \left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle = -\left\langle\mderiv{2}{\ln\mathcal{L}}{a}\right\rangle
        \end{gather}
    \end{remark}

    \newdef{Fisher information}{\index{Fisher!information}\label{statistics:fisher_information}
        \begin{gather}
            I_X(a) := \left\langle\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right\rangle = N\int\left(\deriv{\ln f}{a}\right)^2f\,dX
        \end{gather}
        Using this definition one can rewrite the Cramer-Rao inequality as follows:
        \begin{gather}
            \mathrm{Var}(\hat{a})\geq I_X(a).
        \end{gather}
    }

    \newdef{Finite-sample efficiency}{\index{efficient}
        An unbiased estimator is said to be (finite-sample) efficient if it saturates the Cramer-Rao bound. In general the \textbf{efficiency} of (unbiased) estimators is defined through the Cramer-Rao bound as follows:
        \begin{gather}
            e(\hat{a}) := \frac{I_X(a)^{-1}}{\mathrm{Var}(\hat{a})}.
        \end{gather}
    }

\subsection{Maximum likelihood estimation}

    From definition \ref{statistics:likelihood} it follows that the estimator $\hat{a}_\mathrm{MLE}$ that makes the given measurements most probable is the value of $a$ for which the likelihood function is maximal. It is therefore not the most probable estimator.

    Using Bayes's theorem one finds $f(a|x) = f(x|a)\frac{f(a)}{f(x)}$. The prior density $f(x)$ is fixed since the values $x_i$ are given by the measurement and, hence, does not vary. The density $f(a)$ is generally assumed to be uniform if there is no prior knowledge about $a$. It follows that $f(a|x)$ and $f(x|a)$ are proportional and, hence, the logarithms of these functions differ only by an additive constant. This leads to following method for finding an estimator $\hat{a}$:
    \newmethod{Maximum likelihood estimator}{\index{likelihood!estimator}
        The maximum likelihood estimator $\hat{a}$ is obtained by solving the following equation:
        \begin{gather}
            \label{statistics:maximum_likelihood_estimator}
            \left.\deriv{\ln\mathcal{L}}{a}\right|_{a=\hat{a}} = 0.
        \end{gather}
    }
    \remark{MLE estimators are mostly consistent but often biased.}
    \begin{property}
        MLE estimators are invariant under parameter transformations.
    \end{property}
    \result{The invariance implies that the two estimators $\hat{a}$ and $\widehat{f(a)}$ cannot both be unbiased at the same time.}

    \begin{property}
        Every consistent estimator asymptotically becomes unbiased and efficient.
    \end{property}

    \begin{property}[Minimizing KL-divergence]\label{statistics:minimizing_KL}
        It can be shown that maximizing the log-likelihood is equivalent to minimizing the Kullback-Leibler divergence \ref{prob:kullback_leibler} between the would-be distribution $p(x;\theta)$ and the true distribution $q(x)$:
        \begin{align*}
            \arg\max_\theta\ln\mathcal{L} &= \arg\max_\theta\sum_{i\in I}\ln p(x_i;\theta)\\
            &= \arg\max_\theta\sum_{i\in I}\ln p(x_i;\theta) - \ln q(x_i)\\
            &= \arg\min_\theta\frac{1}{n}\sum_{i\in I}\ln\frac{q(x_i)}{p(x_i;\theta)}\\
            &\longrightarrow\arg\min\theta\int p(x;\theta)\ln\frac{q(x)}{p(x;\theta)}dx = \arg\min_\theta D_\mathrm{KL}(p_\theta\|q),
        \end{align*}
        where the law of large numbers was used in the last line.
    \end{property}

\subsection{Least squares estimation}

    To fit a (parametric) function $y = f(x;a)$ to a set of 2 variables $(x,y)$, where the $x$ values are exact and the $y$ values have an uncertainty $\sigma_i$, one can use the following method:
    \newmethod{Least squares}{\index{least squares}$ $
        \begin{enumerate}
            \item For every event $(x_i,y_i)$ define the residual $d_i := y_i - f(x_i;a)$.
            \item Determine the $\chi^2$-statistic (analytically):
                \begin{gather}
                    \chi^2 := \sum_i\frac{d_i^2}{f_i},
                \end{gather}
                where $f_i = f(x_i;a)$.
            \item Find the most probable value of $\hat{a}$ by solving the equation
                \begin{gather}
                    \deriv{\chi^2}{a} = 0.
                \end{gather}
        \end{enumerate}
    }
    \begin{property}
        The optimal $\chi^2$ is (asymptotically) distributed according to a $\chi^2$-distribution $f(\chi^2;n)$. The number of degrees of freedom $n$ is equal to the number of events $N$ minus the number of fitted parameters $k$. (See more in Section \ref{section:chi_squared_test}.)
    \end{property}

    \newformula{Linear fit}{\index{linear!fit}
        When all uncertainties $\sigma_i$ are equal, the slope $\hat{m}$ and intercept $\hat{c}$ are given by the following formulas:
        \begin{align}
            \label{statistics:least_squares_slope}
            \hat{m} &:= \frac{\overline{xy} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{\mathrm{cov}(x,y)}{\mathrm{Var}(x)}\\
            \label{statistics:least_squares_intercept}
            \hat{c} &:= \overline{y} - \hat{m}\overline{x} = \frac{\overline{x^2} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2}.
        \end{align}
    }
    \remark{The equation $\overline{y} = \hat{c} + \hat{m}\overline{x}$ says that the linear fit passes through the center of mass $(\overline{x},\overline{y})$.}

    \newformula{Errors of linear fit}{
        \begin{align}
            \label{statistics:least_squares_slope_variance}
            \mathrm{Var}(\hat{m}) &= \frac{1}{N(\overline{x^2} - \overline{x}^2)}\sigma^2\\&\nonumber\\
            \label{statistics:least_squares_intercept_variance}
            \mathrm{Var}(\hat{c}) &= \frac{\overline{x^2}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2\\&\nonumber\\
            \label{statistics:least_squares_linear_fit_covariance}
            \mathrm{cov}(\hat{m},\hat{c}) &= \frac{-\overline{x}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2
        \end{align}
    }

    The least squares method is very useful to fit data that has been grouped in bins (histograms):
    \newmethod{Binned least squares}{$ $
        \begin{enumerate}
            \item $N$ i.i.d. events with distributions $f(X;a)$ divided in $N_B$ intervals, where the interval $j$ is centered on the value $x_j$, has a width $W_j$ and contains $n_j$ events.
            \item The ideally expected number of events in the $j^{th}$ interval: $f_j = NW_jf(x_j;a)$.
            \item The real number of events has a Poisson distribution: $\overline{n}_j = \sigma_j^2 = f_j$.
            \item Define the binned $\chi^2$ as
                \begin{gather}
                    \chi^2 := \sum_i^{N_B}\frac{(n_i - f_i)^2}{f_i^2}.
                \end{gather}
        \end{enumerate}
    }

\subsection{Geometric approach}

    Consider a sample $\mathbf{x}:=\{x_1,\ldots,x_n\}$ drawn from a distribution $P(\mathbf{x};\theta)$ in an exponential family. The likelihood \eqref{statistics:likelihood} is given by \[p(\mathbf{x};\theta) = \prod_{i=1}^nP(x_i;\theta).\] The $m$-coordinates of the observed point are
    \begin{gather}
        \eta = \frac{1}{n}\sum_{i=1}^nx_i = \overline{x}.
    \end{gather}
    The optimal value for $\theta$ can be found by maximizing the log-likelihood $\log p(\mathbf{x};\theta)$ or, equivalently according to Property \ref{statistics:minimizing_KL}, by minimizing the Kullback-Leibler divergence between the observed point and the ``true'' distribution $P(x;\xi)$. The latter is found by $m$-projecting the observed point $\eta$ on the submanifold $S$ of ``admissible'' distributions.

\section{Bayesian modelling}

    \newdef{Conjugate distributions}{\index{conjugate!distribution}
        Consider a prior distribution $F(\theta)$ and a posterior distribution $F(\theta|X)$. If these distributions belong to the same family, e.g. they are both Gaussians, they are said to be conjugate. In this case the prior $F(\theta)$ is said to be a \textbf{conjugate prior} for the likelihood $F(X|\theta)$.
    }
    \begin{example}
        The simplest example is the case of binomial distributions, where the conjugate prior is the \textit{Beta distribution}. This can be generalized to multi-class situations. The conjugate prior of a categorical (or even \textit{multinomial}) distribution is the \textit{Dirichlet distribution}.
    \end{example}

\section{Confidence intervals}\index{confidence}

    The true value of a parameter $\varepsilon$ can never be known exactly. However, it is possible to construct an interval $I$ in which this value should lie with a certain confidence $C$.
    \begin{example}[Prediction interval]
        Let $X$ be a normally distributed random variable. A measurement will lie in the interval $[\mu - 1.96\sigma, \mu+1.96\sigma]$ with 95\% \underline{probability}. The true value $\mu$ lies in the interval $[x - 2\sigma, x+2\sigma]$ with 95\% \underline{confidence}.
    \end{example}
    \begin{remark*}
        In the previous example some assumptions were made. All possible values (left or right side of peak) are given the same probability due to the Gaussian distribution. If one removes this symmetry condition, a more careful approach is required. Furthermore, the apparent symmetry between the uncertainty and confidence levels is only valid for Gaussian distributions.
    \end{remark*}

\subsection{Interval types}

    \newdef{Two-sided confidence interval}{\label{statistics:two_sided_interval}
        \begin{gather}
            \mathrm{Pr}(x_-\leq X\leq x_+) = \int_{x_-}^{x_+}f(x)dx = C
        \end{gather}
        There are three possible (often used) two-sided intervals:
        \begin{itemize}
            \item\textbf{symmetric interval}: $x_+ - \mu = \mu - x_-$,
            \item\textbf{shortest interval}: $|x_+ - x_-|$ is minimal, or
            \item\textbf{central interval}: $\int_{-\infty}^{x_-}f(x)dx = \int_{x_+}^\infty f(x)dx = \frac{1-C}{2}$.
        \end{itemize}
        The central interval is the most widely used confidence interval.
    }
    \remark{For Gaussian distributions these three definitions are equivalent.}

    \newdef{One-sided confidence interval}{
        \begin{align}
            \label{statistics:one_sided_interval1}
            \mathrm{Pr}(x\geq x_-) &= \int_{x_-}^\infty f(x)dx = C\\
            \label{statistics:one_sided_interval2}
            \mathrm{Pr}(x\leq x_+) &= \int_{-\infty}^{x_+}f(x)dx = C
        \end{align}
    }

    \newdef{Discrete central confidence interval}{
        For a discrete distribution it is often impossible to find integers $x_{\pm}$ such that the real value lies with exact confidence $C$ in the interval $[x_-,x_+]$.
        \begin{align}
            \label{statistics:central_discreteInterval_lower_bound}
            x_- &= \arg\min_\theta\left[\frac{1-C}{2} - \sum_{x=0}^{\theta - 1}p(x)\right]\\
            \label{statistics:central_discreteInterval_upper_bound}
            x_+ &= \arg\min_\theta\left[\frac{1-C}{2} - \sum_{x=\theta + 1}^\infty p(x)\right]
        \end{align}
    }

\subsection{General construction}

    For every value of the true parameter $X$ it is possible to construct a confidence interval. This leads to the construction of two functions $x_-(X)$ and $x_+(X)$. The 2D diagram obtained by plotting $x_-(X)$ and $x_+(X)$ with the $x$-axis horizontally and $X$-axis vertically is called the \textbf{confidence region}.
    \begin{method}
        Let $x_0$ be a point estimate of the parameter $X$. From the confidence region it is possible to infere a confidence interval $[X_-(x),X_+(x)]$, where the upper limit $X_+$ is not the limit such that there is only a $\frac{1-C}{2}$ chance of having a true parameter $X\geq X_+$, but the limit such that if the true parameter $X\geq X_+$ then there is a chance of $\frac{1-C}{2}$ to have a measurement $x_0$ or smaller.
    \end{method}

\subsection{Interval for a sample mean}

    \newformula{Interval with known variance}{
        If the sample size is large enough, the real distribution is unimportant, because the CLT ensures a Gaussian distribution of the sample mean $\overline{X}$. The $\alpha$-level confidence interval such that $\mathrm{Pr}(-z_{\alpha/2}<Z<z_{\alpha/2})$ with $Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{N}}$ is given by
        \begin{gather}
            \left[\overline{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{N}},\overline{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{N}}\right].
        \end{gather}
    }
    \remark{If the sample size is not sufficiently large, the measured quantity must follow a normal distribution.}

    \newformula{Interval with unknown variance}{
        To account for the uncertainty of the estimated standard deviation $\hat{\sigma}$, the student-t distribution \ref{statistics:student_t_distr} is used instead of a Gaussian distribution to describe the sample mean $\overline{X}$. The $\alpha$-level confidence interval is given by
        \begin{gather}
            \left[\overline{X} - t_{\alpha/2;(n-1)}\frac{s}{\sqrt{N}},\overline{X} + t_{\alpha/2;(n-1)}\frac{s}{\sqrt{N}}\right],
        \end{gather}
        where $s$ is the estimated standard deviation \ref{statistics:variance_bessel_correction}.
    }

    \newformula{Wilson score interval}{\index{Wilson score interval}
        For a sufficiently large sample, a sample proportion $\hat{P}$ is approximately Gaussian distributed with expectation value $\pi$ and variance $\frac{\pi(\pi-1)}{N}$. The $\alpha$-level confidence interval is given by
        \begin{gather}
            \left[\frac{(2N\hat{P} + z^2_{\alpha/2}) - z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})},\frac{(2N\hat{P} + z^2_{\alpha/2}) + z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})}\right].
        \end{gather}
    }
    \sremark{The expectation value and variance are these of a binomial distribution \ref{statistics:binomial_distr} with $r = X/N$.}

\section{Hypothesis testing}\index{hypothesis}

    \newdef{Simple hypothesis}{
        A hypothesis where the distribution is fully specified.
    }
    \newdef{Composite hypothesis}{
        A hypothesis where the distribution is given relative to some parameter values.
    }

\subsection{Testing}

    \newdef{Type I error}{\index{error}
        Rejection of a true null hypothesis.
    }
    \newdef{Type II error}{
        Acceptance of a false null hypothesis.
    }

    \newdef{Significance}{\index{significance}\label{statistics:significance}
        The probability of making a type I error:
        \begin{gather}
            \alpha := \int P_I(x)dx.
        \end{gather}
    }
    \begin{property}
        Let $\alpha_1>\alpha_2$. An $\alpha_2$-level test is also significant at the $\alpha_1$-level.
    \end{property}
    \remark{For discrete distributions it is not always possible to achieve an exact level of significance.}
    \sremark{Type I errors occur occasionally. They cannot be prevented, one can only try to control them.}

    \newdef{Power}{\index{power}
        The probability of not making a type II error:
        \begin{gather}
            \beta := \int P_{II}(x)dx \quad\longrightarrow\quad \text{power: }1-\beta.
        \end{gather}
    }
    \begin{remark}
        A good test is a test with a small significance and a large power. The probabilities $P_I(x)$ and $P_{II}(x)$ should be as different as possible.
    \end{remark}

    \begin{definition}[Likelihood ratio test]\index{likelihood!ratio}\label{statistics:likelihood_ratio}
        The null hypothesis $H_0:\theta=\theta_0$ is rejected in favour of the alternative hypothesis $H_1:\theta=\theta_1$ if the likelihood ratio $\Lambda$ satisfies the following condition:
        \begin{gather}
            \Lambda(x) = \frac{\mathcal{L}(\theta_0|x)}{\mathcal{L}(\theta_1|x)}\leq\eta,
        \end{gather}
        where $P(\Lambda(x)\leq\eta|H_0) = \alpha $.
    \end{definition}
    \sremark{In some references the reciprocal of $\Lambda(x)$ is used as the definition of the likelihood ratio.}
    \begin{theorem}[Neyman-Pearson lemma]\index{Neyman-Pearson}
        The likelihood ratio test is the most powerful test at significance level $\alpha$.
    \end{theorem}

    \newdef{Family-wise error}{\index{error}
        Given a collection of hypothesis tests, the family-wise error is defined as the probability of making at least one type-I error.
    }
    \begin{construct}[Bonferroni correction]\index{Bonferroni correction}
        Consider a set of hypotheses $\{H_i\}_{1\leq i\leq n}$. The higher the number of tests, the higher the chance that by statistical fluctuations at least one of these hypotheses will be rejected. To avoid this problem of multiple comparisons, one can try to control the family-wise error rate, i.e. the probability of falsely rejecting at least one hypothesis. The easiest way to control this error rate is by modifying the individual significance levels:
        \begin{gather}
            \alpha\longrightarrow\frac{\alpha}{n}.
        \end{gather}
    \end{construct}

\section{Comparison tests}

    \newdef{McNemar test}{\index{test!McNemar}
        Consider two models or hypotheses describing a given data set. Construct the contingency table describing the number of true positives and true negatives for both models:
        \begin{gather}
            \begin{array}{c||c|c}
                &\text{TP (model 1)}&\text{TN (model 1)}\\
                \hline
                \text{TP (model 2)}&a&b\\
                \hline
                \text{TN (model 2)}&c&d
            \end{array}
        \end{gather}
        The null hypothesis of the McNemar test is that there is no significant difference between the predictive power of the model, i.e. $ p_a+p_c = p_a+p_b$ and $p_b+p_d = p_c+p_d$ where $p_i$ indicates the proportion of the variable $i$. By noting that the diagonal values are redundant in this description, one can write the hypotheses more concisely:
        \begin{align*}
            H_0&:b=c\\
            H_1&:b\neq c.
        \end{align*}
        The test statistic is the McNemar chi-squared statistic:
        \begin{gather}
            \chi^2 = \frac{(b-c)^2}{b+c}.
        \end{gather}
        When the values of $b$ and $c$ are large enough ($>25$), one can approximate this distribution by an ordinary $\chi^2$-distribution with 1 degree of freedom.
    }
    \begin{remark}[Edwards correction]\index{Edwards correction}
        It is common to apply a continuity correction (similar to the \textit{Yates-correction} for the ordinary chi-squared test):
        \begin{gather}
           \chi^2 := \frac{(|b-c|-1)^2}{b+c}.
        \end{gather}
        This follows from the fact that for small $b,c$ the exact $p$-values should be compared with a binomial test which compares $b$ to $b+c$ (note the factor of 2):
        \begin{gather}
           p = 2\sum_{i=b}^{b+c}\binom{b+c}{i}0.5^i(1 - 0.5)^{b+c-i}.
        \end{gather}
    \end{remark}

    \newdef{Wilcoxon signed-rank test}{\index{test!Wilcoxon}
        Consider a paired data sample, i.e. two dependent data samples for which the $i^{th}$ entries are paired together. This test checks if the population means are different. The test statistic is defined as follows:
        \begin{quote}
            First one calculates the differences $d_i$ and ranks their absolute values (ties are assigned an average rank). Then one calculates the sums of the ranks $R_+,R_-$ for positive and negative differences and takes smallest of these:
            \begin{gather}
                T:=\min(R_+,R_-).
            \end{gather}
            For small data samples ($n<25$) one can look up critical values in the literature. For larger data samples one can (approximately) use a standard normal distribution with statistic \[z := \frac{T-\frac{1}{4}n(n+1)}{\sqrt{\frac{1}{24}n(n+1)(2n+1)}}.\]
        \end{quote}
    }
    \begin{remark}
        The biggest benefit of this test over a signed $t$-test is that the Wilcoxon test does not require the data samples to be drawn from a normal distribution. However in the case where the assumptions for a paired $t$-test are met, the $t$-test is more powerful.
    \end{remark}

    \newdef{Friedman test}{\index{test!Friedman}
        Consider $k$ models tested on $N$ data sets. For every data set one ranks the models according to decreasing performance. For every $i\leq k$ one defines the average rank $R_i=\frac{1}{N}\sum_{j\leq N}r^j_i$, where $r^j_i$ is the rank of the $i^{th}$ model on the $j^{th}$ data set. Under the null hypothesis ``all models perform equally well'', the average ranks should be the same for all models.

        The Friedman statistic
        \begin{gather}
            \chi^2_F := \frac{12N}{k(k+1)}\left(\sum_{i\leq k}R_i^2 - \frac{k(k+1)^2}{4}\right)
        \end{gather}
        follows a $\chi^2$-distribution with $k-1$ degrees of freedom when $N>10$ and $k>5$. For smaller values of these parameters one can look up the exact critical values in the literature.
    }
    \begin{remark}
        It was shown that the original Friedman test is rather conservative and that a better statistic is
        \begin{gather}
            F := \frac{(N-1)\chi^2_F}{N(k+1)-\chi^2_F}.
        \end{gather}
        This follows an $F$-distribution with $k-1$ and $(N-1)(k-1)$ degrees of freedom. As a further remark is that the (nonparametric) Friedman test is weaker than the (parametric) \textit{repeated-measures ANOVA} whenever the assumptions for the latter hold (similar to the case of the Wilcoxon signed-rank test).
    \end{remark}

\subsection{Post-hoc tests}

    After successfully using one of the multi-model tests from the previous section to reject the null hypothesis of equal performance, one is often interested in exactly which model outperforms the others. For this one can use one of the following pairwise tests:

    \newdef{Nemenyi test}{\index{test!Nemenyi}
        Consider the average ranks $R_i$ from the Friedman test. As a test statistic one uses
        \begin{gather}
            z := \frac{R_i - R_j}{\sqrt{\frac{k(k+1)}{6N}}},
        \end{gather}
        where $k$ is the number of models and $N$ is the number of data sets. The exact critical values can either be found in the literature or one can approximately use a normal distribution.
    }

    \newdef{Bonferroni-Dunn test}{\index{test!Bonferonni-Dunn}
        If all one wants to do is see if a particular model performs better than a given baseline model, the Nemenyi test is too conservative since it corrects for $k(k-1)/2$ model comparisons instead of $k-1$. Therefore it is better to use a general method to control the family-wise error for multiple measurements. The Bonferroni-Dunn test modifies the Nemenyi test by performing a Bonferroni correction with $n-1$ degrees of freedom.
    }

    A more powerful test is given by the following strategy:
    \newdef{Holm test}{\index{test!Holm}
        Consider the $p$-values of the Nemenyi test. Instead of comparing all values to a single Bonferroni-corrected significance, one can use a so-called ``step-down'' method. First one orders the $p$-values in ascending order and compares the smallest one to $\frac{\alpha}{k-1}$. If this value is significant, i.e. the hypothesis that the associated models perform equally well is rejected, one compares $p_2$ to $\frac{\alpha}{k-2}$ and so on until one finds a hypothesis that cannot be rejected. All remaining hypotheses are retained as well.
    }

    \begin{remark}
        It is possible that the post hoc test fails to report a significant difference even though the Friedman test rejected the null hypothesis. This is a consequence of the lower power of post hoc tests.
    \end{remark}

\section{Goodness of fit}\index{goodness of fit}

    \newdef{Akaike information criterion}{\index{Akaike information criterion}
        \nomenclature[A_Ak]{AIC}{Akaike information criterion}
        Consider a model $f(x;\theta)$ with $k$ parameters fitted to a given data sample and let $\mathcal{L}_0$ be the maximum of the associated likelihood function. The Akaike information criterion is defined a follows:
        \begin{gather}
            \text{AIC} := 2k - 2\ln(\mathcal{L}_0).
        \end{gather}
        From this definition it is immediately clear that the AIC rewards goodness-of-fit but penalizes overfitting due to the first term.

        This criterion is often useful when trying to select the best model/parameters to describe a certain data set. However it should be noted that it is not an absolute measure of quality.
    }

\subsection{\texorpdfstring{$\chi^2$}{Chi squared}-test}\index{$\chi^2$-test}\label{section:chi_squared_test}

    \begin{property}\label{statistics:chance_for_chi_square}
        If there are $N - n$ fitted parameters one has:
        \begin{gather}
            \int_{\chi^2}^\infty f\left(\chi^2|n\right)d\chi^2 \approx 1\implies
            \begin{cases}
                \circ\text{ good fit}\\
                \circ\text{ errors were overestimated}\\
                \circ\text{ selected measurements}\\
                \circ\text{ lucky shot}
            \end{cases}
        \end{gather}
    \end{property}
    \begin{property}[Reduced chi-squared]
        The reduced chi-squared statistic is defined as follows:
        \begin{gather}
            \chi^2_\mathrm{red} := \chi^2/n,
        \end{gather}
        where $n$ is the number of degrees of freedom. Depending on the value of this statistic one can draw the following conclusions (under the right assumptions):
        \begin{itemize}
            \item $\chi^2_\mathrm{red} \gg 1$: poor modelling,
            \item $\chi^2_\mathrm{red} > 1$: bad modelling or underestimation of the uncertainties,
            \item $\chi^2_\mathrm{red} \approx 1$: good fit, or
            \item $\chi^2_\mathrm{red} < 1$: (improbable) overestimation of the uncertainties.
        \end{itemize}
    \end{property}

\subsection{Runs test}\index{runs test}

    A good $\chi^2$-test does not mean that the fit is good. As mentioned in Property \ref{statistics:chance_for_chi_square}, it is possible that the errors were overestimated. Another condition for a good fit is that the data points vary around the fit, i.e. there are no long sequences of points that lie above/underneath the fit. This condition is tested with a runs test \ref{statistics:runs_distribution}.

    \begin{remark}
        The $\chi^2$-test and runs test are complementary. The $\chi^2$-test only takes the absolute value of the differences between the fit and data points into account, the runs test only takes the signs of the differences into account.
    \end{remark}

    \newformula{Runs distribution}{\index{distribution!runs}\label{statistics:runs_distribution}
        Let $N_+$ and $N_-$ denote the number of points above and below the fit. Under the hypothesis that all points were independently drawn from the same distribution the number of runs is distributed as follows (approximately Gaussian):
        \begin{gather}
            \begin{aligned}
                P(r_\mathrm{even}) &= 2\frac{\binom{N_+ - 1}{\frac{r}{2} - 1}\binom{N_- - 1}{\frac{r}{2} - 1}}{\binom{N}{N_+}}\\\\
                P(r_\mathrm{odd}) &= \frac{\binom{N_+ - 1}{\frac{r - 3}{2}}\binom{N_- - 1}{\frac{r - 1}{2}} + \binom{N_- - 1}{\frac{r - 3}{2}}\binom{N_+ - 1}{\frac{r - 1}{2}}}{\binom{N}{N_+}},
            \end{aligned}
        \end{gather}
        where $C^n_k$ is the binomial coefficient $\binom{n}{k}$. The first two moments of this distribution are given by the following formulas:
        \begin{align}
            \expect{r} &= 1 + 2\frac{N_+ N_-}{N}\\
            \variance{r} &= 2\frac{N_+ N_-}{N}\frac{2N_+ N_- - N}{N(N-1)}.
        \end{align}
    }
    \remark{For $r > 15$, the runs distribution approximates a Gaussian distribution.}
\chapter{Statistics}

In this chapter, most definitions and formulas will be based on either a standard calculus approach or a data-driven approach. For a measure-theory based approach see chapter \ref{chapter:probability}.

\section{Data samples}
\subsection{Moment}

    	\newformula{$r^{th}$ sample  moment}{\index{moment}
        	\begin{equation}
            	\label{statistics:sample_moment}
				\overline{x^r} = \stylefrac{1}{N}\sum_{i=1}^Nx_i^r
                \end{equation}
        }
        \newformula{$r^{th}$ central sample moment}{
        	\begin{equation}
            	\label{statistics:central_sample_moment}
				m_r = \stylefrac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^r
			\end{equation}
        }
	
\subsection{Mean}\index{mean}

        \newdef{Arithmetic mean}{The arithmetic mean is used to average out differences between measurements. It is equal to the $1^{st}$ sample moment:
        	\begin{equation}
            		\label{statistics:arithmetic_mean}
            		\boxed{\overline{x} = \stylefrac{1}{N}\sum_{i=1}^Nx_i}
		\end{equation}
        }
        \newformula{Weighted mean}{
        	Let $f:\mathbb{R}\rightarrow\mathbb{R}^+$ be a weight function. The weighted mean is given by:
		\begin{equation}
            		\overline{x} = \stylefrac{\sum_if(x_i)x_i}{\sum_if(x_i)}
	        \end{equation}
        }
        \begin{result}
        	If the data has been grouped in bins, the weight function is given by the number of elements in each bin. Knowing this the (binned) mean becomes:
        	\begin{equation}
			\label{statistics:binned_arithmetic_mean}
	                \overline{x} = \stylefrac{1}{N}\sum_{i=1}n_ix_i
		\end{equation}
        \end{result}
        \remark{In the above defintions the measurements $x_i$ can be replaced by function values $f(x_i)$ to calculate the mean of the function $f(x)$.}
        \remark{It is also important to notice that $\overline{f}(x) \neq f(\overline{x})$. The equality only holds for linear functions.}
        
        \newdef{Geometric mean}{
        	Let $\{x_i\}$ be a positive data set\footnotemark. The geometric mean is used to average out \textit{normalized} measurements, i.e. ratios with respect to a reference value.
        	\begin{equation}
	            	\label{statistics:geometric_mean}
        	        g = \left(\prod_{i=1}^Nx_i\right)^{1/N}
        	\end{equation}
	        The following relation exists between the arithmetic and geometic mean:
        	\begin{equation}
            		\ln g = \overline{\ln x}
	        \end{equation}
	        \footnotetext{A negative data set is also allowed. The real condition is that all values should have the same sign.}
        }
        
	\newdef{Harmonic mean}{
        	\begin{equation}
        	    	\label{statistics:harmonic_mean}
        	        h = \left(\stylefrac{1}{N}\sum_{i=1}^Nx_i^{-1}\right)^{-1}
        	\end{equation}
	        The following relation exists between the arithmetic and harmonic mean:
        	\begin{equation}
            		\frac{1}{h} = \overline{x^{-1}}
	        \end{equation}
        }
        
        \begin{property}
        	Let $\{x_i\}$ be a positive data set.
        	\begin{equation}
	        	h\leq g\leq\overline{x}
	        \end{equation}
	        where the equalities only hold when all $x_i$ are equal.
        \end{property}
        
        \newdef{Mode}{\index{mode}
        	The most occurring value in a dataset.
	}
        \newdef{Median}{\index{median}
        	The median of dataset is the value $x_i$ such that half of the values is greater than $x_i$ and the other half is smaller than $x_i$.
	}
        
\subsection{Dispersion}

    	\newdef{Range}{\index{range}
        	The simplest indicator for statistical dispersion. It is however very sensitive for extreme values.
        	\begin{equation}
		        R = x_{max} - x_{min}
		\end{equation}
        }
    	
        \newdef{Mean absolute difference}{
        	\begin{equation}
			\text{MD} = \stylefrac{1}{N}\sum_{i=1}^N|x_i - \overline{x}|
		\end{equation}
        }
        
        \newdef{Sample variance}{\index{variance}
        	\begin{equation}
			\label{statistics:sample_variance}
        	        V(X) = \stylefrac{1}{N}\sum_{i=1}^N(x_i-\overline{x})^2
		\end{equation}
        }
        \begin{formula}
        	The variance can also be written in the following way:
        	\begin{equation}
			\label{statistics:variance_without_sum}
	                \boxed{V(X) = \overline{x^2} - \overline{x}^2}
		\end{equation}
        \end{formula}
	\begin{remark}
        	A better estimator for the variance of a sample is the following formula:
        	\begin{equation}
			\label{statistics:bessel_correction}
        	        \hat{s} = \stylefrac{1}{N-1}\sum_{i=1}^N(x_i - \overline{x})^2
		\end{equation}
        	Equation \ref{statistics:sample_variance} gives a good estimation when the sample mean $\overline{x}$ is replaced by the "true" mean $\mu$. Otherwise one should use the estimator \ref{statistics:bessel_correction}. 
        \end{remark}
        
        \newdef{Standard deviation}{
        	\begin{equation}
        		\sigma(X) = \sqrt{V(x)}
        	\end{equation}
        }
        
        \newdef{Skewness}{\index{skewness}
        	The skewness $\gamma$ describes the asymmetry of a distribution. It is defined in relation to the third central moment $m_3$;
	        \begin{equation}
			\label{statistics:skewness}
        	        m_3 = \gamma\sigma^3
		\end{equation}
        	where $\sigma$ is the standard deviation. A positive skewness indicates a tail to the right or alternatively a median smaller than $\overline{x}$. A negative skewness indicates a median larger than $\overline{x}$.
        }
        \newdef{Pearson's mode skewness}{
        	\begin{equation}
			\label{statistics:pearsons_skewness}
		        \gamma_P = \stylefrac{\overline{x} - \text{mode}}{\sigma}
		\end{equation}
        }
        
        \newdef{Kurtosis}{\index{kurtosis}
        	The kurtosis $c$ is an indicator for the "tailedness". It is defined in relation to the fourth central moment $m_4$:
	        \begin{equation}
			\label{statistics:kurtosis}
        	        m_4 = c\sigma^4
		\end{equation}
        }
        \newdef{Excess kurtosis}{
		The excess kurtosis is defined as $c-3$. This fixes the excess kurtosis of all univariate normal distributions at 0. A positive excess is an indicator for long "fat" tails, a negative excess indicates short "thin" tails.
	}
        
        \newdef{Percentile}{\index{percentile}
        	The $p^{th}$ percentile $c_p$ is defined as the value that is larger than $p\%$ of the measurements. The median is the $50^{th}$ percentile.
        }
        
        \newdef{Interquartile range}{
        	The interquartile range is the difference between the upper and lower quartile ($75^{th}$ and $25^{th}$ percentile respectively).
        }
        
        \newdef{FWHM}{\index{FWHM}
        	The \textbf{Full Width at Half Maximum} is the difference between the two values of the independent variable where the dependent variable is half of its maximum.
        }
        \begin{property}
        	For Gaussian distributions the following relation exists between the FWHM and the standard deviation $\sigma$:
         	\begin{equation}
            		\text{FWHM} = 2.35\sigma
	        \end{equation}
        \end{property}
        
\subsection{Multivariate datasets}

        When working with bivariate (or even multivariate) distributions it is useful to describe the relationship between the different random variables. The following two definitions are often used.
        
        \newdef{Covariance}{\index{covariance}
        	Let $X, Y$ be two random variables. The covariance of $X$ and $Y$ is defined as follows:
	        \begin{equation}
			\label{statistics:covariance}
        	        \text{cov}(X,Y) = \stylefrac{1}{N}\sum_{i=1}^N(x_i-\overline{x})(y_i - \overline{y}) = \overline{xy} - \overline{x}\ \overline{y}
		\end{equation}
		The covariance of $X$ and $Y$ is often denoted by $\sigma_{XY}$.
        }
	\begin{formula}
		The covariance and standard deviation are related by the following equality:
		\begin{equation}
			\sigma_X^2 = \sigma_{XX}
		\end{equation}
	\end{formula}

        \newdef{Correlation coefficent}{\index{correlation}
        	\begin{equation}
			\label{statistics:correlation_coefficient}
        	        \rho_{XY} = \stylefrac{\text{cov}(X,Y)}{\sigma_X\sigma_Y}
		\end{equation}
		The correlation coefficient is bounded to the interval $[-1,1]$. It should be noted that its magnitude is only an indicator for the linear dependence.
        }
        \begin{remark}
        	For multivariate distributions the above definitions can be generalized using matrices:
	        \begin{equation}
			\label{statistics:covariance_matrix}
        	        V_{ij} = \text{cov}(x_{(i)}, x_{(j)})
		\end{equation}
        	\begin{equation}
			\label{statistics:correlation_matrix}
                	\rho_{ij} = \rho_{(i)(j)}
		\end{equation}
	        where $\text{cov}(x_{(i)}, x_{(j)})$ and $\rho_{(i)(j)}$ are defined using equations \ref{statistics:covariance} and \ref{statistics:correlation_coefficient}. The following general equality exists:
	        \begin{equation}
        	    	\label{statistics:general_variance_formula}
			V_{ij} = \rho_{ij}\sigma_i\sigma_j
		\end{equation}
        \end{remark}

\section{Law of large numbers}

	\begin{theorem}[Law of large numbers]\index{Large numbers}\label{statistics:theorem:large_numbers}
		If the size $N$ of a sample tends towards infinity, then the observed frequencies tend towards the theoretical propabilities.
	\end{theorem}
	\begin{result}[Frequentist probability\footnotemark]
		\footnotetext{Also called the \textbf{empirical probability}.}
		\begin{equation}
			\label{statistics:frequentist_probability}
		        P(X) = \lim_{n\rightarrow\infty}\stylefrac{f_n(X)}{n}
		\end{equation}
	\end{result}

\section{probability densities}

	\begin{remark*}
		In the following sections and subsections, all distributions will be taken to be continuous. The formulas can be modified for use with discrete distributions by replacing the integral with a summation.
	\end{remark*}
    
	\newdef{probability density functions p.d.f}{\index{pdf|see{probability density function}}\index{probability density function}
    		Let $X$ be a random variable and $P(X)$ the associated probability distribution. The p.d.f. $f(X)$ is defined as follows:
	        \begin{equation}
			\label{statistics:pdf}
        		\boxed{P(x_1\leq X \leq x_2) = \int_{x_1}^{x_2}f(X)dX}
		\end{equation}
		An alternative definition\footnotemark\ is the following:
	        \begin{equation}
			\label{statistics:pdf_derivative}
        		f(X) = \lim_{\delta x\rightarrow 0} \stylefrac{P(x\leq X\leq x+\delta x)}{\delta x}
		\end{equation}
		\footnotetext{A more formal definition uses \textit{measure theory} and the \textit{Radon-Nikodym derivative}.}
	}
	
	\newdef{Cumulative distribution function c.d.f.}{\index{cdf|see{cumulative distribution function}}\index{cumulative distribution function}
    		Let $X$ be a random variable and $f(X)$ the associated p.d.f. The cumulative distribution function $F(X)$ is defined as follows: 
        	\begin{equation}
			\label{statistics:cdf}
        		F(x) = \int_{-\infty}^xf(X)dX
		\end{equation}
	}
	\begin{theorem}
		Let $X$ be a random variable. Let $P(X)$ and $F(X)$ be the associated p.d.f. and c.d.f. Using standard calculus the following equality can be proven:
        	\begin{equation}
			P(x_1\leq X\leq x_2) = F(x_2) - F(x_1)
		\end{equation}
	\end{theorem}
	\begin{theorem}
		$F(X)$ is continuous if and only if $P_X(\{y\}) = 0$ for every $y\in\mathbb{R}$.
	\end{theorem}
    
	\begin{remark}[Normalization]\index{normalization}
		\begin{equation}
        		\label{statistics:normalization}
        		F(\infty) = 1
		\end{equation}
	\end{remark}

	\begin{formula}
		The $p^{th}$ percentile $c_p$ can be computed as follows\footnote{This is clear from the definition of a percentile, as this implies that $F(c_p) = p$.}:
	        \begin{equation}
			c_p = F^{-1}(p)
		\end{equation}
	\end{formula}
    
	\newdef{Parametric family}{\index{parametric family}
    		A parametric family of probability densities $f(X;\vec{\theta})$ is a set of densities described by one or more parameters $\vec{\theta}$.
	}

\subsection{Function of a random variable}\index{random variable}
	\begin{formula}
    	Let $X$ be random variable and $f(X)$ the associated p.d.f. Let $a(X)$ be a function of $X$. The random variable $A = a(X)$ has an associated p.d.f. $g(A)$. If the function $a(x)$ can be inverted, then $g(A)$ can be computed as follows:
        \begin{equation}
			\label{statistics:function_of_random_variable}
            \boxed{g(a) = f(x(a))\left|\deriv{x}{a}\right|}
		\end{equation}
    \end{formula}
    
\subsection{Multivariate distributions}
	\sremark{In this section all defintions and thereoms will be given for bivariate distributions, but can be easily generalized to more random variables.}
    
	\newdef{Joint density}{\index{probability density function!joint pdf}
    	Let $X, Y$ be two random variables. The joint p.d.f. $f_{XY}(x, y)$ is defined as follows:
        \begin{equation}
			\label{statistics:joint_pdf}
            f_{XY}(x, y)dxdy = \left\{
            \begin{array}{r}
				f_x(x\in[x, x+dx])\\
				f_y(y\in[y, y+dy])
			\end{array}\right.
		\end{equation}
    }
    \remark{As $f_{XY}$ is a probability density, the normalization condition \ref{statistics:normalization} should be fulfilled.}
    
    \newdef{Conditional density}{\index{probability density function!conditional pdf}
    	The conditional p.d.f. of $X$ when $Y$ has the value y is given by the following formula:
        \begin{equation}
			\label{statistics:conditional_pdf}
            g(x|y) = \stylefrac{f_{XY}(x,y)}{f_Y(y)}
		\end{equation}
        where we should pay attention to the remark made when we defined \ref{prop:formal_conditional}.
    }
    \result{If $X$ and $Y$ are independent, then by remark \ref{prop:remark_independence} the marginal p.d.f is equal to the conditional p.d.f.}
    
    \begin{theorem}[Bayes' theorem]\index{Bayes!theorem}
		The conditional p.d.f. can be computed without prior knowledge of the joint p.d.f:
        \begin{equation}
			\label{statistics:theorem:bayes}
            \boxed{g(x|y) = \stylefrac{h(y|x)f_X(x)}{f_Y(y)}}
		\end{equation}
	\end{theorem}
    \sremark{This theorem is the statistical (random variable) analogon of theorem \ref{prop:theorem:bayes}.}
    
    \begin{formula}
    	Let $Z = XY$ with $X, Y$ two independent random variables. The distribution $f(z)$ is given by
        \begin{equation}
        	f(z) = \int_{-\infty}^{+\infty}g(x)h(z/x)\frac{dx}{|x|} = \int_{-\infty}^{+\infty}g(z/y)h(y)\frac{dy}{|y|}
        \end{equation}
    \end{formula}
    \begin{result}
    	Taking the Mellin transform \ref{transforms:mellin} of both the positive and negative part of the above integrand (to be able to handle the absolute value) gives following relation
        \begin{equation}
        	\mathcal{M}\{f\} = \mathcal{M}\{g\}\mathcal{M}\{h\}
        \end{equation} 
    \end{result}
    \begin{formula}
    	Let $Z = X + Y$ with $X, Y$ two independent random variables. The distribution $f(z)$ is given by the convolution of $g(x)$ and $h(y)$:
        \begin{equation}
        	f(z) = \int_{-\infty}^{+\infty}g(x)h(z-x)dx = \int_{-\infty}^{+\infty}g(z-y)h(y)dy
        \end{equation}
    \end{formula}

\subsection{Important distributions}
	
	\newformula{Uniform distribution}{
    	\begin{equation}\index{Distribution!uniform}
			\label{statistics:uniform_distr}
            \boxed{P(x;a,b) = \left\{
            \begin{array}{lcr}
				\stylefrac{1}{b-a}&&a\leq x\leq b\\
                0&&\text{elsewhere}
			\end{array}\right.}
		\end{equation}
        \begin{equation}
        	E(x) = \stylefrac{a+b}{2}
		\end{equation}
        \begin{equation}
        	V(x) = \stylefrac{(b-a)^2}{12}
		\end{equation}
    }
    
	\begin{formula}[Normal distribution]\index{Distribution!normal}
		Also called the Gaussian distribution.
        \begin{equation}
			\label{statistics:normal_distr}
            \boxed{\mathcal{G}(x;\mu, \sigma) = \stylefrac{1}{\sqrt{2\pi}\sigma}e^{-\stylefrac{(x-\mu)^2}{2\sigma^2}}}
		\end{equation}
	\end{formula}
    \newformula{Standard normal distribution}{
    	\begin{equation}
			\label{statistics:standard_normal_distr}
            \boxed{\mathcal{N}(z) = \stylefrac{1}{\sqrt{2\pi}}e^{-\stylefrac{z^2}{2}}}
		\end{equation}
    }
    \remark{Every Gaussian distribution can be rewritten as a standard normal distribution by setting $Z = \stylefrac{X-\mu}{\sigma}$.}
    \remark{The c.d.f. of the standard normal distribution is given by the error function: $F(z) = \text{Erf}(z)$.}
    
    \begin{formula}[Exponential distribution]\index{Distribution!exponential}
        \begin{equation}
			\label{statistics:exponential_distr}
            \boxed{P(x;\tau) = \stylefrac{1}{\tau}e^{-\stylefrac{x}{\tau}}}
		\end{equation}
        \begin{equation}
        	E(x) = \tau
		\end{equation}
        \begin{equation}
        	V(x) = \tau^2
		\end{equation}
	\end{formula}
    \begin{theorem}
		The exponential distribution is memoryless:
        \begin{equation}
			\label{statistics:theorem:memoryless_exponential_distribution}
            P(X>x_1+x_2|X>x_2) = P(X>x_1)
		\end{equation}
	\end{theorem}
    
    \newformula{Bernoulli distribution}{\index{Distribution!Bernoulli}
    	A radnom variable that can only take 2 possible values is described by a Bernoulli distribution. When the possible values are 0 and 1, with respective chances $p$ and $1-p$, the distribution is given by:
        \begin{equation}
			\label{statistics:bernoulli_distr}
            \boxed{P(x;p) = p^x(1-p)^{1-x}}
		\end{equation}
        \begin{equation}
        	E(x) = p
		\end{equation}
        \begin{equation}
        	V(x) = p(1-p)
		\end{equation}
    }
    
    \newformula{Binomial distribution}{\index{Distribution!binomial}
    	A process with $n$ identical independent trials, all Bernoulli processes $P(x; p)$, is described by a binomial distribution:
        \begin{equation}
			\label{statistics:binomial_distr}
            \boxed{P(r;p,n) = p^r(1-p)^{n-r}\stylefrac{n!}{r!(n-r)!}}
		\end{equation}
        \begin{equation}
        	E(r) = np
		\end{equation}
        \begin{equation}
        	V(r) = np(1-p)
		\end{equation}
    }
    
    \begin{formula}[Poisson distribution]\index{Distribution!Poisson}
    	A process with known possible outcomes but an unknown number of events is described by a Poisson distribution $P(r;\lambda)$ with $\lambda$ the average expected number of events.
        \begin{equation}
			\label{statistics:poisson_distr}
            \boxed{P(r;\lambda) = \stylefrac{e^{-\lambda}\lambda^r}{r!}}
		\end{equation}
        \begin{equation}
        	E(r) = \lambda
		\end{equation}
        \begin{equation}
        	V(r) = \lambda
		\end{equation}
	\end{formula}
    \begin{theorem}
		If two Poisson processes $P(r;\lambda_a)$ and $P(r;\lambda_b)$ occur simultaneously and if there is no distinction between the two, then the probability of $r$ events is also described by a Poisson distribution with average $\lambda_a+\lambda_b$. 
	\end{theorem}
    \result{The number of events coming from $A$ is given by a binomial distribution $P(r_a;\Lambda_a, r)$ where $\Lambda_a = \stylefrac{\lambda_a}{\lambda_a + \lambda_b}$.}
    \remark{For large values of $\lambda$ ($\lambda\rightarrow\infty$), the Poisson distribution $P(r;\lambda)$ can be approximated by a Gaussian distribution $\mathcal{G}(x;\lambda,\sqrt{\lambda})$.
    
    \begin{formula}[$\chi^2$ distribution]\index{Distribution!$\chi^2$}
    	The sum of $k$ squared independent (standard) normally distributed random variables $Y_i$ defines the random variable:
        \begin{equation}
			\chi^2_k = \sum_{i=1}^kY_i^2
		\end{equation}
        where $k$ is said to be the number of \textbf{degrees of freedom}.
        \begin{equation}
			\label{statistics:chi_squared_distr}
            \boxed{P(\chi^2;n) = \stylefrac{\chi^{n-2}e^{-\stylefrac{\chi^2}{2}}}{2^{\frac{n}{2}}\Gamma\left(\stylefrac{n}{2}\right)}}
		\end{equation}
	\end{formula}
    \remark{Due to the CLT the $\chi^2$ distribution approximates a Guassian distribution for large $k$: $P(\chi^2;k)\xrightarrow{k>30}\mathcal{G}(\sqrt{2\chi^2};\sqrt{2k-1},1)$
    
    \newformula{Student-t distribution}{\index{Distribution!Student-t}
    	The student-t distribution describes a sample with estimated standard deviation $\hat{\sigma}$. 
    	\begin{equation}
			\label{statistics:student_t_distr}
            \boxed{P(t;n) = \stylefrac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\ \Gamma\left(\frac{n}{2}\right)\left(1 + \frac{t^2}{n}\right)^{\frac{n+1}{2}}}}
		\end{equation}
        where
        \begin{equation}
        	t = \stylefrac{(x-\mu) / \sigma}{\hat{\sigma}/\sigma} = \stylefrac{z}{\sqrt{\chi^2/n}}
		\end{equation}
    }
    \sremark{The significance of a difference between the sample mean $\overline{x}$ and the true mean $\mu$ is smaller due to the (extra) uncertainty of the estimated standard deviation.}
    
    \newformula{Cauchy distribution\footnotemark}{\index{Cauchy!distribution}\index{Breit-Wigner|see{Cauchy distribution}}
    	\footnotetext{Also known (especially in particle physics) as the \textbf{Breit-Wigner} distribution.}
    	The general form $f(x; x_0, \gamma)$ is given by:
    	\begin{equation}
    		\label{stat:cauchy_distribution}
    		f(x; x_0, \gamma) = \frac{1}{\pi}\frac{\gamma}{(x - x_0)^2 + \gamma^2}
    	\end{equation}
    	The characteristic function \ref{prop:characteristic_function} is given by:
    	\begin{equation}
    		E\left(e^{itx}\right) = e^{ix_0t - \gamma|t|}
    	\end{equation}
    }
    \begin{property}
    	Both the mean and variance of the Cauchy distribution are undefined.
    \end{property}

\section{Central limit theorem (CLT)}
    \begin{theorem}[Central limit theorem]\index{CLT|see{Central limit theorem}}\index{Central limit theorem}
    	\label{statistics:theorem:CLT}
		A sum of $n$ independent random variables $X_i$ has the following properties:
        \begin{enumerate}
			\item $\mu = \sum_i\mu_i$
            \item $V(X) = \sum_iV_i$
            \item The sum will be approximately (!!) normally distributed. 
		\end{enumerate}
	\end{theorem}
    \begin{remark}
		If the random variables are not independent, property 2 will not be fulfilled.
	\end{remark}
    \remark{The sum of Gaussians will be Gaussian to.}

\subsection{Distribution of sample mean}\index{Distribution}
	The difference between a sample mean $\overline{x}$ and the true mean $\mu$ is described by a distribution with following mean and variance:
	\begin{property}
		\begin{equation}
			<\overline{x}> = \mu
		\end{equation}
	\end{property}
    \begin{property}
		\begin{equation}
        	\label{statistics:variance_sample_mean}
			V(\overline{x}) = \stylefrac{\sigma^2}{N}
		\end{equation}
	\end{property}
    
\section{Errors}

\subsection{Different measurement types}
	When performing a sequence of measurements $x_i$ with different variances $\sigma_i^2$, it is impossible to use the arithmetic mean \ref{statistics:arithmetic_mean} in a meaningful way because the measurements are not of the same type. Therefore it is also impossible to apply the CLT \ref{statistics:theorem:CLT}.
	
    \newdef{Weighted mean}{\index{mean}
    	The appropriate  alternative is the \textit{weighted mean}:
    	\begin{equation}
			\label{statistics:weighted_mean}
            \boxed{\overline{x} = \stylefrac{\sum_i\frac{x_i}{\sigma_i^2}}{\sum_i\frac{1}{\sigma_i^2}}}
		\end{equation}
        }
        The resolution of the weighted mean is given by:
        \begin{equation}
            \label{statistics:weighted_mean_variance}
            V(\overline{x}) = \stylefrac{1}{\sum_i\sigma_i^{-2}}
        \end{equation}
        
\subsection{Propagation of errors}
	\begin{formula}\index{variance}
    	Let $X$ be random variable with variance $V(x)$. The variance of a linear function $f(X) = aX + b$ is given by:
        \begin{equation}
			\label{statistics:variance_linear_function}
            V(f) = a^2V(x)
		\end{equation}
    \end{formula}
    \begin{formula}
    	Let $X$ be random variable with \textbf{small} (!!) variance $V(x)$. The variance of a general function $f(X)$ is given by:
        \begin{equation}
			\label{statistics:variance_general_function}
            V(f) \approx \left(\deriv{f}{x}\right)^2V(x)
		\end{equation}
    \end{formula}
    \result{The correlation coefficient $\rho$ (\ref{statistics:correlation_coefficient}) of a random variable $X$ and a \textbf{linear} function of $X$ is independent of $\sigma_x$ and is always equal to $\pm1$.}
    
    \newformula{Law of error propagation}{\index{propagation}
    	Let $\vec{X}$ be a set of random variables with \textbf{small} variances. The variance of a general function $f(\vec{X})$ is given by:
        \begin{equation}
			\label{statistics:error_propagation}
            \boxed{V(f) = \sum_p\left(\pderiv{f}{X_{(p)}}\right)^2V(X_{(p)}) + \sum_p\sum_{q\neq p}\left(\pderiv{f}{X_{(p)}}\right)\left(\pderiv{f}{X_{(q)}}\right)\text{cov}(X_{(p)}, X_{(q)})}
		\end{equation}
    }
    
    \newdef{Fractional error}{\index{fractional error}
    	Let $X, Y$ be two \textbf{independent} random variables. The standard deviation of $f(x, y) = xy$ is given by the fractional error:
        \begin{equation}
			\label{statistics:fractional_error}
            \left(\stylefrac{\sigma_f}{f}\right)^2 = \left(\stylefrac{\sigma_x}{x}\right)^2 + \left(\stylefrac{\sigma_y}{y}\right)^2
		\end{equation}
    }
    \remark{The fractional error of quantity is equal to the fractional error of the reciprocal of that quantity.}
    \begin{property}
		Let $X$ be a random variable. The error of the logarithm of $X$ is equal to the fractional error of $X$.
	\end{property}
    
    \newdef{Covariance of functions}{\index{covariance}
    	\begin{equation}
			\label{statistics:covariance_functions}
            \text{cov}(f_1, f_2) = \sum_p\sum_q\left(\pderiv{f_1}{X_{(p)}}\right)\left(\pderiv{f_2}{X_{(q)}}\right)\text{cov}(X_{(p)}, X_{(q)})
		\end{equation}
    }
    \result{Let $\vec{f} = \{f_1, ..., f_k\}$. The covariance matrix $V_f$ of the $k$ functions is given by:
    	\begin{equation}
			V_f = G\ V_X\ G^T
		\end{equation}
        where $G$ is the Jacobian matrix of $\vec{f}$.
    }

\subsection{Systematic errors}\index{systematic error}
	Systematic errors are errors that always have the same influence (they shift all values in the same way), that are not independent of eachother and that cannot be directly inferred from the measurements.

\section{Estimators}
	\newdef{Estimator}{\index{estimator}
    	An estimator is a procedure that, given a sample, produces a numerical value for a property of the parent population.
    }

\subsection{General properties}
	\newprop{Consistency}{\index{consistency}
    	\begin{equation}
	        \label{statistics:consistency}
            \boxed{\lim_{N\rightarrow\infty}\hat{a} = a}
		\end{equation}
	}
    \newprop{Unbiased estimator}{\index{unbiased estimator|see{bias}}
    	\begin{equation}
	        \label{statistics:unbiased_estimator}
            \boxed{<\hat{a}> = a}
		\end{equation}
    }
    \newdef{Bias}{\index{bias}
    	\begin{equation}
	        \label{statistics:bias}
            B(\hat{a}) = |<\hat{a}> - a|
		\end{equation}
    }
    \newprop{Efficiency}{\index{efficiency}
    	An estimator $\hat{a}$ is said to be efficient if its variance $V(\hat{a})$ is equal to the minimum variance bound \ref{statistics:theorem:minimum_variance_bound}.
    }
    
    \newdef{Mean squared error}{
    	\begin{equation}
	        \label{statistics:mean_squared_error}
            \Upsilon(\hat{a}) = B(\hat{a})^2 + V(\hat{a})
		\end{equation}
    }
    \remark{If an estimator is unbiased, the MSE is equal to the variance of the estimator.}

\subsection{Fundamental estimators}
	\newprop{Mean estimator}{\index{mean!estimator}
    	The sample mean $\overline{x}$ is a consistent and unbiased estimator for the true mean $\mu$ due to the CLT. The variance $V(\overline{x})$ of the estimator is given by equation \ref{statistics:variance_sample_mean}.
    }
    
    \newprop{Variance estimator for known mean}{\index{variance!estimator}
    	If the true mean $\mu$ is known then a consistent and unbiased estimator for the variance is given by:
        \begin{equation}
			\widehat{V(x)} = \stylefrac{1}{N}\sum_{i=1}^N(x_i-\mu)^2
		\end{equation}
    }
    \newprop{Variance estimator for unknown mean}{
    	If the true mean $\mu$ is unknown and the sample mean has been used to estimate $\mu$, then a consistent and unbiased estimator is given by\footnotemark:
        \begin{equation}\index{Bessel!correction}
        	\label{statistics:variance_bessel_correction}
			s^2 = \stylefrac{1}{N-1}\sum_{i=1}^N(x_i-\overline{x})^2
		\end{equation}
    }
    \footnotetext{The modified factor $\stylefrac{1}{N-1}$ is called the Bessel correction. It corrects the bias of the estimator given by the sample variance \ref{statistics:sample_variance}. The consistency however is guaranteed by the CLT.}
    
\subsection{Estimation error}
	\newformula{Variance of estimator of variance}{
    	\begin{equation}
			V(\widehat{V(x)}) =  \stylefrac{(N-1)^2}{N^3}<(x - <x>)^4> - \stylefrac{(N-1)(N-3)}{N^3}<(x - <x>)^2>^2
		\end{equation}
    }
    \newformula{Variance of estimator of standard deviation}{
    	\begin{equation}
        	V(\hat{\sigma}) = \stylefrac{1}{4\sigma^2}V(\widehat{V(x)})
		\end{equation}
    }
    \remark{The previous result is a little odd, as one has to know the true standard deviation to compute the variance of the estimator. This problem can be solved in two ways. Or a value (hopefully close to the real one) inferred from the sample is used as an estimator or a theoretical one is used in the design phase of an experiment to see what the possible outcomes are.}

\subsection{Likelihood function}
	\newdef{Likelihood}{\index{likelihood}
    	\label{likelihood}
    	The likelihood $\mathcal{L}(a;\vec{x})$ is the probability to find a set of measurements $\vec{x} = \{x_1,...,x_N\}$ given a distribution $P(X;a)$:
        \begin{equation}
			\label{statistics:likelihood}
            \mathcal{L}(a;\vec{x}) = \prod_{i=1}^NP(x_i;a)
		\end{equation}
    }
    \newdef{Log-likelihood}{
    	\begin{equation}
			\label{statistics:log_likelihood}
            \log\mathcal{L}(a;\vec{x}) = \sum_i\ln P(x_i;a)
		\end{equation}
    }
    
    \begin{property}
    	The expectation value of an estimator $\hat{}$ is given by:
        \begin{equation}
			<\hat{a}> = \int \hat{a}\mathcal{L}(\hat{a};X)dX
		\end{equation}
    \end{property}
    
    \begin{theorem}[Minimum variance bound]\index{Minimum variance bound}\index{MVB|see{Minimum variance bound}}
    	The variance of an \textbf{unbiased} estimator has a lower bound: the minimum variance bound\footnotemark\ (MVB).
		\begin{equation}
			\label{statistics:theorem:minimum_variance_bound}
            \boxed{V(\hat{a})\geq\stylefrac{1}{\left<\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right>}}
		\end{equation}
        For a biased estimator with bias $b$ the MVB takes on the following form:
        \begin{equation}
			\label{statistics:theorem:biased_minimum_variance_bound}
            V(\hat{a})\geq\stylefrac{\left(1+\deriv{b}{a}\right)^2}{\left<\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right>}
		\end{equation}
	\end{theorem}
    \footnotetext{It is also known as the Cramer-Rao bound.}
    \remark{
    	\begin{equation}
			\left<\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right> = -\left<\mderiv{2}{\ln\mathcal{L}}{a}\right>
		\end{equation}
    }
    \newdef{Fisher information}{\index{Fisher!information}
    	\begin{equation}
			\label{statistics:fisher_information}
            I_X(a) = \left<\left(\deriv{\ln\mathcal{L}}{a}\right)^2\right> = N\int\left(\deriv{\ln P}{a}\right)^2PdX
		\end{equation}
    }
    
\subsection{Maximum likelihood estimator}
	Following from definition \ref{likelihood} it follows that the best estimator $\hat{a}$ is the value for which the likelihood function is maximal. It is the value that makes the measurements the most propable, but it is therefore not the most propable estimator.
    
    \newmethod{Maximum likelihood estimator}{\index{likelihood!estimator}
    	The maximum likelihood estimator $\hat{a}$ is obtained by solving following equation:
    	\begin{equation}
			\label{statistics:maximum_likelihood_estimator}
            \left.\deriv{\ln\mathcal{L}}{a}\right|_{a=\hat{a}} = 0
		\end{equation}
    }
    \remark{MLH estimators are mostly consistent but often biased.}
    \begin{property}
		MLH estimators are invariant under parameter transformations.
	\end{property}
    \result{The invariance implies that the two estimators $\hat{a}$ and $\widehat{f(a)}$ cannot both be unbiased at the same time.}
    
    \begin{property}
		Asymptotically ($N\rightarrow\infty$) every \textbf{consistent} estimator becomes unbiased and efficient.
	\end{property}

\subsection{Least squares}
	\newmethod{Least squares}{$ $\index{Least squares}
    	\begin{enumerate}
			\item Fitting a function $y = f(x;a)$ to a set of 2 variables $(x, y)$ where the $x$ values are exact and the $y$ valiues have an uncertainty $\sigma_i$ to estimate the value $a$.
            \item For every event $(x_i, y_i)$ define the residual $d_i = y_i - f(x_i;a)$.
            \item Determine (analytically)  the $\chi^2$ value: $\chi^2 = \sum_i\left[\stylefrac{d_i}{\sigma_i}\right]^2$
            \item Find the most propably value of $\hat{a}$ by solving the equation $\deriv{\chi^2}{a} = 0$.
		\end{enumerate}
    }
    \begin{property}
		The optimalized (minimized) $\chi^2$ is distributed according to a $\chi^2$ dsitribution \ref{statistics:chi_squared_distr} $P(\chi^2;n)$. The number of degrees of freedom $n$ is equal to the number of events $N$ minus the number of fitted parameters $k$.
	\end{property}
    
    \newformula{Linear fit}{\index{linear fit}
    	When all uncertainties $\sigma_i$ are equal, the slope $\hat{m}$ and intercept $\hat{c}$ are given by following formulas:
    	\begin{equation}
        	\label{statistics:least_squares_slope}
			\hat{m} = \stylefrac{\overline{xy} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2} = \stylefrac{\text{cov}(x, y)}{V(x)}
		\end{equation}
        \begin{equation}
        	\label{statistics:least_squares_intercept}
			\hat{c} = \overline{y} - \hat{m}\overline{x} = \stylefrac{\overline{x^2} - \overline{x}\ \overline{y}}{\overline{x^2} - \overline{x}^2}
		\end{equation}
    }
    \remark{The equation $\overline{y} = \hat{c} + \hat{m}\overline{x}$ means that the linear fit passes through the center of mass $(\overline{x}, \overline{y})$.}
    \newformula{Errors of linear fit}{
    	\begin{equation}
        	\label{statistics:least_squares_slope_variance}
			V(\hat{m}) = \stylefrac{1}{N(\overline{x^2} - \overline{x}^2)}\sigma^2
		\end{equation}
        \begin{equation}
        	\label{statistics:least_squares_intercept_variance}
			V(\hat{c}) = \stylefrac{\overline{x^2}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2
		\end{equation}
        \begin{equation}
        	\label{statistics:least_squares_linear_fit_covariance}
			\text{cov}(\hat{m}, \hat{c}) = \stylefrac{-\overline{x}}{N(\overline{x^2} - \overline{x}^2)}\sigma^2
		\end{equation}
    }
    \remark{When there are different uncertainties $\sigma_i$, the arithmetic means have to be replaced with weighted means, but the expressions remain the same. The quantity $\sigma^2$ has to be replaced by its weighted variant:
    	\[\overline{\sigma^2} = \stylefrac{\sum\sigma_i^2/\sigma_i^2}{\sum\sigma_i^{-2}} = \stylefrac{N}{\sum\sigma_i^{-2}}\]
    }

\subsection{Binned least squares}
	The least squares method is very useful to fit data which has been grouped in bins (histograms).
	\newmethod{Binned least squares}{$ $\index{Least squares}
    	\begin{enumerate}
			\item N events with distributions $P(X;a)$ divided in $N_B$ intervals. Interval $j$ is centered on the value $x_j$, has a width $W_j$ and contains $n_j$ events.
            \item The ideally expected number of events in the $j^{th}$ interval: $f_j = NW_jP(x_j;a)$
            \item The real number of events has a Poisson distribution: $\overline{n}_j = \sigma_j^2 = f_j$
            \item Define the binned $\chi^2$ as: $\chi^2 = \displaystyle\sum_i^{N_B}\stylefrac{(n_i - f_i)^2}{f_i^2}$
		\end{enumerate}
    }
    
\section{Confidence}\index{confidence}
	The real value of a parameter $\varepsilon$ can never be known exactly. But it is possible to construct an interval $I$ in which the real value should lie with a certain confidence $C$.
	\begin{example}
		Let $X$ be a random variable with distribution $\mathcal{G}(x;\mu, \sigma)$.
        The measurement $x$ lies in the interval $[\mu - 2\sigma;\mu+2\sigma]$ with 95\% \textbf{chance}.
        The real value $\mu$ lies in the interval $[x - 2\sigma;x+2\sigma]$ with 95\% \textbf{confidence}.
	\end{example}
    \sremark{In the previous example there are some Bayesian assumptions: all possible values (left or right side of peak) are given the same possiblity due to the Gaussian distribution, but if one removes the symmetry it is mandatory to use a more careful approach. The symmetry between uncertainties $\sigma$ and confidence levels is only valid for Gaussian distributions.}
    
\subsection{Interval types}
    \newdef{Two-sided confidence interval}{
    	\begin{equation}
			\label{statistics:two_sided_interval}
            \boxed{P(x_-\leq X\leq x_+) = \int_{x_-}^{x_+}P(x)dx = C}
		\end{equation}
        There are three possible (often used) two-sided intervals:
        \begin{itemize}
			\item Symmetric interval: $x_+ - \mu = \mu - x_-$
            \item Shortest interval: $|x_+ - x_-|$ is minimal
            \item Central interval: $\int_{-\infty}^{x_-}P(x)dx = \int_{x_+}^\infty P(x)dx = \stylefrac{1-C}{2}$
		\end{itemize}
    }
    \remark{For Gaussian distributions these three definitions are equivalent.}
    \sremark{The central interval is the (best and) most widely used confidence interval.}
    
    \newdef{One-sided confidence interval}{
    	\begin{equation}
			\label{statistics:one_sided_interval1}
            \boxed{P(x \geq x_-) = \int_{x_-}^{+\infty}P(x)dx = C}
		\end{equation}
        \begin{equation}
			\label{statistics:one_sided_interval2}
            \boxed{P(x \leq x_+) = \int_{-\infty}^{x_+}P(x)dx = C}
		\end{equation}
    }
    
    \remark{For a discrete distribution it is often impossible to find integers $x_{\pm}$ such that the real value lies with exact confidence $C$ in the interval $[x_-;x_+]$.}
    \newdef{Discrete central confidence interval}{
    	\begin{equation}
        	\label{statistics:central_discreteInterval_lower_bound}
			x_- = \max_\theta\left[\sum_{x=0}^{\theta - 1}P(x;X)\right]\leq\stylefrac{1-C}{2}
		\end{equation}
        \begin{equation}
        	\label{statistics:central_discreteInterval_upper_bound}
			x_+ = \min_\theta\left[\sum_{x=\theta + 1}^{+\infty}P(x;X)\right]\leq\stylefrac{1-C}{2}
		\end{equation}
    }

\subsection{General construction}
	For every value of the true parameter $X$ it is possible to construct a confidence interval. This leads to the construction of the two functions $x_-(X)$ and $x_+(X)$. The 2D diagram obtained by plotting $x_-(X)$ and $x_+(X)$ with the $x$-axis horizontally and $X$-axis vertically is called the confidence region.
    \begin{theorem}
		Let $x_0$ be the measured value of a parameter $X$. From the confidence region, it is possible to infere a confidence interval $[X_-(x);X_+(x)]$. The upper limit $X_+$ is not the limit such that there is only a $\stylefrac{1-C}{2}$ chance of having a true parameter $X\geq X_+$, but it is the limit such that if the true parameter $X\geq X_+$ then there is a chance of $\stylefrac{1-C}{2}$ to have a measurement $x_0$ or smaller.
	\end{theorem}
    
\subsection{Extra conditions}
	\newmethod{Bayesian statistics}{
    	\begin{equation}
			p(\text{theory}|\text{result}) = p(\text{result}|\text{theory})\stylefrac{p(\text{theory})}{p(\text{result})}
		\end{equation}
        \indent or more mathematically:
        \begin{equation}
        	\label{statistics:confidence_bayesian_statistics}
			\boxed{p(X|x) = p(x|X)\stylefrac{p(X)}{p(x)}}
		\end{equation}
        
        \begin{itemize}
        	\item The denominator $p(\text{result})$ does not play a real role, it is a normalization constant.
        	\item The probability $p(x|X)$  to have measurement $x$ when the true parameter is $X$ is a Gaussian distribution $\mathcal{G}(x;X,\sigma)$
        \end{itemize}
    }
    \remark{If nothing is known about the theory, $p(X)$ is (exaggerated assumption) a uniform probability \ref{statistics:uniform_distr}.}
    
\subsection{Interval for a sample mean}
	\newformula{Interval with known variance}{
    	If the sample size is large enough, the real distribution is unimportant, because the CLT ensures a Gaussian distribution of the sample mean $\overline{X}$. The $\alpha$-level confidence interval such that $P(-z_{\alpha/2} < Z < z_{\alpha/2})$ with $Z = \stylefrac{\overline{X} - \mu}{\sigma/\sqrt{N}}$ is given by:
    	\begin{equation}
			\left[\overline{X} - z_{\alpha/2}\stylefrac{\sigma}{\sqrt{N}};\overline{X} + z_{\alpha/2}\stylefrac{\sigma}{\sqrt{N}}\right]
		\end{equation}
    }
    \remark{If the sample size is not sufficiently large, the measured quantity must follow a normal distribution.}
    
    \newformula{Interval with unknown variance}{
    	To account for the uncertainty of the estimated standard deviation $\hat{\sigma}$, the student-t distribution \ref{statistics:student_t_distr} is used instead of a Gaussian distribution to describe the sample mean $\overline{X}$. The $\alpha$-level confidence interval is given by:
        \begin{equation}
			\left[\overline{X} - t_{\alpha/2;(n-1)}\stylefrac{s}{\sqrt{N}};\overline{X} + t_{\alpha/2;(n-1)}\stylefrac{s}{\sqrt{N}}\right]
		\end{equation}
        where $s$ is the estimated standard deviation \ref{statistics:variance_bessel_correction}.
    }
    
    \newformula{Wilson score interval}{\index{Wilson score interval}
    	For a sufficiently large sample, a sample proportion $\hat{P}$ is approximately Gaussian distributed with expectation value $\pi$ and variance $\frac{\pi(\pi-1)}{N}$. The $\alpha$-level confidence interval is given by:
        \begin{equation}
			\left[\stylefrac{(2N\hat{P} + z^2_{\alpha/2}) - z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})};\stylefrac{(2N\hat{P} + z^2_{\alpha/2}) + z_{\alpha/2}\sqrt{z^2_{\alpha/2} + 4N\hat{P}(1 - \hat{P})}}{2(N + z^2_{\alpha/2})}\right]
		\end{equation}
    }
    \sremark{The expectation value and variance are these of a binomial distribution \ref{statistics:binomial_distr} with $r = X/N$.}

\subsection{Confidence region}

\section{Hypotheses and testing}

\subsection{Hypothesis}\index{hypothesis}
	\newdef{Simple hypothesis}{
    	A hypothesis is called simple if the distribution is fully specified.
    }
    \newdef{Composite hypothesis}{
    	A hypothesis is called composite if the distribution is given relative to some parameter values.
    }
    
    \newdef{Null hypothesis $H_0$}{
		
	}
    \newdef{Alternative hypothesis $H_1$}
    
\subsection{Testing}
	\newdef{Type I error}{\index{error}
    	Rejecting a true null hypothesis.
    }
    \newdef{Type II error}{
    	Accepting/retaining a false null hypothesis.
    }
    
    \newdef{Significance}{\index{significance}
    	The probability of making a type I error:
        \begin{equation}
			\label{statistics:significance}
            \boxed{\alpha = \int P_{H_0}(x)dx}
		\end{equation}
    }
    \begin{property}
    	Let $a_1 > a_2$. An $a_2$-level test is also significant at the $a_1$-level.
	\end{property}
    \remark{For discrete distributions it is not always possible to achieve an exact level of significance.}
    \sremark{Type I errors occur occasionally. They cannot be prevented, they should however be controlled.}
    
    \newdef{Power}{\index{power}
    	The probability of not making a type II error:
        \begin{equation}
			\label{statistics:power}
            \boxed{\beta = \int P_{H_1}(x)dx\qquad\rightarrow\qquad\text{power: }1-\beta}
		\end{equation}
    }

	\begin{theorem}
    	A good test is a test with a small significance and a large power. The propabilities $P_{H_0}(x)$ and $P_{H_1}(x)$ should be as different as possible.
    \end{theorem}
    
    \begin{theorem}[Neyman-Pearson test]\index{Neyman-Pearson test}
    	The following test is the most powerful test at significance level $\alpha$ for a threshold $\eta$:
    	
        The null hypothesis $H_0$ is rejected in favour of the alternative hypothesis $H_1$ if the likelihood ratio $\Lambda$ satisfies the following condition: 
        \begin{equation}\index{likelihood!ratio}
			\label{statistics:likelihood_ratio}
            \Lambda(x) = \stylefrac{L(x|H_0)}{L(x|H_1)} \leq \eta
		\end{equation}
        where $P(\Lambda(x)\leq\eta|H_0) = \alpha $
    \end{theorem}
    \sremark{In some references the reciprocal of $\Lambda(x)$ is used as the definition of the likelihood ratio.}
    
\subsection{Confindence intervals and decisions}

\section{Goodness of fit}\index{goodness of fit}
	
	Let $f(x|\vec{\theta})$ be the fitted function with $N$ measurements.
	
\subsection{\texorpdfstring{$\chi^2$}{chi square}-test}\index{Goodness of fit!$\chi^2$-test}
    	
    	\begin{formula}
		\begin{equation}
			\label{statistics:gof:chi_square}
	                \chi^2 = \sum_{i=1}^N\frac{\left[y_i - f\left(x_i\right)\right]^2}{\sigma_i^2}
		\end{equation}
	\end{formula}
        \begin{property}If there are $N - n$ fitted parameters we have:
		\begin{equation}
			\label{statistics:gof:chance_for_chi_square}
                	\int_{\chi^2}^\infty f(\chi^2|n)d\chi^2 \approx 1\implies
	                \begin{cases}
				\circ\text{ good fit}\\
	                	\circ\text{ errors were overestimated}\\
	                	\circ\text{ selected measurements}\\
	                	\circ\text{ lucky shot}
        	        \end{cases}
		\end{equation}
	\end{property}
        \begin{property}[Reduced chi-squared $\chi^2_{\text{red}}$]
		Define the reduced chi-squared value as follows: $\chi^2_{\text{red}} = \chi^2/n$ where $n$ is the number of degrees of freedom.
		\begin{itemize}
                	\item $\chi^2_{\text{red}} >> 1$: Poor modelling.
                	\item $\chi^2_{\text{red}} > 1$: Bad modelling or underestimation of the uncertainties.
                	\item $\chi^2_{\text{red}} = 1$: Good fit.
                	\item $\chi^2_{\text{red}} < 1$: Impropable, overestimation of the uncertainties. 
		\end{itemize}
	\end{property}
    
\subsection{Runs test}\index{runs test}
	
	A good $\chi^2$-test does not mean that the fit is good. As mentioned in property \ref{statistics:gof:chance_for_chi_square} it is possible that the errors were overestimated. Another condition for a good fit is that the data points vary around the fit, i.e. there are no long sequences of points that lie above/underneath the fit. (It is a result of the 'randomness' of a data sample') This condition is tested with a runs test \ref{statistics:runs_distr_even}/\ref{statistics:runs_distr_odd}.
    
	\begin{remark}
		The $\chi^2$-test and runs test are complementary. The $\chi^2$-test only takes the absolute value of the differences between the fit and data points into account, the runs test only takes the signs of the differences into account.
	\end{remark}
    
	\newformula{Runs distribution}{\index{distribution!runs}
	    	\begin{equation}
			\label{statistics:runs_distr_even}
			\boxed{P(r_{even}) = 2\stylefrac{C^{N_B - 1}_{\frac{r}{2} - 1}C^{N_O - 1}_{\frac{r}{2} - 1}}{C^N_{N_B}}}
		\end{equation}
		\begin{equation}
			\label{statistics:runs_distr_odd}
			\boxed{P(r_{odd}) = \stylefrac{C^{N_B - 1}_{\frac{r - 3}{2}}C^{N_O - 1}_{\frac{r - 1}{2}} + C^{N_O - 1}_{\frac{r - 3}{2}}C^{N_B - 1}_{\frac{r - 1}{2}}}{C^N_{N_B}}}
		\end{equation}
		\begin{equation}
			E(r) = 1 + 2\stylefrac{N_B N_O}{N}
		\end{equation}
	        \begin{equation}
			V(r) = 2\stylefrac{N_B N_O}{N}\stylefrac{2N_B N_O - 1}{N(N-1)}
		\end{equation}
	}
	\remark{For $r > 10-15$ the runs distribution approximates a Gaussian distribution.}
    
\subsection{Kolmogorov test}
	
	\newdef{Empirical distribution function}{
	    	\begin{equation}
			\label{statistics:empirical_distribution_function}
        		F_n(x) = \frac{1}{n}\sum_{i=1}^n\mathbbm{1}_{]-\infty,x]}(x_i)
		\end{equation}
	        where $\mathbbm{1}_A(x)$ is the indicator function \ref{lebesgue:indicator_function}.
	}
	\newdef{Kolmogorov-Smirnov statistic}{\index{Kolmogorov!Kolmogorov-Smirnov statistic}
    		Let $F(x)$ be a given cumulative distribution function. The $n^{th}$ Kolmogorov-Smirnov statistic is defined as:
    		\begin{equation}
			\label{statistics:kolmogorov_smirnov_statistic}
        		D_n = \sup_x|F_n(x) - F(x)|
		\end{equation}
	}
    
	\newdef{Kolmogorov distribution}{\index{Kolmogorov!distribution}
	    	\begin{equation}
			\label{statistics:kolmogorov_distribution_cumulative}
        		P(K\leq x) = 1 - 2\sum_{i=1}^{+\infty}(-1)^{i-1}e^{-2i^2x^2} = \stylefrac{\sqrt{2\pi}}{x}\sum_{i=1}^{+\infty}e^{-(2i-1)^2\pi^2/(8x^2)}
		\end{equation}
	}
    
	\newprop{Kolmogorov-Smirnov test}{
	    	Let the null hypothesis $H_0$ state that a given data sample is described by a continuous distribution $P(x)$ with cumulative distribution function $F(x)$. The null hypothesis is rejected at significance level $\alpha$ if:
        	\begin{equation}
			D_n\sqrt{n} > K_{\alpha}
		\end{equation}
        	where $K_{\alpha}$ is defined by using the Kolmogorov distribution: $P(K\leq K_{\alpha}) = 1-\alpha$
	}

\section{Eigenvectors}

    \begin{definition}[Eigenvector]\index{eigenvalue}\index{eigenvector}
        A vector $v\in V\setminus\{0\}$ is called an \textbf{eigenvector} of the linear map $f:V\rightarrow V$ if it satisfies the following equation:
       \begin{gather}
            f(v) = \lambda v
        \end{gather}
       where $\lambda\in K$ is the \textbf{eigenvalue} belonging to $v$.
    \end{definition}
    \begin{definition}[Eigenspace]
        The subspace of $V$ consisting of the zero vector and the eigenvectors of a linear map is called the eigenspace associated with that linear map. It is given by:
        \begin{gather}
            \text{ker}(\lambda\mathbbm{1}_V - f).
        \end{gather}
    \end{definition}
    \begin{remark}\label{linalgebra:eigenvalue_remark}
        From the above definitions it is clear that the eigenvalues are those scalars for which the linear map $\lambda\mathbbm{1}_V-f$ is not injective.
    \end{remark}

    \begin{theorem}[Characteristic equation]\label{linalgebra:theorem:eigenvalue_characteristic_equation}
        Let $f\in\text{End}_K(V)$ be a linear map. A scalar $\lambda\in K$ is an eigenvalue of $f$ if and only if it satisfies the characteristic equation \ref{linalgebra:characteristic_equation}.
    \end{theorem}

    \begin{theorem}
        A linear map $f\in\text{End}_K(V)$ defined over an $n$-dimensional $K$-vector space $V$ has at most $n$ different eigenvalues.
    \end{theorem}

    \begin{method}[Finding the eigenvectors of a matrix]
        To calculate the eigenvectors of a matrix one should perform the following steps:
        \begin{enumerate}
            \item First one finds the eigenvalues $\lambda_i$ of $\mathbf{A}$ by applying theorem \ref{linalgebra:theorem:eigenvalue_characteristic_equation}.
            \item Then one can find the eigenvector $v_i$ associated to the eigenvalue $\lambda_i$ by using the following equation:
                \begin{gather}
                    \label{linalgebra:eigenvectors:eigenspace}
                    \left(\mathbf{A} - \lambda_i\mathbbm{1}_V\right)v_i = 0.
                \end{gather}
        \end{enumerate}
    \end{method}

\subsection{Diagonalization}

    \newdef{Diagonalizable map}{
        Let $V$ be a finite-dimensional $K$-vector space. A linear map $f\in\text{End}_K(V)$ is diagonalizable if there exists a matrix representation $A\in M_n(K)$ of $f$ such that $A$ is a diagonal matrix.
    }
    \begin{property}\index{semisimple!operator}
        Every diagonalizable map is semisimple\footnote{See property \ref{linalgebra:jordan_chevalley}.}. Conversely, in finite dimensions (and over an algebraically closed field), a semisimple map is diagonalizable.
    \end{property}

    \begin{theorem}\label{linalgebra:theorem:diagonalizable_PQP}
        A matrix $A\in M_n(K)$ is diagonalizable if and only if there exists a matrix $P\in\emph{GL}_n(K)$ such that $P^{-1}AP$ is diagonal.
    \end{theorem}
    \begin{result}\index{trace}
        Using the fact that the trace of a linear map is invariant under similarity transformations (see property \ref{linalgebra:trace_invariance}) we get following useful formula:
        \begin{gather}
            \text{tr}(f) = \sum_{i=0}^n\lambda_i
        \end{gather}
        where $\{\lambda_i\}_{0\leq i\leq n}$ are the eigenvalues of $f$.
    \end{result}

    \begin{property}\label{linalgebra:diagonalization_properties}
        Let $V$ be an $n$-dimensional $K$-vector space and let $f\in\text{End}_K(V)$ be a linear map. We find the following properties of the eigenvectors/eigenvalues of $f$:
        \begin{enumerate}
            \item The eigenvectors of $f$ belonging to different eigenvalues are linearly independent.
            \item If $f$ has exactly $n$ eigenvalues, then $f$ is diagonalizable.
            \item If $f$ is diagonalizable, then $V$ is the direct sum of the eigenspaces of $f$ belonging to the different eigenvalues of $f$.
        \end{enumerate}
    \end{property}
    Altogether this leads to the following theorem:
    \begin{theorem}\label{linalgebra:theorem:diagonalizable_basis}
        A linear map $f$ defined on a finite-dimensional $K$-vector space $V$ is diagonalizable if and only if the set of eigenvectors of $f$ forms a basis of $V$.
    \end{theorem}

\subsection{Multiplicity}

    \newdef{Multiplicity}{\index{multiplicity}
        Let $V$ be a $K$-vector space and let $f\in\text{End}_K(V)$ be a linear map with characteristic polynomial\footnote{We assume that the characteristic polynomial can be written in this form. However, this depends on the possibility to completely factorize the polynomial over $K$ (i.e. if it has 'enough' roots in $K$). If not, $f$ cannot even be diagonalized. In general there always exists a field $F$ containing $K$, called a \textit{splitting field}, where the polynomial has 'enough' roots.}
        \begin{gather}
            \chi_f(x) = \prod_{i=1}^n(x-\lambda_i)^{n_i}.
        \end{gather}
        We can define the following multiplicities:
        \begin{enumerate}
            \item The \textbf{algebraic multiplicity} of an eigenvalue $\lambda_i$ is equal to $n_i$.
            \item The \textbf{geometric multiplicity} of an eigenvalue $\lambda_i$ is equal to the dimension of the eigenspace belonging to that eigenvalue.
        \end{enumerate}
    }
    \remark{It is clear that the geometric multiplicity is always at least 1.}

    \begin{property}
        The algebraic multiplicity is always greater than or equal to the geometric multiplicity.
    \end{property}
    \begin{theorem}\label{linalgebra:theorem:diagonalizable_multiplicity}
        Let $f\in\text{End}_K(V)$ be a linear map. $f$ is diagonalizable if and only if for every eigenvalue the algebraic multiplicity is equal to the geometric multiplicity.
    \end{theorem}

    \begin{property}\index{Hermitian}\label{linalgebra:diagonalizable_hermitian}
        Every Hermitian linear map $f\in\text{End}_K(\mathbb{C}^n)$ has the following properties:
        \begin{enumerate}
            \item All the eigenvalues of $f$ are real.
            \item Eigenvectors belonging to different eigenvalues are orthogonal.
            \item $f$ is diagonalizable and there always exists an orthonormal basis of eigenvectors of $f$.\footnote{This implies that the matrix $P$ diagonalizing the associated Hermitian matrix $A_f$ is unitary, i.e. $P^{-1} = P^\dag$.}
        \end{enumerate}
    \end{property}

    \begin{property}\index{commutator}
        Let $A,B\in\text{End}_K(V)$ be two linear maps. If the commutator $[A, B]$ is zero, then the two maps have a common eigenbasis.
    \end{property}

    \begin{theorem}[Sylvester's law of inertia]\index{Sylvester's law of inertia}
        Let $S$ be a symmetric matrix. The number of positive and negative eigenvalues is invariant with respect to similarity transformations\footnote{Also with respect to conjugation, which are equivalent to similarity transformations according to property \ref{linalgebra:con_equivalence}.}.
    \end{theorem}
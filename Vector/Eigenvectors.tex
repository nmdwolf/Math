\section{Eigenvectors}

    \begin{definition}[Eigenvector]\index{eigenvalue}\index{eigenvector}
        A vector $v\in V\setminus\{0\}$ is called an \textbf{eigenvector} of the linear map $f:V\rightarrow V$ if it satisfies the equation
       \begin{gather}
            f(v) = \lambda v
        \end{gather}
       for some $\lambda\in K$. The scalar $\lambda$ is called the \textbf{eigenvalue} associated to $v$.
    \end{definition}
    \begin{definition}[Eigenspace]\label{linalgebra:eigenvalue_remark}
        The subspace of $V$ spanned by the eigenvectors of a linear map is called the eigenspace of that linear map. It is given by
        \begin{gather}
            \text{ker}(\lambda\mathbbm{1}_V - f).
        \end{gather}
        It follows that the eigenvalues are exactly those scalars for which the linear map $\lambda\mathbbm{1}_V-f$ is not injective. (This is generalized in Section \ref{section:spectrum}.)
    \end{definition}

    \begin{theorem}[Characteristic equation]\index{characteristic!equation}\label{linalgebra:theorem:eigenvalue_characteristic_equation}
        Let $f\in\mathrm{End}(V)$ be a linear map. A scalar $\lambda\in K$ is an eigenvalue of $f$ if and only if it satisfies the characteristic equation \ref{linalgebra:characteristic_equation}.
    \end{theorem}

    \begin{theorem}
        A linear map $f\in\mathrm{End}(V)$ defined over an $n$-dimensional vector space $V$ has at most $n$ different eigenvalues.
    \end{theorem}

    \begin{method}[Finding the eigenvectors of a matrix]
        To calculate the eigenvectors of a matrix one should perform the following steps:
        \begin{enumerate}
            \item Find the eigenvalues $\lambda_i$ of $A$ by applying theorem \ref{linalgebra:theorem:eigenvalue_characteristic_equation}.
            \item Find the eigenvector $v_i$ associated to the eigenvalue $\lambda_i$ through the following equation:
                \begin{gather}
                    \label{linalgebra:eigenvectors:eigenspace}
                    \left(A - \lambda_i\mathbbm{1}_V\right)v_i = 0.
                \end{gather}
        \end{enumerate}
    \end{method}

\subsection{Diagonalization}

    \newdef{Diagonalizable map}{
        Let $V$ be a finite-dimensional vector space. A linear map $f\in\text{End}(V)$ is diagonalizable if $f$ admits a diagonal matrix representation.
    }
    \begin{property}\index{semisimple!operator}
        Every diagonalizable map is semisimple \ref{linalgebra:jordan_chevalley}. Conversely, in finite dimensions (and over an algebraically closed field), a semisimple map is diagonalizable.
    \end{property}

    \begin{theorem}\label{linalgebra:theorem:diagonalizable_PQP}
        A matrix $A\in M_n(K)$ is diagonalizable if and only if there exists a matrix $P\in\mathrm{GL}_n(K)$ such that $P^{-1}AP$ is diagonal.
    \end{theorem}
    \begin{result}\index{trace}
        Using the fact that the trace of a linear map is invariant under similarity transformations (Property \ref{linalgebra:trace_invariance}), the following useful formula can be proven:
        \begin{gather}
            \text{tr}(f) = \sum_{i=0}^n\lambda_i,
        \end{gather}
        where $\{\lambda_i\}_{i\leq n}$ are the eigenvalues of $f$.
    \end{result}

    \begin{property}\label{linalgebra:diagonalization_properties}
        Let $V$ be an $n$-dimensional vector space and let $f\in\text{End}(V)$ be a linear map. The eigenvectors/eigenvalues of $f$ satisfy the following properties:
        \begin{itemize}
            \item The eigenvectors of $f$ belonging to different eigenvalues are linearly independent.
            \item If $f$ has exactly $n$ eigenvalues, $f$ is diagonalizable.
            \item If $f$ is diagonalizable, then $V$ is the direct sum of the eigenspaces of $f$ belonging to the different eigenvalues of $f$.
        \end{itemize}
    \end{property}
    \begin{theorem}\label{linalgebra:theorem:diagonalizable_basis}
        A linear map $f$ defined on a finite-dimensional vector space $V$ is diagonalizable if and only if the set of eigenvectors of $f$ forms a basis of $V$.
    \end{theorem}

\subsection{Multiplicity}

    \newdef{Multiplicity}{\index{multiplicity}
        Let $V$ be a vector space and let $f\in\text{End}(V)$ be a linear map with characteristic polynomial
        \begin{gather}
            \chi_f(x) = \prod_{i=1}^n(x-\lambda_i)^{n_i}.
        \end{gather}
        The multiplicities are defined as follows:
        \begin{enumerate}
            \item The \textbf{algebraic multiplicity} of an eigenvalue $\lambda_i$ is equal to $n_i$.
            \item The \textbf{geometric multiplicity} of an eigenvalue $\lambda_i$ is equal to the dimension of the eigenspace belonging to that eigenvalue.
        \end{enumerate}
    }
    \begin{remark}
        In the previous definition it was assumed that the characteristic polynomial can be completely factorized. However, this depends on the possibility to completely factorize the polynomial over $K$ (i.e. if it has 'enough' roots in $K$). If not, $f$ cannot even be diagonalized. In general there always exists a field $F$ containing $K$, called a \textit{splitting field}, where the polynomial has ''enough'' roots.
    \end{remark}

    \begin{property}
        The algebraic multiplicity is always greater than or equal to the geometric multiplicity.
    \end{property}
    \begin{theorem}\label{linalgebra:theorem:diagonalizable_multiplicity}
        Let $f\in\mathrm{End}(V)$ be a linear map. $f$ is diagonalizable if and only if for every eigenvalue the algebraic multiplicity is equal to the geometric multiplicity.
    \end{theorem}

    \begin{property}\index{Hermitian}\label{linalgebra:diagonalizable_hermitian}
        Every Hermitian linear map $f\in\text{End}(\mathbb{C}^n)$ has the following properties:
        \begin{itemize}
            \item All the eigenvalues of $f$ are real.
            \item Eigenvectors belonging to different eigenvalues are orthogonal.
            \item $f$ is diagonalizable and there always exists an orthonormal basis of eigenvectors of $f$, in particular, the diagonalizing matrix $P$ is unitary, i.e. $P^{-1} = P^\dag$.
        \end{itemize}
    \end{property}

    \begin{property}\index{commutator}
        Let $A,B\in\text{End}(V)$ be two diagonalizable maps. If the commutator $[A,B]$ is zero, the two maps have a common eigenbasis.
    \end{property}

    \begin{theorem}[Sylvester's law of inertia]\index{Sylvester's law of inertia}
        Let $S$ be a Hermitian matrix. The number of positive and negative eigenvalues is invariant with respect to $\dag$-congruence (or conjugation due to Property \ref{linalgebra:con_equivalence}).
    \end{theorem}
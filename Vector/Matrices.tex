\section{Matrices}

    \begin{notation}\label{linalgebra:matrix_set}
        The set of all $m\times n$-matrices defined over the field $K$ is denoted by $M_{m,n}(K)$. If $m=n$, the set is denoted by $M_n(K)$ or $M(n, K)$.
    \end{notation}

    \begin{property}[Dimension]\index{dimension}\label{linalgebra:dimension_of_matrix_space}
        The dimension of $M_{m,n}(K)$ as a vector space is $mn$.
    \end{property}

    \newdef{Trace}{\index{trace}\label{linalgebra:trace}
        Let $A = (a_{ij})\in M_n(K)$. We define the trace of $A$ as follows:
        \begin{gather}
            \text{tr}(A) := \sum_{i=1}^na_{ii}.
        \end{gather}
    }
    \begin{property}\label{linalgebra:trace_commutative}
        Let $A, B\in M_n(K)$. The trace satisfies the following properties:
        \begin{enumerate}
            \item $\text{tr}:M_n(K)\rightarrow K$ is a linear map
            \item $\text{tr}(AB) = \text{tr}(BA)$
            \item $\text{tr}(AB) \neq \text{tr}(A)\text{tr}(B)$
            \item $\text{tr}(A^T) = \text{tr}(A)$
        \end{enumerate}
    \end{property}

    \newformula{Hilbert-Schmidt norm\footnotemark}{\index{Frobenius!norm}\index{Hilbert-Schmidt norm}
        \footnotetext{Also called the \textbf{Frobenius norm}.}
        The Hilbert-Schmidt norm is given by the following formula:
        \begin{gather}
            \label{linalgebra:hilbert_schmidt_norm}
            ||A||^2_{HS} := \sum_{i, j}|A_{ij}|^2 = \text{tr}(A^\dag A).
        \end{gather}
        If one identifies $M_{n}(\mathbb{C})$ with $\mathbb{C}^{2n}$, then this norm equals the standard Hermitian norm.
    }

    \newformula{Hadamard product}{\index{Hadamard!product}\label{linalgebra:hadamard_product}
        The Hadamard product of two matrices $A, B\in M_{m\times n}(K)$ is defined as the entry-wise product:
        \begin{gather}
            (A\circ B)_{ij} := A_{ij}B_{ij}.
        \end{gather}
    }

    \newdef{General linear group}{\index{general linear group}\label{linalgebra:GL_matrices}
        \nomenclature[S_GLn]{GL$(n,K)$}{General linear group: group of all invertible $n$-dimensional matrices over the field $K$.}
        The set of invertible matrices is called the general linear group and is denoted by GL$_n(K)$ or GL$(n, K)$.
    }
    \begin{property}
        For all $A\in\text{GL}_n(K)$ we have:
        \begin{itemize}
            \item $A^T\in\text{GL}_n(K)$
            \item $\left(A^T\right)^{-1}=\left(A^{-1}\right)^T$
        \end{itemize}
    \end{property}

    \begin{property}\label{linalgebra:dim_columns_rows}
        Let $A\in M_{m,n}(K)$. Denote the set of columns as $\{A_1, A_2, \ldots, A_n\}$ and the set of rows as $\{R_1, R_2, \ldots, R_m\}$. The set of columns is a subset of $K^m$ and the set of rows is a subset of $K^n$ and as such we can define their span. These spaces satisfy the following property:
        \begin{gather}
            \dim(\text{span}(A_1, \ldots, A_n)) = \dim(\text{span}(R_1, \ldots, R_m)).
        \end{gather}
    \end{property}

    \newdef{Rank}{\index{rank}
        Using the invariance relation from previous property we can define the rank of a matrix $A\in M_{m,n}(K)$ as follows:
        \begin{gather}
            \label{linalgebra:matrix_rank}
            \text{rk}(A) := \dim(\text{span}(A_1,\ldots,A_n)) \overset{\ref{linalgebra:dim_columns_rows}}{=} \dim(\text{span}(R_1,\ldots,R_m)).
        \end{gather}
    }

    \begin{property}\label{linalgebra:rank_properties}
        The rank of a matrix has the following properties:
        \begin{enumerate}
            \item $\text{rk}(AC)\leq\text{rk}(A)$ and $\text{rk}(AC)\leq\text{rk}(C)$
            \item $\text{rk}(BC)=\text{rk}(C)$
            \item $\text{rk}(BD)=\text{rk}(D)$
        \end{enumerate}
        where $A\in M_{m,n}(K)$, $B\in\text{GL}_n(K)$, $C\in M_{n,r}(K)$ and $D\in M_{r,n}(K)$.
    \end{property}
    \begin{property}\label{linalgebra:dim_matrix_left_multiplication}
        Let $A\in M_{m,n}(K)$. We first define the following linear map:
        \begin{gather}
            \label{linalgebra:matrix_left_multiplication}
            L_A:K^n\rightarrow K^m:v\mapsto Av.
        \end{gather}
        This map has the following properties:
        \begin{enumerate}
            \item $\text{im}(L_A) = \text{span}(A_1,\ldots,A_n)$
            \item $\dim(\text{im}(L_A))=\text{rk}(A)$
        \end{enumerate}
    \end{property}
    \sremark{The second property is a direct consequence of the first one together with definition \ref{linalgebra:matrix_rank}.}

\subsection{System of equations}

    \begin{property}\label{linalgebra:matrix_and_equations}
        Let $AX=w$ with $A\in M_{m,n}(K)$, $w\in K^m$ and $X\in K^n$ be a system of $m$ equations in $n$ variables. Let $L_A$ be the linear map as defined in equation \ref{linalgebra:matrix_left_multiplication}. We then have the following properties:
        \begin{enumerate}
            \item The system is false if and only if $w\not\in\text{im}(L_A)$.
            \item If the system is not false, the solution set is an affine space. If $v_0\in K^n$ is a solution, then the solution set is given by: $L_A^{-1}(w)=v_0+\text{ker}(L_A)$.
            \item If the system is homogeneous ($AX=0$), then the solution set is equal to $\text{ker}(L_A)$.
        \end{enumerate}
    \end{property}
    \begin{property}[Uniqueness]\label{linalgebra:rank_unique_solution}
        Let $AX=w$ with $A\in M_n(K)$ be a system of $n$ equations in $n$ variables. If $\text{rk}(A)=n$, then the system has a unique solution.
    \end{property}

    \newformula{Cramer's rule}{\index{Cramer}\label{linalgebra:cramers_rule}
        Let $AX = w$ be a system of linear equations where the matrix $A$ has a nonzero determinant. Then Cramer's rule gives a unique solution:
        \begin{gather}
            X_i = \stylefrac{\det(A_i)}{\det(A)}
        \end{gather}
        where $A_i$ is the matrix obtained by replacing the $i^{th}$ column of $A$ by the column matrix $w$.
    }

\subsection{Coordinates and matrix representations}

    \newdef{Coordinate vector}{\label{linalgebra:coordinate_vector}
        Let $\mathcal{B} = \{b_1, \ldots, b_n\}$ be a basis of $V$. Let $v\in V$ such that $v=\sum_{i=1}^n\lambda_ib_i$. We define the coordinate vector of $v$ with respect to $\mathcal{B}$ as $(\lambda_1,\ldots,\lambda_n)^T$. The $\lambda_i$'s are called the \textbf{coordinates} of $v$ with respect to $\mathcal{B}$.
    }
    \newdef{Coordinate isomorphism}{\label{linalgebra:coordinate_isomorphism}
        With the previous definition in mind we can define the coordinate isomorphism of $v$ with respect to $\mathcal{B}$ as follows:
        \begin{gather}
            \beta:V\rightarrow K^n:\sum_{i=1}^n\lambda_ib_i\mapsto(\lambda_1,\ldots,\lambda_n)^T.
        \end{gather}
    }

    \begin{construct}[Matrix representation]\label{linalgebra:matrix_representation}
        Let $V$ be an $n$-dimensional vector space and $W$ an $m$-dimensional vector space. Let $f:V\rightarrow W$ be a linear map and consider two bases $\mathcal{B}=\{b_1, \ldots, b_n\}, \mathcal{C} = \{c_1, \ldots, c_m\}$ for respectively $V$ and $W$. The matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$ can be derived as follows:

        \qquad For every $j\in\{1, \ldots, n\}$ we can write $f(b_j) = \sum_{i=1}^ma_{ij}c_i$. Based on this observation we define the matrix $(a_{ij})\in M_{m,n}(K)$ as the matrix representation of $f$. This way the $j^{th}$ column of $A_{f, \mathcal{B}, \mathcal{C}}$ coincides with the coordinate vector of $f(b_j)$ (with respect to $\mathcal{C}$). Using this relation we can construct $A_{f, \mathcal{B}, \mathcal{C}}$ by writing for every $j\in\{1, \ldots, n\}$ the coordinate vector of $f(b_j)$ in the $j$-th column.
    \end{construct}
    \newnot{Matrix representation of a linear map}{
        The matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$ is sometimes denoted by $A_{f, \mathcal{B}, \mathcal{C}}$ or $A_f$ when the choice of bases is clear.
    }

    \begin{property}\label{linalgebra:theorem:matrix_representation}
        Let $(\lambda_1,\ldots,\lambda_n)^T$ be the coordinate vector of $v\in V$ with respect to $\mathcal{B}$. Let $(\mu_1, \ldots, \mu_m)^T$ be the coordinate vector of $f(v)$ with respect to $\mathcal{C}$. Then the following relation holds:
        \begin{gather}
            \left(
            \begin{array}{c}
                \mu_1\\
                \vdots\\
                \mu_m
            \end{array}\right)
            = A_{f, \mathcal{B}, \mathcal{C}}
            \left(
            \begin{array}{c}
                \lambda_1\\
                \vdots\\
                \lambda_n
            \end{array}\right).
        \end{gather}
    \end{property}

    The following property shows that the matrix algebra $M_{m, n}(K)$ is isomorphic to the algebra\footnote{Where the multiplication is given by the composition of linear maps.} of linear maps $\mathcal{L}(K^n, K^m)$:
    \begin{property}\label{linalgebra:theorem:map_matrix_link}
        For every matrix $A\in M_{m,n}(K)$ there exists a linear map $f:K^n\rightarrow K^m$ such that $A_{f, \mathcal{B}, \mathcal{C}} = A$. Conversely, for every linear map $f:K^m\rightarrow K^n$ there exists a matrix $A\in M_{n,m}(K)$ such that $f=L_A$.
    \end{property}
    \begin{property}
        Let $\beta$ and $\gamma$ be the coordinate isomorphisms with respect to respectively $\mathcal{B}$ and $\mathcal{C}$. From theorem \ref{linalgebra:theorem:matrix_representation} it follows that
        \begin{gather}
            \gamma(f(v)) = A_f\cdot\beta(v)
        \end{gather}
        or alternatively
        \begin{gather}
            \gamma\circ f = L_{A_f}\circ\beta.
        \end{gather}
    \end{property}

    \begin{result}\label{linalgebra:matrix_invertible_map}
        Let $f\in\text{End}_K(V)$ and let $A_f$ be the corresponding matrix representation. The linear map $f$ is invertible if and only if $A_f$ is invertible. Furthermore, if $A_f$ is invertible, we have that \[\left(A_f\right)^{-1} = A_{f^{-1}}.\] In other words, the following map is an isomorphism:
        \begin{gather}
            \text{GL}_K(V)\rightarrow\text{GL}_n(K):f\mapsto A_f
        \end{gather}
        where $n=\dim(V)$.
    \end{result}

    \begin{formula}[Linear forms]
        Let $V \cong K^n$ and consider a linear form $f\in V^*$. From construction \ref{linalgebra:matrix_representation} it follows that $A_f = (f(e_1), \ldots, f(e_n))\in M_{1,n}(K)$ with respect to the standard basis of $V$. This combined with property \ref{linalgebra:theorem:matrix_representation} gives
        \begin{gather}
            f\left((\lambda_1, \ldots, \lambda_n)^T\right) = (f(e_1), \ldots, f(e_n))(\lambda_1, \ldots, \lambda_n)^T = \sum_{i=1}^nf(e_i)\lambda_i
        \end{gather}
        or alternatively in terms of the standard dual basis $\{\varepsilon_1, \ldots, \varepsilon_n\}$:
        \begin{gather}
            \label{linalgebra:map_in_function_of_dual_basis}
            f = \sum_{i=1}^nf(e_i)\varepsilon_i.
        \end{gather}
    \end{formula}

    \begin{property}
        Let $f:V\rightarrow W$ be a linear map and let $f^*:W^*\rightarrow V^*$ be the corresponding dual map. If $A_f$ is the matrix representation of $f$ with respect to $\mathcal{B}$ and $\mathcal{C}$, then the transpose $A_f^T$ is the matrix representation of $f^*$ with respect to the dual basis of $\mathcal{C}$ and the dual basis of $\mathcal{B}$.
    \end{property}

\subsection{Coordinate transformations}

    \newdef{Transition matrix}{\label{linalgebra:transition_matrix}
        Let $\mathcal{B} = \{b_1, \ldots, b_n\}$ and $\mathcal{B}' = \{b_1', \ldots, b_n'\}$ be two bases of $V$. By definition every element of $\mathcal{B}'$ can be written as a linear combination of elements in $\mathcal{B}$:
        \begin{gather}
            b_j' = q_{1j}b_1 + \ldots + q_{nj}b_n.
        \end{gather}
        The matrix $Q = (q_{ij})\in M_n(K)$ is called the transition matrix from the ''old'' basis $\mathcal{B}$ to the ''new'' basis $\mathcal{B}'$.
    }

    \begin{property}\label{linalgebra:theorem:transition_matrix}
        Let $\mathcal{B}, \mathcal{B}'$ be two bases of $V$ and let $Q$ be the transition matrix from $\mathcal{B}$ to $\mathcal{B}'$. We find the following statements:
        \begin{enumerate}
            \item Let $\mathcal{C}$ be an arbitrary basis of $V$ with $\gamma$ the corresponding coordinate isomorphism. Define the following matrices: \[B:=(\gamma(b_1), \ldots, \gamma(b_n))\quad\text{and}\quad B':=(\gamma(b_1'), \ldots, \gamma(b_n')).\] In terms of these matrices we find that $BQ = B'$.
            \item $Q\in\text{GL}_n(K)$ and $Q^{-1}$ is the transition matrix from $\mathcal{B}'$ to $\mathcal{B}$.
            \item Consider $v\in V$. Let $(\lambda_1, \ldots, \lambda_n)^T$ be the coordinate vector with respect to $\mathcal{B}$ and let $(\lambda_1', \ldots, \lambda_n')^T$ be the coordinate vector with respect to $\mathcal{B}'$, then
                \[
                    Q\left(
                    \begin{array}{c}
                        \lambda_1'\\
                        \vdots\\
                        \lambda_n'
                    \end{array}
                    \right)
                    =
                    \left(
                    \begin{array}{c}
                        \lambda_1\\
                        \vdots\\
                        \lambda_n
                    \end{array}
                    \right)\quad\text{and}\quad\left(
                    \begin{array}{c}
                        \lambda_1'\\
                        \vdots\\
                        \lambda_n'
                    \end{array}
                    \right)=Q^{-1}\left(
                    \begin{array}{c}
                        \lambda_1\\
                        \vdots\\
                        \lambda_n
                    \end{array}
                    \right).
                \]
        \end{enumerate}
    \end{property}

    \begin{property}[Basis change]\label{linalgebra:theorem:transition_matrix_representation}
        Let $V,W$ be two finite-dimensional $K$-vector spaces. Consider two bases $\mathcal{B}, \mathcal{B}'$ of $V$ and two bases $\mathcal{C}, \mathcal{C}'$ of $W$. Let $Q, P$ be the transition matrices from $\mathcal{B}$ to $\mathcal{B}'$ and from $\mathcal{C}$ to $\mathcal{C}'$ respectively. The matrix representations $A=A_{f, \mathcal{B}, \mathcal{C}}$ and $A' = A_{f, \mathcal{B}', \mathcal{C}'}$ of a linear map $f:V\rightarrow W$ are related in the following way:
        \begin{gather}
            A' = P^{-1}AQ.
        \end{gather}
    \end{property}

    \newdef{Matrix conjugation}{\index{conjugacy class}\index{matrix!conjugation}\label{linalgebra:conjugacy_class}
        Let $A\in M_n(K)$. The set
        \begin{gather}
            \big\{Q^{-1}AQ:Q\in\text{GL}_n(K)\big\}
        \end{gather}
        is called the conjugacy class\footnote{This is the general definition of conjugacy classes for groups. Furthermore, these classes induce a partitioning of the group.} of $A$. Another name for conjugation is \textbf{similarity transformation}.
    }
    \begin{remark}
        If $A$ is a matrix representation of a linear operator $f$, then the conjugacy class of $A$ consists out of every possible matrix representation of $f$.
    \end{remark}

    \begin{property}[Trace]
        From property \ref{linalgebra:trace_commutative} it follows that the trace of a matrix is invariant under similarity transformations:
        \begin{gather}
            \label{linalgebra:trace_invariance}
            \text{tr}(Q^{-1}AQ) = \text{tr}(A).
        \end{gather}
    \end{property}

    \newdef{Matrix congruence}{\index{congruence}\label{linalgebra:matrix_congruence}
        Let $A, B\in M_n(K)$. If there exists a matrix $P$ such that
        \begin{gather}
            A = P^TBP
        \end{gather}
        then the matrices are said to be congruent.
    }
    \begin{property}
        Every matrix congruent to a symmetric matrix is also symmetric.
    \end{property}

    \begin{property}[Orthogonality of basis changes]\label{linalgebra:theorem:orthogonal_transition_matrix}
        Let $(V, \langle\cdot|\cdot\rangle)$ be an inner product space defined over $\mathbb{R}$ (or $\mathbb{C}$). Let $\mathcal{B}, \mathcal{B}'$ be two orthonormal bases of $V$ and let $Q$ be the transition matrix. Then $Q$ is orthogonal:
        \begin{gather}
            Q^TQ = \mathbbm{1}_n.
        \end{gather}
    \end{property}

\subsection{Determinant}

    \newdef{Minor}{\index{minor}
        The $(i, j)$-th minor of $A$ is defined as
        \begin{gather}
            \det(A_{ij})
        \end{gather}
        where $A_{ij}\in M_{n-1}(K)$ is the matrix obtained by removing the $i^{th}$ row and the $j^{th}$ column from $A$.
    }
    \newdef{Cofactor}{\index{cofactor}
        The cofactor $\alpha_{ij}$ of the matrix element $a_{ij}$ is equal to
        \begin{gather}
            (-1)^{i+j}\det(A_{ij})
        \end{gather}
        where $\det(A_{ij})$ is the minor as defined above.
    }
    \newdef{Adjugate matrix}{\index{adjugate matrix}\label{linalgebra:adjugate_matrix}
        The adjugate matrix of $A\in M_n(K)$ is defined as follows:
       \begin{gather}
            \text{adj}(A) := \left(
            \begin{array}{cccc}
                \alpha_{11}&\alpha_{21}&\dotsm&\alpha_{n1}\\
                \alpha_{12}&\alpha_{22}&\dotsm&\alpha_{n2}\\
                \vdots&\vdots&\vdots&\vdots\\
                \alpha_{1n}&\alpha_{2n}&\dotsm&\alpha_{nn}\\
            \end{array}
            \right)
        \end{gather}
        or in terms of the cofactors: $\text{adj}(A) = (\alpha_{ij})^T$.
    }
    \begin{remark*}
        It is important to notice that we have to transpose the matrix after the elements have been replaced by their cofactor.
    \end{remark*}

    \begin{formula}[Laplace]\index{Laplace!determinant formula}\label{linalgebra:laplace_formula}
        The determinant of a matrix $A=(a_{ij})\in M_n(K)$ can be evaluated as follows:
        \begin{gather}
            \det(A) = \sum_{i=1}^n(-1)^{i+k}a_{ik}\det(A_{ik}).
        \end{gather}
    \end{formula}
    \begin{property}\label{linalgebra:determinant_properties}
        Let $A,B\in M_n(K)$. Denote the columns of $A$ as $A_1,\ldots, A_n$. We have the following properties of the determinant:
        \begin{enumerate}
            \item $\det(A^T) = \det(A)$
            \item $\det(AB) = \det(A)\det(B)$
            \item $\det(A_1, \dotso, A_i+\lambda A_i', \dotso, A_n) = \det(A_1, \dotso, A_i, \dotso, A_n) + \lambda\det(A_1, \dotso,A_i', \dotso, A_n)$ for all $A_i,A_i'\in M_{n,1}(K)$
            \item $\det(A_{\sigma(1)},\dotso,A_{\sigma(n)}) = \text{sgn}(\sigma)\det(A_1,\dotso,A_n)$
        \end{enumerate}
    \end{property}
    \begin{result}
        Items 3 and 4 from this list imply that a matrix with two identical columns has a vanishing determinant.
    \end{result}

    \begin{property}\label{linalgebra:theorem:rank_det_equivalence}
        Let $A\in M_n(K)$. The following statements are equivalent:
        \begin{enumerate}
            \item $\det(A)\neq 0$
            \item $\text{rk}(A) = n$
            \item $A\in\text{GL}_n(K)$
        \end{enumerate}
    \end{property}
    \begin{property}\label{linalgebra:theorem:adjugate_matrix}
        For all $A\in M_n(K)$ we find that $A\ \text{adj}(A) = \text{adj}(A)A = \det(A)I_n$.
    \end{property}
    \begin{formula}\label{linalgebra:theorem:determinant_inverse}
        For all $A\in\text{GL}_n(K)$ we find
        \begin{gather}
            A^{-1} = \det(A)^{-1}\ \text{adj}(A).
        \end{gather}
    \end{formula}

    An alternative definition of a $k\times k$-minor is:
    \begin{adefinition}[Minor]\index{minor}
        Let $A\in M_{m,n}(K)$ and $k\leq\min(m, n)$. A $k\times k$-minor of $A$ is the determinant of a $k\times k$-partial matrix obtained by removing $m-k$ rows and $n-k$ columns from $A$.
    \end{adefinition}
    \begin{property}
        Let $A\in M_{m,n}(K)$ and $k\leq\min(m, n)$. We find that $\text{rk}(A)\geq k$ if and only if $A$ contains a non-zero $k\times k$-minor.
    \end{property}

    \begin{property}
        Let $f\in\textup{End}_K(V)$. The determinant of the matrix representation of $f$ is invariant under basis transformations.
    \end{property}
    \newdef{Determinant of a linear operator}{\index{determinant}\label{linalgebra:operator_determinant}
        The previous property allows us to unambiguously define the determinant of $f\in\textup{End}_K(V)$ as follows:
        \begin{gather}
            \det(f) := \det(A)
        \end{gather}
        where $A$ is some matrix representation of $f$.
    }

\subsection{Characteristic polynomial}

    \begin{definition}[Characteristic polynomial\footnotemark]\index{characteristic!polynomial}\label{linalgebra:characteristic_polynomial}
        \footnotetext{This polynomial can also be used directly for a matrix $A$ as theorem \ref{linalgebra:theorem:map_matrix_link} matches every matrix $A$ with some linear operator $f$.}
        Let $V$ be a finite-dimensional $K$-vector space. Let $f\in \text{End}_K(V)$ be a linear operator with the matrix representation $A$. We then find that
        \begin{gather}
            \chi_f(x) := \det(x\mathbbm{1}_n - A) \in K[x]
        \end{gather}
        is a monic polynomial of degree $n$ in the variable $x$. Furthermore, the polynomial does not depend on the choice of basis.
    \end{definition}

    \begin{definition}[Characteristic equation\footnotemark]\index{characteristic!equation}
        \footnotetext{This equation is sometimes called the \textbf{secular equation}.}
        The following equation is called the characteristic equation of $f$:
        \begin{gather}
            \label{linalgebra:characteristic_equation}
            \chi_f(x) = 0
        \end{gather}
        where $\chi_f$ is the characteristic polynomial defined above.
    \end{definition}

    \begin{formula}\label{linalgebra:parts_of_characteristic_polynomial}
        Consider a matrix $A=(a_{ij})\in M_n(K)$ with characteristic polynomial \[\chi_A(x) = x^n + c_{n-1}x^{n-1} + \dotso + c_1x + c_0.\] Some of the coefficients $c_i$ have a simple expression:
        \begin{gather}
            \begin{cases}
                c_0 = (-1)^n\det(A)\\
                c_{n-1} = -\text{tr}(A)
            \end{cases}
        \end{gather}
    \end{formula}

    \begin{theorem}[Cayley-Hamilton]\index{Cayley-Hamilton}\label{linalgebra:cayley_hamilton}\
        Consider a linear map $f\in\textup{End}_K(V)$ with characteristic polynomial $\chi_f(x)$. We find that
        \begin{gather}
            \chi_f(f) = f^n + \sum_{i=1}^{n-1}c_if^i=0.
        \end{gather}
    \end{theorem}
    \begin{result}
        From theorem \ref{linalgebra:minimal_polynomial_divisor} and the Cayley-Hamilton theorem it follows that the minimal polynomial $\mu_f(x)$ is a divisor of the characteristic polynomial $\chi_f(x)$.
    \end{result}

\subsection{Linear groups}\label{linalgebra:section:linear_groups}

    \newdef{Elementary matrix}{\label{linalgebra:elementary_matrix}
    An elementary matrix is a matrix of the following form:
        \[
            \left(
            \begin{array}{cccc}
                1&0&\dotsm&0\\
                0&1&c_{ij}&0\\
                \vdots&\vdots&\ddots&\vdots\\
                0&0&\vdots&1
            \end{array}
            \right),\left(
            \begin{array}{cccc}
                1&0&\dotsm&0\\
                0&1&\dotsm&0\\
                \vdots&c_{ij}&\ddots&\vdots\\
                0&0&\vdots&1
            \end{array}
            \right),\dotso
        \]
        i.e. it is equal to the sum of an identity matrix and a multiple of a matrix unit $U_{ij}$ $(i\neq j)$.
    }
    \newnot{Elementary matrix}{$E_{ij}(c)$ is the elementary matrix with element $c$ on position $(i,j)$.}
    \begin{property}
        Elementary matrices have determinant 1. This implies that $E_{ij}(c)\in\text{GL}_n(K)$ for all $c\in K$.
    \end{property}
    \begin{property}
        We find the following results concerning the multiplication by an elementary matrix:
        \begin{enumerate}
            \item Left multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $i^{th}$ row of the matrix with the $i^{th}$ row plus $c$ times the $j^{th}$ row.
            \item Right multiplication by an elementary matrix $E_{ij}(c)$ comes down to replacing the $j^{th}$ column of the matrix with the $j^{th}$ column plus $c$ times the $i^{th}$ column.
        \end{enumerate}
    \end{property}

    \begin{property}\label{linalgebra:theorem:elementary_matrices}
        Every matrix $A\in\text{GL}_n(K)$ can be written in the following way: \[A = SD\] where $S$ is a product of elementary matrices and $D=\text{diag}(1,\dotso,1,\det(A))$.
    \end{property}

    \newdef{Special linear group}{\label{linalgebra:special_linear_group}
        \nomenclature[S_SLn]{$\text{SL}_n(K)$}{Special linear group: group of all invertible $n$-dimensional matrices with unit determinant over the field $K$.}
        The following subset of GL$_n(K)$ is called the special linear group:
        \begin{gather}
            \text{SL}_n(K) := \{A\in \text{GL}_n(K):\det(A) = 1\}.
        \end{gather}
    }
    \begin{property}
        Every $A\in\text{SL}_n(K)$ can be written as a product of elementary matrices.\footnote{This follows readily from theorem \ref{linalgebra:theorem:elementary_matrices}.}
    \end{property}

    \newdef{Orthogonal group}{\label{linalgebra:orthogonal_group}\index{orthogonal!group}
        The orthogonal and special orthogonal group are defined as follows:
        \begin{align}
            \text{O}_n(K) &:= \{A\in \text{GL}_n(K):AA^T = A^TA = I_n\}\nonumber\\
            \text{SO}_n(K) &:= \text{O}_n(K)\cap \text{SL}_n(K).\nonumber
        \end{align}
    }
    \begin{property}\label{linalgebra:con_equivalence}
        For orthogonal matrices, conjugacy \ref{linalgebra:conjugacy_class} and congruency \ref{linalgebra:matrix_congruence} are equivalent.
    \end{property}

    \newdef{Unitary group}{\label{linalgebra:unitary_group}\index{involution}
        The unitary and special unitary group are defined as follows:
        \begin{align}
            \text{U}_n(K, \sigma) &:= \{A\in \text{GL}_n(K):A\overline{A}^T = \overline{A}^TA = I_n\}\nonumber\\
            \text{SU}_n(K, \sigma) &:= \text{U}_n(K)\cap \text{SL}_n(K)\nonumber
        \end{align}
        where $\sigma$ denotes the \textit{involution}\footnote{An involution is an operator that is its own inverse.} $a^\sigma:=\overline{a}$.
    }

    \begin{remark*}
        If $K=\mathbb{C}$ where the involution is taken to be the complex conjugate, the $\sigma$ is often ommited in the definition: U$_n(K)$ and SU$_n(K)$.
    \end{remark*}

    \newdef{Unitary equivalence}{
        Let $A, B$ be two matrices in M$_n(K)$. If there is a unitary matrix $U$ such that \[A = U^\dag BU\] then the matrices $A$ and $B$ are said to be unitarily equivalent.
    }

    \newdef{Symplectic group}{\index{symplectic!group}
        \nomenclature[S_SymA]{Sp$_n(K)$}{Symplectic group: Group of matrices preserving the canonical symplectic form over the field $K$.}
        Consider a vector space $V$ with an antisymmetric nonsingular matrix $\Omega$. The symplectic group Sp$_n(V, \Omega)$ is defined as follows:
        \begin{gather}
            \label{linalgebra:symplectic_group}
            \text{Sp}(V, \Omega) := \{A\in\text{GL}(V):A^T\Omega A = \Omega\}.
        \end{gather}
        Over the real or complex numbers one can define the canonical \textbf{symplectic} matrix
        \begin{gather}
            \Omega_{st} := \begin{pmatrix}0&-\mathbbm{1}\\\mathbbm{1}&0\end{pmatrix}.
        \end{gather}
        The groups of matrices that preserve this matrix are often denoted by Sp$_n(\mathbb{R})$ or Sp$_n(\mathbb{C})$.
    }
    \begin{property}
        Symplectic groups can only be defined on even-dimensional spaces because antisymmetric matrices (such as $\Omega$) can only be nonsingular if the dimension $n$ is even.
    \end{property}

    \newdef{Compact symplectic group}{
        \nomenclature[S_SymC]{Sp$(n)$}{Compact symplectic group: Sp$_{2n}(\mathbb{C})\cap\text{U}(2n)$.}
        The compact symplectic group is defined as follows:
        \begin{gather}
            \text{Sp}(n) := \text{Sp}_{2n}(\mathbb{C})\cap\text{U}(2n).
        \end{gather}
        This group is in fact isomorphic to the \textit{quaternionic unitary group} in $n$ quaternionic dimensions.
    }
    \begin{example}
        For $n=1$ we find Sp$(1)\cong\text{SU}(2)$.
    \end{example}

\subsection{Matrix decompositions}

    \begin{method}[QR Decomposition]\index{QR!decompositon}
        Every square complex matrix $M$ can be decomposed as
        \begin{gather}
            M = QR
        \end{gather}
        where $Q$ is unitary and $R$ is upper-triangular. The easiest (but not the most numerically stable) way to do this is by applying the Gram-Schmidt orthonormalisation process:

        \qquad Let $\{v_i\}_{i\leq n}$ be a basis for the column space of $M$. By applying the Gram-Schmidt process to this basis one obtains a new orthonormal basis $\{e_i\}_{i\leq n}$. The matrix $M$ can then be written as $QR$ where:
        \begin{itemize}
            \item $R$ is an upper-triangular matrix with entries $R_{ij} = \langle e_i|\text{col}_j(M) \rangle$ where col$_j(M)$ denotes the $j^{th}$ column of $M$.
            \item $Q = (e_1,\ldots,e_n)$ is the unitary matrix constructed by setting the $i^{th}$ column equal to the $i^{th}$ basis vector $e_i$
        \end{itemize}
    \end{method}
    \begin{property}
        If $M$ is invertible and if the diagonal elements of $R$ are required to have positive norm then the QR-decomposition is unique.
    \end{property}
\chapter{Vector \& Tensor Calculus}

    References for this chapter are \cite{jeevanjee, AMP1}. For a more geometric approach to some of the concepts and results in this chapter, we refer to the content of chapters \ref{chapter:curves_surfaces}, \ref{diff:chapter:vector_bundles} and \ref{chapter:integration_manifolds}.

    \newdef{Vector field}{\index{vector field}
        By a vector field we will mean a smooth vector-valued function $\vector{f}(\vector{x})\equiv\big(u(\vector{x}), v(\vector{x}), w(\vector{x})\big)$ on $\mathbb{R}^3$. By smooth we mean that all three functions $u,v$ and $w$ are smooth.
    }

\section{Nabla-operator}\label{vectorcalculus:nabla}

    \sremark{The geometric approach to this section is summarized in remark \ref{forms:vector_calculus}.}

    \begin{definition}[Nabla]\index{nabla}
        The operator $\nabla$ is defined as follows:
        \begin{gather}
                \nabla := \left(\pderiv{}{x}, \pderiv{}{y}, \pderiv{}{z}\right).
        \end{gather}
    \end{definition}

    \newdef{Gradient}{\index{gradient}
        Let $V$ be a smooth real-valued function on $\mathbb{R}^3$.
        \begin{gather}
            \label{vectorcalculus:gradient}
            \nabla V := \left(\pderiv{V_x}{x}, \pderiv{V_y}{y}, \pderiv{V_z}{z}\right)
        \end{gather}
    }
    \begin{formula}
        Let $\varphi(\vector{x})$ be a smooth real-valued field. The total differential $d\varphi$ can be rewritten as follows:
            \begin{gather}
            d\varphi = \nabla\varphi\cdot d\vector{r}.
        \end{gather}
    \end{formula}

    \begin{property}
        The gradient of a smooth real-valued function $V$ is perpendicular to the level sets \ref{set:level_set} of $V$.
    \end{property}

    \newdef{Directional derivative}{\index{directional derivative}\label{vectorcalculus:directional_derivative}
        Let $\hat{a}$ be a unit vector. The directional derivative $\nabla_{\hat{a}}V$ is defined as the change of the function $V$ in the direction of $\hat{a}$:
        \begin{gather}
            \nabla_{\hat{a}}V := (\hat{a}\cdot\nabla)V.
        \end{gather}
    }
    \begin{example}
        Let $\varphi(\vector{x})$ be a smooth real-valued function and let $\deriv{\vector{r}}{s}$ denote the tangent vector to a curve $\vector{r}(s)$ with natural parameter\footnote{See definition \ref{diff:natural_parameter}.} $s$. The variation of the scalar field $\varphi(\vector{x})$ along $\vector{r}(s)$ is given by
        \begin{gather}
            \pderiv{\varphi}{s} = \nabla\varphi\cdot\deriv{\vector{r}}{s}.
        \end{gather}
    \end{example}

    \newdef{Conservative vector field}{\index{conservative}
        A vector field obtained as the gradient of a scalar function.
    }
    \begin{property}
        A vector field is conservative if and only if its line integral is path independent, i.e. it only depends on the values at the end points. (This is a corollary of Stokes' theorem \ref{forms:theorem:stokes_theorem}.)
    \end{property}

    \newdef{Gradient of a tensor}{\index{gradient}
        Let $T$ be a tensor field with coordinates $x^i$ and let $\vector{e}^{\,i}(x^1, x^2, x^3)$ be a (curvilinear) orthogonal frame\footnote{See definition \ref{diff:frame}.}. The gradient of $T$ is defined as follows:
        \begin{gather}
            \nabla T := \pderiv{T}{x^i}\otimes\vector{e}^{\,i}.
        \end{gather}
    }

    \newdef{Divergence}{\index{divergence}\label{vectorcalculus:divergence}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \nabla\cdot\vector{A} := \pderiv{A_x}{x} + \pderiv{A_y}{y} + \pderiv{A_z}{z}
        \end{gather}
    }
    \newdef{Solenoidal vector field}{\index{solenoidal}
        A vector field $\vector{V}(\vector{x})$ is said to be solenoidal if it satisfies
        \begin{gather}
            \nabla\cdot\vector{V} = 0.
        \end{gather}
        It is also known as a \textbf{divergence-free vector field}.
    }

    \newdef{Rotor / curl}{\index{curl}\index{rotor|see{curl}}\label{vectorcalculus:rotor}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \nabla\times\vector{A} := \left(\pderiv{A_z}{y} - \pderiv{A_y}{z}, \pderiv{A_x}{z} - \pderiv{A_z}{x}, \pderiv{A_y}{x} - \pderiv{A_x}{y}\right)
        \end{gather}
    }

    \newdef{Irrotational vector field}{\index{irrotational}
        A vector field $\vector{V}(\vector{x})$ is said to be irrotational if it satisfies
        \begin{gather}
            \nabla\times\vector{V} = 0.
        \end{gather}
    }
    \begin{remark}
        All conservative vector fields are irrotational but irrotational vector fields are only conservative if the domain is simply-connected\footnote{See definition \ref{topology:simply_connected}.}.
    \end{remark}

\subsection{Laplacian}

    \newdef{Laplacian}{\index{Laplace!operator}
        Let $V$ and $\vector{A}$ be smooth real-valued and vector-valued functions on $\mathbb{R}^3$ respectively.
        \begin{gather}
            \label{vectorcalculus:laplacian}
            \bigtriangleup V\equiv\nabla^2V = \mpderiv{2}{V}{x} + \mpderiv{2}{V}{y} + \mpderiv{2}{V}{z}
        \end{gather}
        \begin{gather}
            \label{vectorcalculus:vector_laplacian}
            \nabla^2\vector{A} = \nabla\left(\nabla\cdot\vector{A}\right) - \nabla\times \left(\nabla\times\vector{A}\right)
        \end{gather}
        The latter operator is sometimes called the \textbf{vector Laplacian}.
    }

    \begin{formula}[Mixed properties]\label{vectorcalculus:mixed_properties}
        The different differential operators introduced above satisfy the following identities:
        \begin{gather}
            \label{vectorcalculus:rotor_of_gradient}
            \nabla \times \left(\nabla V\right) = 0
        \end{gather}
        \begin{gather}
            \label{vectorcalculus:divergence_of_rotor}
            \nabla \cdot \left(\nabla \times \vector{V}\right) = 0.
        \end{gather}

        In Cartesian coordinates equation \ref{vectorcalculus:vector_laplacian} can be rewritten as follows:
        \begin{gather}
            \label{vectorcalculus:vector_laplacian_carthesian}
            \nabla^2\vector{A} = \left(\bigtriangleup A_x, \bigtriangleup A_y, \bigtriangleup A_z\right)
        \end{gather}
    \end{formula}

    \newformula{Helmholtz decomposition}{\index{Helmholtz!decomposition}\label{vectorcalculus:helmholtz_decomposition}
        Let $\vector{P}$ be a vector field that decays rapidly (faster than $1/r$) when $r\rightarrow\infty$. $\vector{P}$ can be written as follows:
        \begin{gather}
            \vector{P} = \nabla\times\vector{A} + \nabla V.
        \end{gather}
    }

\subsection{Curvilinear coordinates}

    In this section the differential operators are generalized to curvilinear coordinates. To do this we need the scale factors as formally defined in equation \ref{diff:scale_factor}. Furthermore, we will not use the Einstein summation convention to make everything as explicit as possible.

    \newformula{Unit vectors}{
        \begin{gather}
            \pderiv{\vector{r}}{q^i} = h_i\hat{e}_i
        \end{gather}
    }
    \newformula{Gradient}{
        \begin{gather}
            \nabla V = \sum_{i=1}^3\stylefrac{1}{h_i}\pderiv{V}{q^i}\hat{e}_i
        \end{gather}
    }
    \newformula{Divergence}{
        \begin{gather}
            \nabla\cdot\vector{A} = \stylefrac{1}{h_1h_2h_3}\left(\pderiv{}{q^1}(A_1h_2h_3) + \pderiv{}{q^2}(A_2h_3h_1) + \pderiv{}{q^3}(A_3h_1h_2)\right)
        \end{gather}
    }
    \newformula{Rotor}{
        \begin{gather}
            (\nabla\times\vector{A})_i = \sum_{j,k=1}^3\stylefrac{\varepsilon_{ijk}}{h_jh_k}\left(\pderiv{}{q^j}(A_kh_k) - \pderiv{}{q^k}(A_jh_j)\right)
        \end{gather}
        where $\varepsilon_{ijk}$ is the 3-dimensional Levi-Civita symbol \ref{tensor:levi_civita_symbol}.
    }

    \newformula{Laplacian in different coordinate systems}{
        The Laplacian operator can also be expressed in different coordinate systems:
        \begin{itemize}
            \item Cylindrical coordinates $(\rho,\phi,z)$:
            \begin{gather}
                \label{laplacian:cylindrical}
                \stylefrac{1}{\rho}\pderiv{}{\rho}\left(\rho\pderiv{}{\rho}\right) + \stylefrac{1}{\rho^2}\mpderiv{2}{}{\phi} + \mpderiv{2}{}{z}
            \end{gather}
            \item Spherical coordinates $(r,\phi,\theta)$:
            \begin{gather}
                \label{laplacian:spherical}
                \stylefrac{1}{r^2}\left[\pderiv{}{r}\left(r^2\pderiv{}{r}\right) + \stylefrac{1}{\sin^2\theta}\mpderiv{2}{}{\phi} + \stylefrac{1}{\sin\theta}\pderiv{}{\theta}\left(\sin\theta\pderiv{}{\theta}\right)\right]
            \end{gather}
        \end{itemize}
    }

\section{Integration}
\subsection{Line integrals}\index{line!integral}\index{path!integral|see{line integral}}

    \newformula{Line integral of a continuous function}{\label{vectorcalculus:line_integral_scalar}
        Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ be a continuous function and let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $f$ along $\Gamma$ as follows:
        \begin{gather}
            \int_\Gamma f(s)ds = \int_a^b f(\vector{\varphi}(t))||\vector{\varphi}'(t)||dt.
        \end{gather}
    }
    \newformula{Line integral of a continuous vector field}{\label{vectorcalculus:line_integral_vector}
        Let $\vector{F}$ be a continuous vector field on $\mathbb{R}^3$ and let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $F$ along $\Gamma$ as follows:
        \begin{gather}
            \int_\Gamma \vector{F}(\vector{s})\cdot d\vector{s} = \int_a^b \vector{F}(\vector{\varphi}(t))\cdot\vector{\varphi}'(t)dt.
        \end{gather}
    }

\subsection[Integral theorems]{Integral theorems\footnotemark}
    \footnotetext{These theorems follow from a more general theorem by Stokes (see theorem \ref{forms:theorem:stokes_theorem}).}

    \begin{theorem}[Fundamental theorem of calculus for line integrals]\index{fundamental theorem!for line integrals}\label{vectorcalculus:fundamental_theorem}
        Let $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^3$ be a piecewise smooth curve defined on the interval $[a,b]$.
        \begin{gather}
            \int_\Gamma\nabla f(\vector{r})\cdot d\vector{r} = \varphi(\Gamma(b)) - \varphi(\Gamma(a))
        \end{gather}
    \end{theorem}

    \begin{theorem}[Kelvin-Stokes' theorem]\index{Stokes!Kelvin-Stokes theorem}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$ and let $S$ be a smooth surface with boundary $\partial S$.
        \begin{gather}
            \label{vectorcalculus:stokes_theorem}
            \oint_{\partial S}\vector{A}\cdot d\vector{l} = \iint_S \left(\nabla \times \vector{A}\right)dS
        \end{gather}
    \end{theorem}

    \begin{theorem}[Divergence theorem\footnotemark]\index{divergence!theorem}
        \footnotetext{Also known as \textbf{Gauss' theorem} or the \textbf{Gauss-Ostrogradsky theorem}.}
        Let $\vector{A}$ be a vector field on $\mathbb{R}^3$.
        \begin{gather}
            \label{vectorcalculus:divergence_theorem}
            \oiint_{\partial V}\vector{A}\cdot d\vector{S} = \iiint_V \left(\nabla \cdot \vector{A}\right)dV
        \end{gather}
    \end{theorem}
    \begin{result}[Green's identity]\index{Green!identity}
        Let $\phi, \psi$ be smooth real-valued functions on $\mathbb{R}^3$.
        \begin{gather}
            \label{vectorcalculus:green_indentity}
            \oiint_{\partial V}\left(\psi\nabla\phi - \phi\nabla\psi\right)\cdot d\vector{S} = \iiint_V \left(\psi\nabla^2\phi - \phi\nabla^2\psi\right) dV
        \end{gather}
    \end{result}

\section{Tensors}
\subsection{Tensor product}\index{tensor!product}
    \nomenclature[O_zsymbinten]{$V\otimes W$}{Tensor product of the vector spaces $V$ and $W$.}

    There are two possible (equivalent) ways to introduce the concept of a \textit{tensor} on finite-dimensional vector spaces. One way is to interpret tensors as multilinear maps, while another way interprets the components as expansion coefficients with respect to some chosen tensor space basis.

    \begin{definition}[Tensor product space]\label{tensor:tensor_product}\index{outer!product}
        The tensor product of two vector spaces $V$ and $W$ is defined as\footnote{''\textit{isomorphic to}'' would be a better terminology. See the ''universal property'' \ref{tensor:prop:universal_property} below. For a complete proof and explanation, see \cite{jeevanjee}.} the set of multilinear maps on the Cartesian product $V^*\times W^*$. Let $v, w$ be vectors in respectively $V$ and $W$ and let $g, h$ be vectors in the corresponding dual spaces. The tensor product of $v$ and $w$ is then defined as follows:
        \begin{gather}
            (v\otimes w)(g, h) := v(g)w(h).
        \end{gather}
        In this incarnation the tensor product is sometimes also known as the \textbf{outer product}.
    \end{definition}

    \newdef{Tensor component}{
        One way to define the tensor components is as follows:

        \qquad Let $\mathbf{T}$ be a tensor that takes $r$ vectors and $s$ covectors as input and returns a scalar (element of the underlying field). The components of $\mathbf{T}$ are defined as $\mathbf{T}(e_i, \ldots, e_j, e^k, \ldots, e^l) \equiv T_{i\ldots j}^{\ \ \ \ k\ldots  l}$.
    }

    The above definition can be restated as a universal property (it is also the right way to generalize tensor to infinite-dimensional spaces):
    \begin{uproperty}\index{universal!property}\label{tensor:prop:universal_property}
        Let $Z$ be a vector space. For every bilinear map $T:V\times W\rightarrow Z$ there exists a unique linear map $f:V\otimes W\rightarrow Z$ such that $T = f\circ\varphi$ where $\varphi:V\times W\rightarrow V\otimes W$ is the bilinear map $(v,w)\mapsto v\otimes w$.
    \end{uproperty}
    \begin{result}
        The tensor product is unique up to a linear isomorphism. This results in the commutativity of the tensor product:
        \begin{gather}
           \label{tensor:commutativity}
            V\otimes W \cong W\otimes V.
        \end{gather}
    \end{result}

    \begin{notation}[Tensor power]\index{tensor!power}
        \begin{gather}
            V^{\otimes n} := \underbrace{V\otimes\ldots\otimes V}_{n\text{ copies}}
        \end{gather}
    \end{notation}
    \newdef{Tensors of mixed type}{\index{tensor!type}\label{tensor:type}
        More generally, the tensor product of $r$ copies of $V$ and $s$ copies of $V^*$ is the vector space $\mathcal{T}^r_s(V) = V^{\otimes r}\otimes V^{*\otimes s}$. These tensors are said to be of \textbf{type} $(r, s)$.
    }

    The following remark is strongly related to property \ref{linalgebra:dual_space_dimension}:
    \begin{remark}
        For finite-dimensional vector spaces the space $\mathcal{T}^1_1V$ is isomorphic to $\text{End}(V)$ and the space $\mathcal{T}^1_0V$ is also isomorphic to $V$ itself.

        In general however, when we also include infinite-dimensional spaces, the space $\mathcal{T}^1_1V$ is only isomorphic to the endomorphism space $\text{End}(V^*)$. The isomorphism is given by the map $\hat{T}:V^*\rightarrow V^*:\omega\mapsto\mathbf{T}(\cdot, \omega)$ for every $\mathbf{T}\in\mathcal{T}^1_1V$. Furthermore, the spaces $\mathcal{T}^0_1V$ and $V^*$ are also isomorphic.
    \end{remark}
    \begin{definition}[Scalar]\index{scalar}
            The scalars (elements of the underlying field) are by definition the $(0,0)$-tensors.
    \end{definition}

    One can also define the tensor product space as follows:
    \begin{adefinition}[Tensor product]\index{tensor!product}
        Consider two vector spaces $V, W$ over a field $K$. First construct the free vector space $F(V\times W)$ over $K$. Then construct the subspace $N$ of $F(V\times W)$ spanned by the following elements:
        \begin{itemize}
            \item $(v+v', w) - (v, w) - (v', w)$
            \item $(v, w+w') - (v, w) - (v, w')$
            \item $(kv, w) - k(v, w)$
            \item $(v, lw) - l(v, w)$
        \end{itemize}
        where $v\in V, w\in W$ and  $k,l\in K$. The tensor product $V\otimes W$ is defined as the quotient $F(V\times W)/N$. It can be shown that this construction is associative, i.e. $U\otimes(V\otimes W)\cong(U\otimes V)\otimes W$, and as such we will drop the brackets in all expressions.

        Now, consider the case where $W=V^*$. In this case the basis of the tensor product $\mathcal{T}^r_s(V)$ will be denoted by
        \[\underbrace{e_i\otimes\ldots\otimes e_j}_{r\text{ basis vector}}\ \ \otimes\underbrace{\varepsilon^k\otimes\ldots\otimes \varepsilon^l_{\textcolor{white}{a}}}_{s\text{ dual basis vectors}}\]
        and the expansion coefficients will be denoted by $T_{i\ldots j}^{\ \ \ \ k\ldots l}$.
    \end{adefinition}

    \newprop{Dimension}{
        From the previous construction it follows that the dimension of $\mathcal{T}^r_s(V)$ is equal to $rs$.
    }

    We now have to proof that the values of the tensor operating on $r$ basis vectors and $s$ basis covectors are equal to the corresponding expansion coefficients:
    \begin{proof}
        Consider a general tensor $\mathbf{T} = T_{i\ldots j}^{\ \ \ \ k\ldots l} e_k\otimes\ldots\otimes e_l\otimes\varepsilon^i\otimes\ldots\otimes\varepsilon^j$. Applying \ref{tensor:tensor_product} and using the definition of the dual vectors \ref{linalgebra:dual_basis_2} gives us
        \begin{align*}
            \mathbf{T}(\varepsilon^m, \ldots, \varepsilon^n, e_a, \ldots, e_b) &= T_{i\ldots j}^{\ \ \ \ k\ldots l}e_k(\varepsilon^m)\ldots e_l(\varepsilon^n)\varepsilon^i(e_a)\ldots\varepsilon^j(e_b)\\
            &= T_{i\ldots j}^{\ \ \ \ k\ldots l}\delta_k^m\ldots\delta_l^n\delta_a^i\ldots\delta_b^j\\
            &= T_{a\ldots b}^{\ \ \ \ m\ldots n}.
        \end{align*}
        This is exactly the same result as the one we get by applying the first definition.\qed

        Although the first definition using multilinear maps is often the most useful one in explicit calculations it is the second one which generalizes easily to infinite-dimensional spaces.
    \end{proof}

    \newdef{Tensor algebra}{\index{tensor!algebra}\label{tensor:tensor_algebra}
        The tensor algebra over a vector space $V$ is defined as follows:
        \begin{gather}
            T(V) := \bigoplus_{k\geq0}V^{\otimes k}.
        \end{gather}
    }

\section{Transformation rules}

    In this section we consider the behaviour of tensors under basis transformations on $V$ of the form $e_i' = A^j_{\ i}e_j$.

    \begin{definition}[Contravariant]\label{tensorcalculus:contravariant}\index{contravariant}
        A tensor component that transforms by the following rule is said to be contravariant:
        \begin{gather}
            v^i = A^i_{\ j}v'^j.
        \end{gather}
    \end{definition}

    \begin{definition}[Covariant]\label{tensorcalculus:covariant}\index{covariant}
        A tensor component that transforms by the following rule is said to be covariant:
        \begin{gather}
            p_i = B^j_{\ i}\ p'_j.
        \end{gather}
    \end{definition}

    \begin{example}[Mixed tensor]
        As an example for the transformation of a mixed tensor we give the formula for the mixed third-order tensor $T_{\ ij}^k$:
        \begin{gather}
            T_{\ ij}^k = A^k_{\ w}B^u_{\ i}B^v_{\ j}T_{\ \ uv}'^w.
        \end{gather}
    \end{example}

    \begin{method}[Quotient rule]
        Assume that we are given an equation such as $K_iA^{jk} = B_i^{\ jk}$ or $K_i^{\ j}A_{jl}^{\ \ k} = B_{il}^{\ \ k}$ with $A$ and $B$ two known tensors\footnotemark. The quotient rule asserts the following: ''If the equation of interest holds under all transformations, then $K$ is a tensor of the indicated type.''
        \footnotetext{This rule does not necessarily hold when $B = 0$ because transformation rules are not well-defined for the zero-tensor.}
    \end{method}
    \sremark{This rule is a useful substitute for the ''illegal'' division of tensors.}

\section{Tensor operations}
\subsection{General operations}

    \newdef{Contraction}{\index{contraction}\label{tensor:contraction}
        Let $A$ be a tensor of type $(m, n)$. Taking a subscript and superscript to be equal number and summing over this index gives a new tensor of type $(m-1, n-1)$. This operation is called the contraction of $A$. It is induced by the evaluation map\footnote{See also definition \ref{cat:dual} for a generalization of this map.}
        \begin{gather}
            V\otimes V^*:e_i\otimes e^j\mapsto e^j(e_i).
        \end{gather}
    }

    \newdef{Direct product}{\index{direct product}
        Let $A$ and $B$ be two tensors. The tensor constructed by the componentwise multiplication of $A$ and $B$ is called the direct product of $A$ and $B$. This is a generalization of the Hadamard product \ref{linalgebra:hadamard_product}.
    }
    \begin{example}
        Let $A_{\ k}^i$ and $B_{\ lm}^j$ be two tensors. Their direct product is equal to \[C_{\ k\ lm}^{i\ j} = A_{\ k}^iB_{\ lm}^j.\]
    \end{example}

    \newformula{Operator product}{\label{tensor:operator_product}
        It is also possible to combine operators acting on different vector spaces so to make them act on the tensor product space. To this intent we use the following definition:
        \begin{gather}
            (\hat{A}\otimes\hat{B})(v\otimes w) := (\hat{A}v)\otimes(\hat{B}w).
        \end{gather}
    }
    \begin{remark*}
        Consider an operator $\hat{A}$ acting on a vector space $V_1$. When working with a tensor product space $V_1\otimes V_2$ the corresponding operator is in fact $\hat{A}\otimes\mathbbm{1}$ but it is often still denoted by $\hat{A}$ in the physics literature.
    \end{remark*}

    \begin{notation}[Symmetric part]\index{symmetric!part}
        Consider a tensor with two indices $T_{ij}$. The symmetric and antisymmetric part of $T$ are sometimes denoted by
        \begin{gather}
            T_{(ij)]} = \frac{1}{2}\left(T_{ij} + T_{ji}\right)
        \end{gather}
        and
        \begin{gather}
            T_{[ij]} = \frac{1}{2}\left(T_{ij} - T_{ji}\right).
        \end{gather}
        This notation is easily generalized to other types of tensors.
    \end{notation}

    \begin{property}[Gradient of tensor products]\index{gradient}
        The gradient of a tensor product can be defined using the Leibniz rule:
        \begin{gather}
            \nabla\cdot(\vector{A}\otimes\vector{B}) = (\nabla\cdot\vector{A})\vector{B}+(\vector{A}\cdot\nabla)\vector{B}.
        \end{gather}
    \end{property}

\subsection{Complexification}

    \newdef{Complexification}{\index{complexification}\label{tensor:complexification}
        Let $V$ be a real vector space. The complexification of $V$ is defined as the following tensor product:
        \begin{gather}
            V^{\mathbb{C}} := V\otimes\mathbb{C}.
        \end{gather}
        This space can still be considered a real vector space, but we can also turn it into a complex vector space by generalizing the scalar product as follows:
        \begin{gather}
            \alpha(v\otimes\beta) := v\otimes(\alpha\beta)
        \end{gather}
        for all $\alpha\in\mathbb{C}$.
    }
    \begin{property}\label{tensor:complexification_decomposition}
        By noting that every element $v_{\mathbb{C}}\in V^{\mathbb{C}}$ can be written as \[v_{\mathbb{C}} = (v_1\otimes1) + (v_2\otimes i),\] we can decompose the complexification as follows:
        \begin{gather}
            V^{\mathbb{C}} \cong V\oplus iV.
        \end{gather}
    \end{property}

\section{Symmetrized tensors}
\subsection{Antisymmetric tensors}

    \newdef{Antisymmetric tensor}{\index{antisymmetry}
        Tensors that change sign under the interchange of any two indices.
    }
    \begin{notation}[Symmetric tensors]
        \nomenclature[S_SnV]{$S^n(V)$}{Space of symmetric rank $n$ tensors over a vector space $V$.}
        The space of symmetric $(n, 0)$-tensors is denoted by $S^n(V)$. The space of symmetric $(0,n)$-tensors is denoted by $S^n(V^*)$.
    \end{notation}
    \begin{notation}\label{tensor:not:antysimmetric_space}
        \nomenclature[S_zsymLambda]{$\Lambda^n(V)$}{Space of antisymmetric rank $n$ tensors over a vector space $V$.}
        The space of antisymmetric $(n, 0)$-tensors is denoted by $\Lambda^n(V)$. The space of antisymmetric $(0, n)$-tensors is denoted by $\Lambda^n(V^*)$.
    \end{notation}

    \begin{property}
            Let $n = \dim(V)$. The space $\Lambda^r(V)$ equals the zero space for all $r\geq n$.
    \end{property}

\subsection{Determinant}

    \newdef{Form}{\index{form}
        An $n$-form is a totally antisymmetric element $\omega\in\mathcal{T}^0_nV$.
    }
    \newdef{Volume form}{\index{volume!form}
        A form of rank $\dim(V)$ is also called a \textbf{top form} or \textbf{volume form}.
    }

    \newdef{Determinant}{\index{determinant}
        Consider a finite-dimensional vector space $V$ with basis $\{e_i\}_{i\leq n}$. Let $\varphi$ be a tensor in $\mathcal{T}^1_1V\cong\text{End}(V)$ and let $\omega$ be a volume form on $V$. The determinant of $\varphi$ is then defined as follows:
        \begin{gather}
        \det\varphi = \frac{\omega(\varphi(e_1), \ldots, \varphi(e_n))}{\omega(e_1, \ldots, e_n)}.
        \end{gather}
        This definition is well-defined, i.e. it is independent of the choice of volume form and basis. Furthermore, it coincides with definition \ref{linalgebra:operator_determinant}.

        One should note that the determinant is only well-defined for $(1,1)$-tensors. Although other types of tensors can also be represented as matrices, definition \ref{linalgebra:operator_determinant} would not be independent of a choice of basis any more. A more general concept can be defined using the language  principal bundles and more precisely frame bundles (see section \ref{manifolds:section:principal_bundles}).
    }

\subsection{Levi-Civita symbol}

    \newdef{Levi-Civita symbol}{\index{Levi-Civita!symbol}\label{tensor:levi_civita_symbol}
        In $n$ dimensions we define the Levi-Civita symbol as follows:
        \begin{gather}
            \varepsilon_{i\ldots n} =
            \begin{cases}
            1&\text{if }(i\ldots n)\text{ is an even permutation of }(12\ldots n)\\
            -1&\text{if }(i\ldots n)\text{ is an odd permutation of }(12\ldots n)\\
            0&\text{if any of the indices occurs more than once}
            \end{cases}
        \end{gather}
    }
    \begin{remark}\label{tensor:remark:levi_civita_symbol}
        The Levi-Civita symbol is not a tensor, but a \textit{pseudotensor}. This means that the sign changes under reflections (or any transformation with determinant $-1$). To turn it into a proper tensor one should multiply it by a factor $\sqrt{g}$ where $g$ is the determinant of the metric.
    \end{remark}

    \newformula{Cross product}{\index{cross product}\label{linalgebra:cross_product}
        By using the Levi-Civita symbol, we can define the $i^{th}$ component of the cross product\footnote{Because of remark \ref{tensor:remark:levi_civita_symbol} above the cross product is in fact not a vector, but a instead it is a ''pseudovector''.} as follows:
        \begin{gather}
        (\vector{v}\times\vector{w})_i = \varepsilon_{ijk}v_jw_k.
        \end{gather}
    }
    \begin{remark}
        It is important to note that this construction is only valid in 3 dimensions.
    \end{remark}

\subsection{Wedge product}\label{tensor:section:wedge_product}

    \begin{formula}[Antisymmetrization]\label{tensors:antisymmetrization}
        Let $\{P_i\}_i = S_k$ be the permutation group, i.e. the group of all permutations of the ordered set $\{1, \ldots, k\}$. We define the antisymmetrization operator as follows:
        \begin{gather}
            \text{Alt}(e_1\otimes\ldots\otimes e_k) = \sum_i \sgn(P_i)e_{P_i(1)}\otimes\ldots\otimes e_{P_i(k)}.
        \end{gather}
    \end{formula}

    \newdef{Wedge product}{\index{wedge!product}\label{tensor:wedge_product}
        Let $\{e_i\}_{1\leq i\leq \dim(V)}$ be a basis for $V$. The wedge product of basisvectors is defined as follows:
        \begin{gather}
            e_1 \wedge\ldots\wedge e_k = \text{Alt}(e_1\otimes\ldots\otimes e_k)
        \end{gather}
        From this definition it immediately follows that the wedge product is (totally) antisymmetric.
    }

    \begin{construct}
        Let $\{e_i\}_{1 \leq i\leq \dim(V)}$ be a basis for $V$. The above definition clearly implies that a basis for $\Lambda^r(V)$ is given by
        \[\{e_{i_1}\wedge\ldots\wedge e_{i_r}\ :\ \forall k: 1\leq i_k \leq \dim(V)\}.\]
        Accordingly the dimension of this space is given by
        \begin{gather}
            \label{tensor:wedge_dimension}
            \dim\Lambda^r(V) = \binom{n}{r}.
        \end{gather}
        For $r=0$ this construction would be vacuous, so we just define $\Lambda^0(V) = \mathbb{R}$.
    \end{construct}

    \begin{formula}
        Let $v\in\Lambda^r(V)$ and $w\in\Lambda^m(V)$. The wedge product \ref{tensor:wedge_product} can be generalized as follows:
        \begin{gather}
            v\wedge w = \frac{1}{r!m!}\text{Alt}(v\otimes w)
        \end{gather}
        where the antisymmetrization operator $\text{Alt}$ was defined in equation \ref{tensors:antisymmetrization}.
    \end{formula}

    \begin{definition}[Blades]\index{blade}\index{pure!vector}
        Elements of $\Lambda^k(V)$ that can be written as the wedge product of $k$ vectors are generally known as $k$-\textbf{blades} or \textbf{pure $k$-vectors}.
    \end{definition}

    \begin{formula}\index{cross!product}
        In dimension 3 there exists an important isomorphism $J:\Lambda^2(\mathbb{R}^3)\rightarrow\mathbb{R}^3$:
        \begin{gather}
            \label{tensor:wedge_to_cross}
            J(\lambda)^i = \frac{1}{2}\varepsilon^i_{\ jk}\lambda^{jk}
        \end{gather}
        where $\lambda\in\Lambda^2(\mathbb{R}^3)$. See also the Hodge $*$-operator \ref{tensor:hodge_star} further down.

        Looking at the definition of the cross product \ref{linalgebra:cross_product}, we can see that $\vector{v}\times\vector{w}$ is actually the same as $J(\vector{v}\wedge\vector{w})$. One can thus use the wedge product to generalize the cross product to higher dimensions.
    \end{formula}

\subsection{Exterior algebra}

    \newdef{Exterior power}{\index{exterior!power}\index{form}
        In the theory of exterior algebras, the space $\Lambda^k(V)$ is often called the $k^{th}$ exterior power of $V$. As mentioned before, its elements are called (exterior) \textbf{$k$-forms}.
    }
    \newdef{Exterior algebra}{\index{exterior!algebra}\index{Grassmann!algebra}\label{tensor:exterior_algebra}
        We can define a graded vector space\footnote{See definition \ref{linalgebra:graded_vector_space}.} $\Lambda^*(V)$ as follows:
        \begin{gather}
            \Lambda^*(V) := \bigoplus_{k\geq0}\Lambda^k(V).
        \end{gather}
        This graded vector space can be turned into a graded algebra by taking the wedge product as the multiplication:
        \begin{gather}
            \wedge:\Lambda^k(V)\times\Lambda^l(V)\rightarrow \Lambda^{k+l}(V).
        \end{gather}
        This algebra is called the exterior algebra or \textbf{Grassmann algebra} over $V$. Elements of the space $\otimes_{k\text{ even}}\Lambda^k(V)$ are said to be \textbf{Grassmann-even} and elements of $\otimes_{k\text{ odd}}\Lambda^k(V)$ are said to be \textbf{Grassmann-odd}.
    }

    \newadef{$\dag$}{\label{tensor:adef_exterior_algebra}
        Let $T(V)$ be the tensor algebra over the vector space $V$, i.e.
        \begin{gather}
            T(V) = \bigoplus_{k\geq0} V^{\otimes k}.
        \end{gather}
        The exterior algebra over $V$ is in general defined as the quotient of $T(V)$ by the two-sided ideal $I$ generated by the elements $\{v\otimes v:v\in V\}$.
    }

    \begin{property}
        The exterior algebra is both a unital associative algebra (with identity $1\in K$) and a coalgebra. Furthermore, it is also commutative in the graded sense (see \ref{algebra:graded_commutativity}).
    \end{property}

    \begin{property}
        The graded commutativity implies that the wedge product of any odd exterior form with itself is identically 0. The wedge product of an even exterior form with itself vanishes if and only if the form can be decomposed as a product of 1-forms, i.e. if it is a pure $k$-form.
    \end{property}

\subsection{Hodge star}

    Equation \ref{tensor:wedge_dimension} tells us that the spaces $\Lambda^k(V)$ and $\Lambda^{n-k}(V)$ have the same dimension and hence there exists a linear isomorphism between them. This map is given by the Hodge star operator. We should however note that this map can only be defined independently of the choice of (ordered) basis if we restrict ourselves to vector spaces equipped with a nondegenerate Hermitian form \ref{linalgebra:NDH_form}.

    When equipped with an inner product and hence an orthonormal basis $\{e_i\}_{i\leq\dim V}$, every finite-dimensional vector space admits a canonical volume form given by
    \begin{gather}
        \text{Vol} = e_1\wedge\ldots\wedge e_n.
    \end{gather}
    This will also be the convention adopted in the remainder of this section.

    \newdef{Orientation}{\index{orientation}\label{tensor:orientation}
        Let $\text{Vol}(V)$ be the standard volume form on the vector space $V$ as defined above. From the definition of a volume form it follows that every other $\dim(V)$-form is a scalar multiple of $\text{Vol}(V)$ (let's denote this number by $r$). Hence the choice of volume form induces an orientation on $V$: If $r>0$ then the orientation is said to be \textbf{positive}, if $r<0$ then the orientation is said to be \textbf{negative}.
    }

    \begin{formula}[Inner product]\index{inner!product}
        Let $V$ be equipped with an inner product $\langle\cdot,\cdot\rangle$. Then we can extend this to an inner product on $\Lambda^k(V)$ by first defining it on decomposable forms as follows (and extending it by linearity to all of $\Lambda^k(V)$):
        \begin{gather}
            \label{tensor:wedge_inner_product}
            \langle v_1\wedge\ldots\wedge v_k | w_1\wedge\ldots\wedge w_k \rangle_k = \det(\langle v_i|w_j\rangle).
        \end{gather}
        For an orthogonal basis this formula factorizes into
        \begin{gather}
            \langle v_1\wedge\ldots\wedge v_k | w_1\wedge\ldots\wedge w_k \rangle_k = \langle v_1|w_1 \rangle\cdots \langle v_k|w_k \rangle.
        \end{gather}
    \end{formula}
    \newdef{Hodge star}{\index{Hodge!star}\label{tensor:hodge_star}
        The Hodge star $\ast: \Lambda^k(V)\rightarrow\Lambda^{n-k}(V)$ is defined as the unique isomorphism $\ast$ such that for all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ we have the following equality:
        \begin{gather}
            \omega\wedge\rho = \langle\ast\omega|\rho\rangle_{n-k}\text{Vol}(V)
        \end{gather}
        where $\langle\cdot|\cdot\rangle_{n-k}$ is the inner product \ref{tensor:wedge_inner_product} on $\Lambda^{n-k}(V)$. The element $\ast\omega$ is often called the \textbf{(Hodge) dual} of $\omega$.
        \begin{proof}
            Fix an element $\omega\in\Lambda^k(V)$, then for every element $\rho\in\Lambda^{n-k}(V)$ we see that $\omega\wedge\rho$ is an element of $\Lambda^n(V)$ and as such is a scalar multiple of $\text{Vol}(V)$. This implies that it can be written as \[c_\omega(\rho)\text{Vol}(V).\] The map $c_\omega:\Lambda^{n-k}(V)\rightarrow\mathbb{R}:\rho\mapsto c_\omega(\rho)$ is a bounded linear map and thus a continuous map, so we can apply Riesz' representation theorem to identify $c$ with a unique element $\ast\omega\in\Lambda^{n-k}(V)$ such that \[c_\omega(\rho) = \langle\ast\omega|\rho\rangle_{n-k}.\]
        \qed
        \end{proof}
    }

    \begin{formula}
        Let $\{e_i\}_{i\leq n}$ be a positively oriented ordered orthonormal (possibly in a Lorentzian signature) basis for $V$. An explicit formula for the Hodge star is given by the following construction:

        \qquad Let $\{i_1, \ldots, i_k\}$ and $\{j_1 , \ldots,j_{n-k}\}$ be two complementary index sets (with increasing subindices) and consider an element $\omega = e_{i_1}\wedge\ldots\wedge e_{i_k}\in\Lambda^k(V)$.
        \begin{gather}
            \label{tensor:explicit_hodge_star}
            \ast\omega =\sgn(\tau)\prod_{m = 1}^{n-k}\langle e_{j_m}|e_{j_m} \rangle e_{j_1}\wedge\ldots\wedge e_{j_{n-k}}
        \end{gather}
        where $\tau$ is the permutation that maps $e_{i_1}\wedge\ldots\wedge e_{i_k}\wedge e_{j_1}\wedge\ldots\wedge e_{j_{n-k}}$ to $\text{Vol}(V)$.
    \end{formula}

    Using this formula one can easily prove the following important property:
    \begin{property}
        Consider an inner product space $V$. The double dual satisfies the following equation:
        \begin{gather}
            \ast\ast\ \omega = (-1)^{k(n-k)}\omega.
        \end{gather}
    \end{property}

    Taking the defining relation of the Hodge star operator together with the above property implies the following formula (which is often found in the literature as the defining relation):
    \begin{formula}
        For all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ the Hodge star operator satisfies the following formula:
        \begin{gather}
            \omega\wedge\ast\rho = \langle\omega|\rho\rangle\text{Vol}(V)
        \end{gather}
    \end{formula}

    \begin{result}\label{tensor:hodge_star_vectorcalculus}
        Consider three vectors $u, v, w\in\mathbb{R}^3$.
        \begin{align}
            \ast(v\wedge w) &= v\times w \label{tensor:cross_by_hodge_star}\\
            \ast(v\times w) &= v\wedge w\\
            \ast(u\wedge v\wedge w) &= u\cdot(v\times w)
        \end{align}
    \end{result}
    \begin{remark}
        Formula \ref{tensor:wedge_to_cross} is an explicit evaluation of the first equation \ref{tensor:cross_by_hodge_star}.
        \begin{proof}
            The sign $\sgn(\tau)$ can be written using the Levi-Civita symbol $\varepsilon_{ijk}$ as defined in \ref{tensor:levi_civita_symbol}. The factor $\frac{1}{2}$ is introduced to correct for the double counting due to the contraction over both the indices $j$ and $k$.
        \end{proof}
    \end{remark}

    \newdef{Self-dual}{\index{dual!self-dual}
        Let $V$ be a 4-dimensional inner product space. Consider $\omega\in\Lambda^2(V)$. Then $\omega$ is said to be self-dual if $\ast\omega = \omega$. Furthermore every $\rho\in\Lambda^2(V)$ can be uniquely decomposed as the sum of a self-dual and an anti-self-dual 2-form.
    }

\section{Berezin calculus}

    Although this section does not really belong in the current chapter about tensors, we have included it here as it is an application of (or variation on) the concept of exterior algebras. The concept of Grassmann numbers/variables is used in QFT when performing calculations in the fermionic sector (and in Faddeev-Popov quantization). References for this section are \cite{losev_berezin, AMP2}.

    \newdef{Grassmann numbers}{\index{Grassmann!number}\label{tensor:grassmann_number}
        Let $V$ be a vector space spanned by a set of elements $\theta_i$. The Grassmann algebra with Grassmann variables $\theta_i$ is the exterior algebra over $V$. The wedge symbol of Grassmann variables is often ommitted when writing the product: \[\theta_i\wedge\theta_j \equiv \theta_i\theta_j.\]
    }
    \begin{remark}
        From the (anti)commutativity it follows that we can regard the Grassmann variables as being nonzero square roots of zero.
    \end{remark}

    \begin{property}
        Consider a one-dimensional Grassmann algebra (with generator $\theta$). When constructing the polynomial ring $\mathbb{C}[\theta]$ generated by $\theta$, we see that, due to the anticommutativity, $\mathbb{C}[\theta]$ is spanned only by $1$ and $\theta$. All higher degree terms vanish because $\theta^2 = 0$. This implies that the most general polynomial over a one-dimensional Grassmann algebra is of the form
        \begin{gather}
            p(\theta) = a + b\theta
        \end{gather}
    \end{property}

    \begin{definition}\index{DeWitt!convention}
        We can equip the exterior algebra $\Lambda$ with Grassmann variables $\theta_i$ with an involution:
        \begin{gather}
            (\theta_i\theta_j\ldots\theta_k)^* := \theta_k\ldots\theta_j\theta_i.
        \end{gather}
        Elements $z\in\Lambda$ such that $z^* = z$ are called \textbf{(super)real}, elements such that $z^* = -z$ are called \textbf{(super)imaginary}. This convention is called the \textbf{DeWitt convention}.
    \end{definition}

    To keep this discussion on Grassmann variables self-contained we will also introduce the calculus of Grassmann variables here:
    \newdef{Derivative of Grassmann variables}{
        Consider the polynomial algebra $\mathbb{C}[\theta_1,\ldots,\theta_n]$ on $n$ Grassmann variables (more general functions would be defined through a series expansion, but given that $\theta^2=0$, these always reduce to a simple polynomial). Differentiation on this ring is defined through the following relations:
        \begin{gather}
            \pderiv{\theta_i}{\theta_j} = \delta^i_j\qquad\qquad\qquad\theta_i\pderiv{}{\theta_j}+\pderiv{}{\theta_j}\theta_i = 0
        \end{gather}
        The second relation implies that the partial derivatives are also Grassmann-odd.
    }

    Next to differentiation we also need some kind of integration theory. Instead of working with a definition \`a la Riemann the integral will be defined purely axiomatically:
    \newdef{Berezin integral: axiomatic}{\index{Berezin!integral}
        Consider a function$f$ defined on $n$ Grassmann variables $\{\theta_i\}_{1\leq i\leq n}$. The Berezin integral is defined by the following axioms:
        \begin{enumerate}
            \item The map $f\mapsto\int_Bf(\theta)d\theta$ is linear.
            \item The result $\int_Bf(\theta)d\theta$ is independent of the variable(s) $\theta$, i.e. it is a number.
            \item The result is invariant under a translation of the integration variable.
        \end{enumerate}
    }

    It can be shown that this definition is equivalent to the following one:
    \newadef{Berezin integral: formulaic}{
        First consider functions on one Grassmann variable, i.e. $f(\theta) = a+b\theta$. The Berezin integral is then defined as follows:\footnote{Technically the axioms only imply this formula up to some multiplicative constant. We will follow the original paper by Berezin and choose this constant to be $1$.}
        \begin{gather}
            \int_B(a+b\theta)d\theta := b.
        \end{gather}
        This means that the integral is equal to the coefficient of the ''highest'' degree term. As a simple generalization we define
        \begin{gather}
            \int_Bf(\theta_1,\ldots,\theta_n)d\theta := \text{coefficient of }\theta_1\cdots\theta_n.
        \end{gather}
        Some authors reverse the order of the variables in the above definition. Depending on the number of variables, this might lead to a minus sign.
    }
    \remark{It is interesting to see that the (one-dimensional) Berezin integral is equal to the (Grassmann) derivative. This is completely different from the usual integral in calculus. It also gives some intuition for the distinct transformation behaviour of the Berezin integral as explained in the following property.}

    \begin{formula}[Change of variables]
        Consider a general Berezin integral $\int_Bf(\theta)d\theta$. Now suppose that we perform a transformation on the Grassmann variables $\theta\rightarrow\xi(\theta)$. If $J$ is the Jacobian matrix associated to this transformation then the transformation is given by the following formula:
        \begin{gather}
            \int_Bf(\xi)d\xi = \int_Bf(\theta)(\det J)^{-1}d\theta
        \end{gather}
    \end{formula}

    The Berezin calculus above can easily be integrated in ordinary calculus by using the fact that ordinary coordinates (even parity) commute with Grassmann numbers (odd parity). A mixed derivative (resp. integral) can always be factorized as the composition of a Berezin derivative (resp. integral) and an ordinary one.\footnote{The order is merely a convention.} The transformation behaviour is then generalized to this case by introducing the so-called superdeterminant or Berezinian (see section \ref{section:graded_spaces} and in particular definition \ref{linalgebra:berezinian}):

    \qquad Consider a set of ordinary coordinates $\{x_i\}_{1\leq i\leq m}$ and a set of Grassmann variables $\{\theta_i\}_{1\leq i\leq n}$. Now assume that we perform a general coordinate transformation that may possibly mix up the ordinary and Grassmann variables, i.e. we obtain new even coordinates $y(x, \theta)$ (these can thus contain even combinations of the Grassmann variables) and ordd variables $\xi(x, \theta)$. The Jacobian of this transformation can be written as a block matrix \[J=\begin{pmatrix}A&B\\C&D\end{pmatrix}\] where $A$ determines the mixing in the ordinary sector, $D$ determines the mixing in the Grassmann sector and $B, C$ determine the mixing between sectors. From the choice of transformation it should be clear that this is an even supermatrix. The Jacobian determinant in the transformation formula should now be replaced by the Berezian:
    \begin{gather}
        \text{Ber}(J) = \det(A - BD^{-1}C)\det(D)^{-1}
    \end{gather}
    We immediately see that if there is no mixing between the ordinary and Grassmann sector, i.e. $B=C=0$, then this reduces to the simple cases above.
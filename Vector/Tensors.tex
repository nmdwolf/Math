\chapter{Vector \& Tensor Calculus}

References for this chapter are \cite{jeevanjee}.

\section{Nabla-operator}\label{vectorcalculus:nabla}
	
	\begin{definition}[Nabla]\index{nabla}
		\begin{equation}
        		\nabla\equiv\left(\pderiv{}{x}, \pderiv{}{y}, \pderiv{}{z}\right)
		\end{equation}
	\end{definition}

	Following formulas can be found by using basic properties of (vector) calculus.    
	\newformula{Gradient}{\index{gradient}
		\begin{equation}
			\label{vectorcalculus:gradient}
			\nabla V = \left(\pderiv{V_x}{x}, \pderiv{V_y}{y}, \pderiv{V_z}{z}\right)
		\end{equation}
	}
	\begin{formula}
		Let $\varphi(\vector{x})$ be a scalar field. The total differential $d\varphi$ can be rewritten as
	        \begin{equation}
			d\varphi = \nabla\varphi\cdot d\vec{r}
		\end{equation}
	\end{formula}
    
	\begin{property}
		The gradient of a scalar function $V$ is perpendicular to the level sets \ref{set:level_set} of $V$.
	\end{property}
    
	\newdef{Directional derivative}{\index{directional derivative}
	    	Let $\vec{a}$ be a unit vector. The directional derivative $\nabla_{\vector{a}}V$ is defined as the change of the function $V$ in the direction of $\vec{a}$:
	    	\begin{equation}
			\label{vectorcalculus:directional_derivative}
		        \nabla_{\vector{a}}V \equiv (\vec{a}\cdot\nabla)V
		\end{equation}
	}
	\begin{example}
		Let $\varphi(\vector{x})$ be a scalar field. Let $\vec{t}$ denote the tangent vector to a curve $\vec{r}(s)$ with $s$ natural parameter. The variation of the scalar field $\varphi(\vector{x})$ along $\vec{r}(s)$ is given by
	        \begin{equation}
			\pderiv{\varphi}{s} = \deriv{\vec{r}}{s}\cdot\nabla\varphi
		\end{equation}
	\end{example}
    
	\newdef{Conservative vector field}{\index{conservative}
	    	A vector field obtained as the gradient of a scalar function.
	}
	\begin{property}
		A vector field is conservative if and only if its line integral is path independent.
	\end{property}
	
	\newformula{Gradient of tensor}{
		Let $T$ be a tensor field with coordinates $x^i$. Let $\vector{e}^i(x^1, x^2, x^3)$ be a curvilinear orthogonal frame\footnote{See definition \ref{diff:frame}.}. The gradient of $T$ is defined as follows:
		\begin{equation}
			\nabla T = \pderiv{T}{x^i}\otimes\vector{e}^i
		\end{equation}
	}
	
	\newformula{Divergence}{\index{divergence}
		\begin{equation}
			\label{vectorcalculus:divergence}
			\nabla\cdot\vector{A} = \pderiv{A_x}{x} + \pderiv{A_y}{y} + \pderiv{A_z}{z}
		\end{equation}
	}
	\newdef{Solenoidal vector field}{\index{solenoidal}
		A vector field $\vector{V}(\vector{x})$ is said to be solenoidal if it satisfies:
		\begin{equation}
			\nabla\cdot\vector{V} = 0
		\end{equation}
		It is also known as a \textbf{divergence free vector field}.
	}

	\newformula{Rotor / curl}{\index{curl}\index{rotor|see{curl}}
		\begin{equation}
			\label{vectorcalculus:rotor}
			\nabla\times\vector{A} = \left(\pderiv{A_z}{y} - \pderiv{A_y}{z}, \pderiv{A_x}{z} - \pderiv{A_z}{x}, \pderiv{A_y}{x} - \pderiv{A_x}{y}\right)
		\end{equation}
	}
    
	\newdef{Irrotational vector field}{\index{irrotational}
		A vector field $\vector{V}(\vector{x})$ is said to be irrotational if it satisfies:
	    	\begin{equation}
	    		\nabla\times\vector{V} = 0
	    	\end{equation}
	}
	\begin{remark}
		All conservative vector fields are irrotational but irrotational vector fields are only conservative if the domain is simply-connected\footnote{See definition \ref{topology:simply_connected}.}
	\end{remark}

\subsection{Laplacian}

	\newdef{Laplacian}{\index{Laplace!operator}
		\begin{equation}
			\label{vectorcalculus:laplacian}
			\bigtriangleup V\equiv\nabla^2V = \mpderiv{2}{V}{x} + \mpderiv{2}{V}{y} + \mpderiv{2}{V}{z}
		\end{equation}
		\begin{equation}
			\label{vectorcalculus:vector_laplacian}
		        \nabla^2\vector{A} = \nabla\left(\nabla\cdot\vector{A}\right) - \nabla\times \left(\nabla\times\vector{A}\right)
		\end{equation}
	}
	\remark{Equation \ref{vectorcalculus:vector_laplacian} is called the \textbf{vector laplacian}.}
    
	\newformula{Laplacian in different coordinate systems}{\ 
	    	\begin{itemize}
		        \item Cylindrical coordinates $(\rho,\phi,z)$:
		    	        \begin{equation}
		        	    	\label{laplacian:cylindrical}
					\stylefrac{1}{\rho}\pderiv{}{\rho}\left(\rho\pderiv{}{\rho}\right) + \stylefrac{1}{\rho^2}\mpderiv{2}{}{\phi} + \mpderiv{2}{}{z}
				\end{equation}
		        \item Spherical coordinates $(r,\phi,\theta)$:
	        		\begin{equation}
					\label{laplacian:spherical}
                    			\stylefrac{1}{r^2}\left[\pderiv{}{r}\left(r^2\pderiv{}{r}\right) + \stylefrac{1}{\sin^2\theta}\mpderiv{2}{}{\phi} + \stylefrac{1}{\sin\theta}\pderiv{}{\theta}\left(\sin\theta\pderiv{}{\theta}\right)\right]
				\end{equation}
		\end{itemize}
	}
    
\subsection[Mixed properties]{Mixed properties\footnotemark}\label{vectorcalculus:mixed_properties}
	\footnotetext{See remark \ref{forms:vector_calculus} for a differential geometric approach.}
	
	\begin{equation}
		\label{vectorcalculus:rotor_of_gradient}
        	\nabla \times \left(\nabla V\right) = 0
	\end{equation}
	\begin{equation}
		\label{vectorcalculus:divergence_of_rotor}
	        \nabla \cdot \left(\nabla \times \vector{V}\right) = 0
	\end{equation}
    
	In Cartesian coordinates equation \ref{vectorcalculus:vector_laplacian} can be rewritten as follows:
	\begin{equation}
		\label{vectorcalculus:vector_laplacian_carthesian}
		\nabla^2\vector{A} = \left(\bigtriangleup A_x, \bigtriangleup A_y, \bigtriangleup A_z\right)
	\end{equation}
    
\subsection{Helmholtz decomposition}

	\newformula{Helmholtz decomposition}{\index{Helmholtz!decomposition}
		Let $\vector{P}$ be a vector field that decays rapidly (more than $1/r$) when $r\rightarrow\infty$. $\vector{P}$ can be written as follows:
	        \begin{equation}
			\label{vectorcalculus:helmholtz_decomposition}
		        \vector{P} = \nabla\times\vector{A} + \nabla V
		\end{equation}
	}

\section{Line integrals}\index{line!integral}\index{path!integral|see{line integral}}

	\newformula{Line integral of a continuous function}{\label{vectorcalculus:line_integral_scalar}
	    	Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ be a continuous function. Let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $f$ over $\Gamma$ as follows:
        	\begin{equation}
			\int_\Gamma f(s)ds = \int_a^b f(\vector{\varphi}(t))||\vector{\varphi}'(t)||dt
		\end{equation}
	}
	\newformula{Line integral of a continuous vector field}{\label{vectorcalculus:line_integral_vector}
	    	Let $\vector{F}$ be a continuous vector field. Let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $F$ over $\Gamma$ as follows:
        	\begin{equation}
			\int_\Gamma \vector{F}(\vector{s})\cdot d\vector{s} = \int_a^b \vector{F}(\vector{\varphi}(t))\cdot\vector{\varphi}'(t)dt
		\end{equation}
	}

\section[Integral theorems]{Integral theorems\footnotemark}
	\footnotetext{These theorems follow from the more general Stokes' theorem \ref{forms:theorem:stokes_theorem}.}

	\begin{theorem}[Fundamental theorem of calculus for line integrals]\index{Fundamental theorem!for line integrals}~\newline
	    	Let $\vec\Gamma:\mathbb{R}\rightarrow\mathbb{R}^3$ be a smooth curve.
		\begin{equation}
			\label{vectorcalculus:fundamental_theorem}
		        \int_{\Gamma(a)}^{\Gamma(b)}\nabla f(\vector{r})\cdot d\vector{r} = \varphi(\Gamma(b)) - \varphi(\Gamma(a))
		\end{equation}
	\end{theorem}
        
	\begin{theorem}[Kelvin-Stokes' theorem]\index{Stokes!Kelvin-Stokes theorem}
	    	\begin{equation}
			\label{vectorcalculus:stokes_theorem}
		        \oint_{\partial S}\vector{A}\cdot d\vector{l} = \iint_S \left(\nabla \times \vector{A}\right)dS
		\end{equation}
	\end{theorem}
    
	\begin{theorem}[Divergence theorem\footnotemark]\index{divergence!theorem}
	    	\footnotetext{Also known as \textit{Gauss's theorem} or the \textit{Gauss-Ostrogradsky theorem}.}
	    	\begin{equation}
			\label{vectorcalculus:divergence_theorem}
		        \oiint_{\partial V}\vector{A}\cdot d\vector{S} = \iiint_V \left(\nabla \cdot \vector{A}\right)dV
		\end{equation}
	\end{theorem}
	\begin{result}[Green's identity]\index{Green!identity}
	    	\begin{equation}
			\label{vectorcalculus:green_indentity}
		        \oiint_{\partial V}\left(\psi\nabla\phi - \phi\nabla\psi\right)\cdot d\vector{S} = \iiint_V \left(\psi\nabla^2\phi - \phi\nabla^2\psi\right) dV
		\end{equation}
	\end{result}
    
\section{Curvilinear coordinates}

	In this section the differential operators are generalized to curvilinear coordinates. To do this we need the scale factors as formally defined in equation \ref{diff:scale_factor}. Also there is no Einstein summation used, all summations are written explicitly.
    
	\newformula{Unit vectors}{
	    	\begin{equation}
			\pderiv{\vec{r}}{q^i} = h_i\hat{e}_i
		\end{equation}
	}
	\newformula{Gradient}{
	    	\begin{equation}
			\nabla V = \sum_{i=1}^3\stylefrac{1}{h_i}\pderiv{V}{q^i}\hat{e}_i
		\end{equation}
	}
	\newformula{Divergence}{
	    	\begin{equation}
			\nabla\cdot\vector{A} = \stylefrac{1}{h_1h_2h_3}\left(\pderiv{}{q^1}(A_1h_2h_3) + \pderiv{}{q^2}(A_2h_3h_1) + \pderiv{}{q^3}(A_3h_1h_2)\right)
		\end{equation}
	}
	\newformula{Rotor}{
	   	\begin{equation}
			(\nabla\times\vector{A})_i = \stylefrac{1}{h_jh_k}\left(\pderiv{}{q^j}(A_kh_k) - \pderiv{}{q^k}(A_jh_j)\right)
		\end{equation}
	        where $i\neq j\neq k$.
	}

\section{Tensor product}
\subsection{Tensor product}\index{tensor!product}
	\nomenclature[O_zsymbinten]{$X\otimes Y$}{Tensor product of the vector spaces $X$ and $Y$.}

	There are two possible ways to introduce the components of a tensor (on finite dimensional spaces). One way is to interpret tensors as multilinears maps another way is to interpret the components as expansion coefficients with respect to the tensor space basis.
    
	\begin{definition}\label{tensor:tensor_product}
    		The tensor product of vector spaces $V$ and $W$ is defined as\footnotemark\ the set of multilinear maps on the Cartesian product $V^*\times W^*$. Let $v, w$ be vectors in respectively $V$ and $W$. Let $g, h$ be vectors in the corresponding dual spaces. The tensor product of $v$ and $w$ is then defined as:
    		\footnotetext{\textit{isomorphic to} would be a better terminology. See the "universal property" \ref{tensor:prop:universal_property}. For a complete proof and explanation, see \cite{principal_bundles}.}
        	\begin{equation}
        		\boxed{(v\otimes w)(g, h) = v(g)w(h)}
	        \end{equation}
	\end{definition}
	
	\newdef{Tensor component}{
    		One way to define the tensor components is as follows: Let $\mathbf{T}$ be a tensor that takes $r$ vectors and $s$ covectors as input and returns a scalar. The different components are given by  $\mathbf{T}(e_i, ..., e_j, e^k, ...., e^l) = T_{i...j}^{\ \ \ \ k...l}$.
	}
    
	The following property can also be seen as the defining property of a tensor product (also in the case of infinite-dimensional spaces):
	\begin{uproperty}\index{universal!property}\label{tensor:prop:universal_property}
	   	Let $Z$ be a vector space. For every bilinear map $T:V\times W\rightarrow Z$ there exists a linear map $f:V\otimes W\rightarrow Z$ such that $T = f\circ\varphi$, where $\varphi$ is the bilinear map $V\times W\rightarrow V\otimes W$.
	\end{uproperty}
	\begin{result}
	    	The tensor product is unique up to a linear isomorphism. This results in the commutativity of the tensor product:
	    	\begin{equation}
			\label{tensor:prop:change}
	        	V\otimes W \cong W\otimes V
		\end{equation}
		where the isomorphism is explicitly given by:
	        \begin{equation}
	        	v(f) \equiv f(v)
	        \end{equation}
	        for all $v\in V$ and $f\in V^*$.
	\end{result}
    
	\begin{notation}[Tensor power]\index{tensor!power}
	    	\begin{equation}
	    		V^{\otimes n} = \underbrace{V\otimes...\otimes V}_{n\text{ copies}}
	    	\end{equation}
	\end{notation}
	\begin{remark}
	    	More generally, the tensor product of $r$ copies of $V$ and $s$ copies of $V^*$ is the vector space $\mathcal{T}^r_s(V) = V^{\otimes r}\otimes V^{*\otimes s}$. These tensors are said to be of \textbf{type} $(r, s)$.
	\end{remark}
	
	\begin{remark}
		Generally the space $\mathcal{T}^1_1V$ is only isomorphic to the space $\text{End}(V^*)$. The isomorphism is given by the map $\hat{T}:V^*\rightarrow V^*:\omega\mapsto\mathbf{T}(\cdot, \omega)$ for every $\mathbf{T}\in\mathcal{T}^1_1V$. Furthermore the spaces $\mathcal{T}^0_1V$ and $V^*$ are isomorphic.
		
		For finite-dimensional vector spaces the space $\mathcal{T}^1_1V$ is also isomorphic to $\text{End}(V)$ (see property \ref{linalgebra:dual_space_dimension}) and the space $\mathcal{T}^1_0V$ will also be isomorphic to $V$ itself.
	\end{remark}
	\begin{definition}
	    	The scalars (elements of the base field $K$) are by definition the $(0,0)$ tensors.
	\end{definition}

	\begin{adefinition}\index{tensor!type}\label{tensor:type}
    		The tensor space $\mathcal{T}^r_s(V)$ is spanned by the elements
    		\[\underbrace{e_i\otimes...\otimes e_j}_{r\text{ basis vector}}\otimes\underbrace{\varepsilon^k\otimes...\otimes \varepsilon^l_{\textcolor{white}{a}}}_{s\text{ dual basis vectors}}\]
    		where the operation $\otimes$ satisfies following properties:
        	\begin{enumerate}
        		\item Associativity: $u\otimes(v\otimes w) = u \otimes v\otimes w$
		        \item Multilinearity: $a(v\otimes w) = (av)\otimes w = v\otimes (aw)$ and $v\otimes (u+w) = v\otimes u + v\otimes w$
	        \end{enumerate}
	        The expansion coefficients in this basis are written as $T^{i...j}_{\ \ \ \ k...l}$
	\end{adefinition}

	\newprop{Dimension of tensor product}{
	    	From the previous construction it follows that the dimension of $\mathcal{T}^r_s(V)$ is equal to $rs$.
	}
    
    	We now have to proof that the values of the tensor operating on $r$ basis vectors and $s$ basis covectors are equal to the corresponding expansion coefficients:
      	\begin{proof}
        	Let $\mathbf{T} = T_{i...j}^{\ \ \ \ k...l}e^i\otimes...\otimes e^j\otimes e_k\otimes...\otimes e_l$. Applying \ref{tensor:tensor_product} and using the definition of the dual vectors \ref{linalgebra:dual_basis_2} we have:
		\[
            	\begin{array}{ccl}
            		\mathbf{T}(e_a, ..., e_b, \varepsilon^m, ..., \varepsilon^n) &=& T_{i...j}^{\ \ \ \ k...l}e^i(e_a)...e^j(e_b)e_k(e^m)...e_l(e^n)\\
                	&=& T_{i...j}^{\ \ \ \ k...l}\delta_a^i...\delta_b^j\delta_k^m...\delta_l^n\\
                	&=& T_{a...b}^{\ \ \ \ m...n}
	        \end{array}
		\]
		This is exactly the same result as the one we get by applying the first definition.\qed
      	\end{proof}
      	
      	\newdef{Tensor algebra}{\index{tensor!algebra}\label{tensor:tensor_algebra}
      		The tensor algebra over a vector space $V$ is defined as follows:
      		\begin{equation}
      			T(V) = \bigoplus_{k\geq0}V^{\otimes k}
      		\end{equation}
      	}

\subsection{Quotient space}

	On infinite-dimensional spaces there exists a more general definition (that coincides with the previous one on finite-dimensional spaces\footnote{This can be checked using the universal property.}):
	\begin{construct}[Tensor product]\index{tensor!product}
		Consider two vector spaces $V, W$ over a field $K$. First construct the free vector space $F(V\times W)$ over $K$. Then construct the subspace $N$ of $F(V\times W)$ spanned by the following elements:
		\begin{itemize}
			\item $(v+v', w) - (v, w) - (v', w)$
			\item $(v, w+w') - (v, w) - (v, w')$
			\item $(kv, w) - k(v, w)$
			\item $(v, lw) - l(v, w)$
		\end{itemize}
		where $v\in V, w\in W$ and  $k,l\in K$. The tensor product $V\otimes W$ is then given by the quotient $F(V\times W)/N$.
	\end{construct}

\section{Transformation rules}

	Let the basis for $V$ transform as $e_i' = A^j_{\ i}e_j$ and $e_i = B^j_{\ i}e_j'$. Because the basis transformation should be well-defined, the operators $A$ and $B$ are each other's inverses: $B = A^{-1}$.

	\begin{definition}[Contravariant]
		A tensor component that transforms by the following rule is called contravariant:
	        \begin{equation}
			\label{tensorcalculus:contravariant}
		        v^i = A^i_{\ j}v'^j
		\end{equation}
	\end{definition}
    
	\begin{definition}[Covariant]
		A tensor component that transforms by the following rule is called covariant:
	        \begin{equation}
			\label{tensorcalculus:covariant}
		        p_i = B^j_{\ i}\ p'_j
		\end{equation}
	\end{definition}
    
	\begin{example}[Mixed tensor]
		As an example of a mixed tensor we give the transformation formula for the mixed third-order tensor $T_{\ ij}^k$:
	        \[
	        	T_{\ ij}^k = A^k_{\ w}B^u_{\ i}B^v_{\ j}T_{\ \ uv}'^w
	        \]
	\end{example}

	\begin{theorem}[Quotient rule]
		Assume we have an equation such as $K_iA^{jk} = B_i^{\ jk}$ or $K_i^{\ j}A_{jl}^{\ \ k} = B_{il}^{\ \ k}$ with $A$ and $B$ two known tensors\footnotemark. The quotient rule asserts the following: "If the equation of interest holds under all transformations, then $K$ is a tensor of the indicated rank and covariant/contravariant character".
		\footnotetext{This rule does not necessarily hold when $B = 0$ as transformations rules are not defined for the null-tensor.}
	\end{theorem}
	\sremark{This rule is a useful substitute for the "illegal" division of tensors.}

\section{Tensor operations}
\subsection{General operations}

	\newdef{Contraction}{\index{contraction}
	    	Let $A$ be a tensor of type $(n, m)$. Setting a sub- and superscript equal and summing over this index gives a new tensor of type $(n-1, m-1)$. This operation is called the contraction of $A$. It is given by the evaluation map
	    	\begin{equation}
	    		\label{tensor:contraction}
	    		V\otimes V^*:e_i\otimes e^j\mapsto e^j(e_i)
	    	\end{equation}
	}
    
	\newdef{Direct product}{\index{direct product}
	    	Let $A$ and $B$ be two random tensors (both rank and co-/contravariancy). The tensor constructed by the componentwise multiplication of $A$ and $B$ is called the direct product of $A$ and $B$.
	}
	\begin{example}
		Let $A_{\ k}^i$ and $B_{\ lm}^j$ be two tensors. The direct product is equal to: \[C_{\ k\ lm}^{i\ j} = A_{\ k}^iB_{\ lm}^j\]
	\end{example}
    
	\newformula{Operator product}{
    		It is also possible to combine operators working on different vector spaces so to make them work on the tensor product space. To do this we use following definition:
        	\begin{equation}
	        	\label{tensor:operator_product}
			\boxed{(\hat{A}\otimes\hat{B})(v\otimes w) = (\hat{A}v)\otimes(\hat{B}w)}
	        \end{equation}
	}
	\begin{remark*}
	    	Consider an operator $\hat{A}$ working on a space $V_1$. When working with a combined space $V_1\otimes V_2$ the corresponding operator is in fact $\hat{A}\otimes\mathbbm{1}$ but it is often still denoted by $\hat{A}$ in physics.
	\end{remark*}
	
	\begin{notation}
		Consider a tensor with two indices $T_{ij}$. The antisymmetric part is written as follows:
		\begin{equation}
			T_{[ij]} = \frac{1}{2}\left(T_{ij} - T_{ji}\right)
		\end{equation}
	\end{notation}
	
\subsection{Determinant}

	\newdef{Form}{\index{form}
		An $n$-form is a totally antisymmetric element $\omega\in\mathcal{T}^0_nV$.
	}
	\newdef{Volume form}{\index{volume!form}
		A form of rank $\dim V$ is also called a \textbf{top form} or \textbf{volume form}.
	}

	\newdef{Determinant}{\index{determinant}
		Let $V$ be finite-dimensional with basis $\{e_i\}_{i\leq n}$. Let $\varphi$ be a tensor in $\mathcal{T}^1_1V\cong\text{End}(V)$ and let $\omega$ be a volume form. The determinant of $\varphi$ is then defined as:
		\begin{equation}
			\det\varphi = \frac{\omega(\varphi(e_1), ..., \varphi(e_n))}{\omega(e_1, ..., e_n)}
		\end{equation}
		This definition is well-defined, i.e. it is independent of the choice of volume form and basis. Furthermore it coincides with definition \ref{linalgebra:operator_determinant}.
		
		One should note that the determinant is only well-defined for $(1,1)$-tensors. Although other types of tensors can also be represented as matrices, definition \ref{linalgebra:operator_determinant} would not be independent of a choice of basis anymore. An alternative concept can be defined using principal bundles and more precisely frame bundles (see section \ref{manifolds:section:principal_bundles}).
	}
	
	\begin{result}
		\begin{equation}
			\omega(e_1, ..., e_{i-1}, X, e_{i+1}, ..., e_n) = X_i
		\end{equation}
	\end{result}

\subsection{Differentiation}
	
	\begin{property}
		\begin{equation}
        		\nabla\cdot(\vector{A}\otimes\vector{B}) = (\nabla\cdot\vector{A})\vector{B}+(\vector{A}\cdot\nabla)\vector{B}
		\end{equation}
	\end{property}    
    
\subsection{Levi-Civita tensor}
	
	\newdef{Levi-Civita tensor}{\index{Levi-Civita!symbol}
    		Let $e^i$ be the dual vector to $e_i$. In $n$ dimensions, we define the Levi-Civita tensor as follows:
    		\begin{equation}
        		\label{tensor:levi_civita_symbol}
			\boldsymbol{\varepsilon} = \varepsilon_{12...n}e^1\otimes e^2\otimes...\otimes e^n
        	\end{equation}
		where
        	\[
        	\varepsilon_{i...n} = 
        	\left\{
        	\begin{array}{rcl}
			1&\qquad&\text{if }(i...n)\text{ is an even permutation of }(12...n)\\
                	-1&\qquad&\text{if }(i...n)\text{ is an odd permutation of }(12...n)\\
	                0&\qquad&\text{if any of the indices occurs more than once}
		\end{array}
        	\right.
		\]
	}
	\begin{remark}\label{tensor:remark:levi_civita_symbol}
		The Levi-Civita symbol is not a tensor, but a pseudotensor. This means that the sign changes under reflections (or any transformation with determinant $-1$). To turn it into a proper tensor one should multiply it by a factor $\sqrt{g}$ where $g$ is the determinant of the metric.
    	\end{remark}

	\newformula{Cross product}{
		By using the Levi-Civita symbol, we can define the $i$-th component of the cross product\footnotemark\ as follows:
		\footnotetext{Following from remark \ref{tensor:remark:levi_civita_symbol} we can see that the cross product is in fact not a vector, but a pseudovector.}
		\begin{equation}
			\boxed{(\vector{v}\times\vector{w})_i = \varepsilon_{ijk}v_jw_k}
		\end{equation}
	}

\subsection{Complexification}

	\newdef{Complexification}{\index{complexification}\label{tensor:complexification}
		Let $V$ be a real vector space. The complexification of $V$ is defined as the following tensor product:
		\begin{equation}
			V^{\mathbb{C}} = V\otimes\mathbb{C}
		\end{equation}
		As such this is still a real vector space. However we can turn this space into a complex vector space by generalizing the scalar product as follows:
		\begin{equation}
			\alpha(v\otimes\beta) = v\otimes(\alpha\beta)
		\end{equation}
		for all $\alpha,\beta\in\mathbb{C}$.
	}
	\begin{property}
		By noting that every element $v_{\mathbb{C}}\in V^{\mathbb{C}}$ can be written as \[v_{\mathbb{C}} = (v_1\otimes1) + (v_2\otimes i)\] we can decompose the complexification as follows:
		\begin{equation}
			V^{\mathbb{C}} \cong V\oplus iV
		\end{equation}
	\end{property}

\section{Symmetrized tensors}
\subsection{Symmetric tensors}

	\begin{notation}
		\nomenclature[S_SnV]{$S^n(V)$}{Space of symmetric rank $n$ tensors over a vector space $V$.}
		The space of symmetric $(n, 0)$ tensors is denoted by $S^n(V)$. The space of symmetric $(0,n)$ tensors is denoted by $S^n(V^*)$.
	\end{notation}
    
\subsection{Antisymmetric tensors}

	\newdef{Antisymmetric tensor}{\index{antisymmetry}
		Tensors that change sign under the interchange of any two indices.
	}
	\begin{notation}\label{tensor:not:antysimmetric_space}
		\nomenclature[S_zsymLambda]{$\Lambda^n(V)$}{Space of antisymmetric rank $n$ tensors over a vector space $V$.}
		The space of antisymmetric $(0,n)$ tensors is denoted by $\Lambda^n(V^*)$. The space of antisymmetric $(n, 0)$ tensors is denoted by $\Lambda^n(V)$.
	\end{notation}

	\begin{remark*}\index{bivector}\index{blade}
    		Elements of $\Lambda^2(V)$ are also known as \textbf{bivectors}. Elements of $\Lambda^k(V)$ are generally known as $k$-\textbf{blades}.
	\end{remark*}
    
	\begin{property}
    		Let $n = \dim(V)$. $\Lambda^r(V)$ equals the null-space for all $r\geq n$.
	\end{property}
    
\subsection{Wedge product}

	\begin{formula}[Antisymmetrization]
		Let $\{P_i\}_i$ be the set of all permutations of the sequence $(1, ..., k)$.
		\begin{equation}
			\label{tensors:antisymmetrization}
			\text{Alt}(e_1\otimes...\otimes e_k) = \sum_i \sgn(P_i)e_{P_i(1)}\otimes...\otimes e_{P_i(k)}
		\end{equation}
	\end{formula}

	\newdef{Wedge product}{\index{wedge!product}
		Let $\{e_i\}_{1\leq i\leq \dim(V)}$ be a basis for $V$.
		\begin{equation}
			\label{tensor:wedge_product}
    			e_1 \wedge ... \wedge e_k = \text{Alt}(e_1\otimes...\otimes e_k)
    		\end{equation}
		From this definition it immediately follows that the wedge product is (totally) antisymmetric.
	}
    
	\begin{construct}
    		Let $\{e_i\}_{1 \leq i\leq \dim(V)}$ be a basis for $V$. It is clear from the definition \ref{tensor:wedge_product} that a basis for $\Lambda^r(V)$ is given by
		\[
			\{e_{i_1}\wedge...\wedge e_{i_r}\ :\ \forall k: 1\leq i_k \leq \dim(V)\}
		\]
		The dimension of this space is given by:
		\begin{equation}
			\label{tensor:wedge_dimension}
			\dim\Lambda^r(V) = \binom{n}{r}
		\end{equation}
		From the antisymmetry it follows that for $r>\dim(V)$ the spaces $\Lambda^r(V)$ are zero.
	\end{construct}
	\begin{remark}
		For $k=0$, the above construction is not useful, so we just define $\Lambda^0(V) = \mathbb{R}$.
	\end{remark}
	
	\begin{formula}
		Let $v\in\Lambda^r(V)$ and $w\in\Lambda^m(V)$.
		\begin{equation}
			v\wedge w = \frac{1}{r!m!}\text{Alt}(v\otimes w)
		\end{equation}
		where the antisymmetrization operator $\text{Alt}$ is defined in equation \ref{tensors:antisymmetrization}.
	\end{formula}
    
    \newformula{Levi-Civita symbol}{\index{Levi-Civita!symbol}
    	The Levi-Civita tensor in $n$ dimensions as introduced in \ref{tensor:levi_civita_symbol} can now be rewritten more concisely as:
        \begin{equation}
        	\boldsymbol{\varepsilon} = e_1\wedge...\wedge e_n
        \end{equation}
    }
    
    \begin{formula}\index{cross!product}
    	In 3 dimensions there exists an important isomorphism $J:\Lambda^2(\mathbb{R}^3)\rightarrow\mathbb{R}^3$:
        \begin{equation}
		\label{tensor:wedge_to_cross}
	        	J(\lambda)^i = \frac{1}{2}\varepsilon^i_{\ jk}\lambda^{jk}
        \end{equation}
        where $\lambda\in\Lambda^2(\mathbb{R}^3)$.

	Looking at the definition of the cross product \ref{linalgebra:cross_product}, we can see that $\vector{v}\times\vector{w}$ is actually the same as $J(\vector{v}\wedge\vector{w})$. One can thus use the wedge product to generalize the cross product to higher dimensions.
    \end{formula}
    
    \begin{example}
    	Let $A, B$ and $C$ be three vectors in $V$. Now consider following expression:
        \[
        	(C\wedge B)(L(A), \cdot)
        \]
        where $L(A)$ is the metric dual of $A$ (see \ref{linalgebra:metric_dual}). Evaluating this formula using the properties of the wedge and tensor products leads to the well known BAC-CAB rule of triple cross products:
        \[
        	(C\cdot A)B - (B\cdot A)C
        \]
    \end{example}
    
\subsection{Exterior algebra}
	
	\newdef{Exterior power}{\index{exterior!power}\index{form}
		In the theory of exterior algebras, the space $\Lambda^k(V)$ is often called the $k^{th}$ exterior power of $V$. Its elements are called (exterior) \textbf{$k$-forms}.
	}
	\newdef{Exterior algebra}{\index{exterior!algebra}\index{Grassmann!algebra}\label{tensor:exterior_algebra}
		We can define a graded vector space\footnotemark\ $\Lambda^*(V)$ as follows:
		\[
			\Lambda^*(V) = \bigoplus_{k\geq0}\Lambda^k(V)
		\]
		Then we can turn this graded vector space into a graded algebra by taking the wedge product as the multiplication:
		\[
			\wedge:\Lambda^k(V)\times\Lambda^l(V)\rightarrow \Lambda^{k+l}(V)
		\]
		This algebra is called the exterior algebra or \textbf{Grassmann algebra} over $V$.
		\footnotetext{See definition \ref{linalgebra:graded_vector_space}.}
	}
	
	\newadef{$\dag$}{\label{tensor:adef_exterior_algebra}
    		Let $T(V)$ be the tensor algebra over the vector space $V$, i.e.
	    	\begin{equation}
			T(V) = \bigoplus_{k\geq0} V^{\otimes k}
		\end{equation}
		The exterior algebra over $V$ is generally defined as the quotient of $T(V)$ by the two-sided ideal $I$ generated by $\{v\otimes v:v\in V\}$.
	}
	
	\begin{property}
		The exterior algebra is both a unital associative algebra with unit element $1\in\mathbb{R}$ and a coalgebra. Furthermore it is also commutative in the graded sense (see \ref{group:graded_commutativity}).
	\end{property}
	
	\begin{property}
		The graded commutativity implies that the wedge product of any odd exterior form with itself is identically 0. The wedge product of an even exterior form with itself vanishes if and only if the form can be decomposed as a product of 1-forms. 
	\end{property}
	
\subsection{Hodge star}

	It follows from equation \ref{tensor:wedge_dimension} that the spaces $\Lambda^k(V)$ and $\Lambda^{n-k}(V)$ have the same dimension, so there exists an isomorphism between them. This map is given by the Hodge star $\ast$. However this map can only be defined independent of the choice of (ordered) basis if we restrict ourselves to vector spaces equipped with a non-degenerate Hermitian form \ref{linalgebra:NDH_form}.

	When equipped with an inner product and hence an orthonormal basis $\{e_i\}$, every finite-dimensional vector space admits a canonical volume form given by Vol$ = e_1\wedge...\wedge e_n$. This will be the convention adopted in the remainder of this section.

	\newdef{Orientation}{\index{orientation}\label{tensor:orientation}
		Let Vol be the choice of volume form on the vector space $V$. From the definition of a volume form it follows that every other $\dim(V)$-blade is a scalar multiple of $\text{Vol}$. Hence the choice of volume form induces an orientation on $V$: if the scalar $r>0$ then the orientation is said to be \textbf{positive}, if $r<0$ then the orientation is \textbf{negative}.
	}
	
	\begin{formula}[Inner product]\index{inner!product}
		Let $V$ be equipped with an inner product $\langle\cdot,\cdot\rangle$. Then we can define an inner product on $\Lambda^k(V)$ by:
		\begin{equation}
			\label{tensor:wedge_inner_product}
			\boxed{\langle v_1\wedge...\wedge v_k | w_1\wedge...\wedge w_k \rangle_k = \det(\langle v_i, w_j\rangle)}
		\end{equation}
		For an orthogonal basis, this formula factorises into:
		\begin{equation}
			\langle v_1\wedge...\wedge v_k | w_1\wedge...\wedge w_k \rangle_k = \langle v_1|w_1 \rangle\cdots \langle v_k|w_k \rangle
		\end{equation}
	\end{formula}
	\newdef{Hodge star}{\index{Hodge star}
		The Hodge star $\ast: \Lambda^k(V)\rightarrow\Lambda^{n-k}(V)$ is defined as the isomorphism such that for all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ we have the following equality:
		\begin{equation}
			\label{tensor:hodge_star}
			\omega\wedge\rho = \langle\ast\omega, \rho\rangle_{n-k}\text{Vol}(V)
		\end{equation}
		where $\langle\cdot,\cdot\rangle$ is the inner product \ref{tensor:wedge_inner_product} on $\Lambda^{n-k}(V)$. Furthermore, this isomorphism is unique.
		\begin{proof}
			Because $\omega\wedge\rho$ is an element of $\Lambda^n(V)$ it is a scalar multiple of $\text{Vol}(V)$. This implies that it can be written as \[c(\rho)\text{Vol}(V)\] The map $c : \Lambda^{n-k}(V)\rightarrow\mathbb{R}:\rho\mapsto c(\rho)$ is a linear map and thus a continuous map, so we can apply Riesz' representation theorem to identify $c$ with a unique element $\ast\omega\in\Lambda^{n-k}(V)$ such that \[c(\rho) = \langle\ast\omega, \rho\rangle_{n-k}\]
		\qed
		\end{proof}
	}
	
	\begin{formula}
		Let $\{e_i\}_{i\leq n}$ be a positively oriented ordered orthonormal (possibly in a Lorentzian signature) basis for $V$. An explicit formula for the Hodge star is given by the following construction. Let $\{i_1, ..., i_k\}$ and $\{j_1 ,...,j_{n-k}\}$ be two complementary index sets with increasing subindices. Let $\omega = e_{i_1}\wedge...\wedge e_{i_k}$.
		\begin{equation}
			\label{tensor:explicit_hodge_star}
			\boxed{\ast\omega =\sgn(\tau)\prod_{m = 1}^{n-k}\langle e_{j_m}|e_{j_m} \rangle e_{j_1}\wedge...\wedge e_{j_{n-k}}}
		\end{equation}
		where $\tau$ is the permutation that maps $e_{i_1}\wedge...\wedge e_{i_k}\wedge e_{j_1}\wedge...\wedge e_{j_{n-k}}$ to $\text{Vol}(V)$.
	\end{formula}
	
	\begin{result}
		\label{tensor:hodge_star_vectorcalculus}
		Consider three vectors $u, v, w\in\mathbb{R}^3$.
		\begin{align}
			\ast(v\wedge w) &= v\times w \label{tensor:cross_by_hodge_star}\\
			\ast(v\times w) &= v\wedge w\\
			\ast(u\wedge v\wedge w) &= u\cdot(v\times w)
		\end{align}
	\end{result}
	\begin{remark}
		Formula \ref{tensor:wedge_to_cross} is an explicit evaluation of the first equation \ref{tensor:cross_by_hodge_star}.
		\begin{proof}
			The sign $\sgn(\tau)$ can be written using the Levi-Civita symbol $\varepsilon_{ijk}$ as defined in \ref{tensor:levi_civita_symbol}. The factor $\frac{1}{2}$ is introduced to correct for the double counting due to the contraction over both the indices $j$ and $k$.
		\end{proof}
	\end{remark}
	
	\begin{property}
		Consider an inner product space $V$, then
		\begin{equation}
			\boxed{\ast\ast\ \omega = (-1)^{k(n-k)}\omega}
		\end{equation}
		In $n=4$ this leads to $\ast\ast\omega = \omega$ which means that the Hodge star is an involution in 4-dimensional inner product spaces.
	\end{property}
	
	\newdef{Self-dual}{\index{dual!self-dual}
		Let $V$ be a 4-dimensional inner product space. Consider $\omega\in\Lambda^2(V)$. Then $\omega$ is said to be self-dual if $\ast\omega = \omega$. Furthermore every $v\in\Lambda^2(V)$ can be uniquely decomposed as the sum of a self-dual and an anti-self-dual 2-form.
	}
	
\subsection{Grassmann numbers}

	Although this section does not really belong to the chapter about tensors, we have included it here as it is an application of the concept of exterior algebras. The concept of Grassmann numbers (or variables) is used in QFT when performing calculations in the fermionic sector.
	
	\newdef{Grassmann numbers}{\index{Grassmann!number}\label{tensor:grassmann_number}
		Let $V$ be a complex vector space spanned by a set of generators $\theta_i$. The Grassmann algebra with Grassmann variables $\theta_i$ is the exterior algebra over $V$. The wedge symbol of Grassmann variables is often ommitted when writing the product: $\theta_i\wedge\theta_j \equiv \theta_i\theta_j$.
	}
	\begin{remark}
		Furthermore, from the anti-commutativity it follows that we can regard the Grassmann variables as being non-zero square-roots of zero.
	\end{remark}
	
	\begin{property}
		Consider a one-dimensional Grassmann algebra. When constructing the polynomial ring $\mathbb{C}[\theta]$ generated by $\theta$, we see that, due to the anti-commutativity, $\mathbb{C}[\theta]$ is spanned only by $1$ and $\theta$. All higher degree terms vanish because $\theta^2 = 0$. This implies that the most general polynomial over a one-dimenisonal Grassmann algebra can be written as
		\begin{equation}
			p(\theta) = a + b\theta
		\end{equation}
	\end{property}
	
	\begin{definition}\index{DeWitt!convention}
		We can equip the exterior algebra $\Lambda$ with Grassmann variables $\theta_i$ with an involution similar to that on $\mathbb{C}$:
		\begin{equation}
			(\theta_i\theta_j...\theta_k)^* = \theta_k...\theta_j\theta_i
		\end{equation}
		Elements $z\in\Lambda$ such that $z^* = z$ are called \textbf{(super)real}, elements such that $z^* = -z$ are called \textbf{(super)imaginary}. This convention is called the \textit{DeWitt} convention.
	\end{definition}

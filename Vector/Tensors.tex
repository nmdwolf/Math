\chapter{Vector \& Tensor Calculus}

References for this chapter are \cite{jeevanjee}. For a more geometric approach to some of the concepts and results in this chapter we refer to the content of chapters \ref{chapter:curves_surfaces}, \ref{diff:chapter:vector_bundles} and \ref{chapter:integration_manifolds}.

\section{Nabla-operator}\label{vectorcalculus:nabla}

	\sremark{The geometric approach to this section is summarized in remark \ref{forms:vector_calculus}.}
	
	\begin{definition}[Nabla]\index{nabla}
		\begin{gather}
        		\nabla\equiv\left(\pderiv{}{x}, \pderiv{}{y}, \pderiv{}{z}\right)
		\end{gather}
	\end{definition}

	\newdef{Gradient}{\index{gradient}
		\begin{gather}
			\label{vectorcalculus:gradient}
			\nabla V = \left(\pderiv{V_x}{x}, \pderiv{V_y}{y}, \pderiv{V_z}{z}\right)
		\end{gather}
	}
	\begin{formula}
		Let $\varphi(\vector{x})$ be a scalar field. The total differential $d\varphi$ can be rewritten as
	        \begin{gather}
			d\varphi = \nabla\varphi\cdot d\vec{r}.
		\end{gather}
	\end{formula}
    
	\begin{property}
		The gradient of a scalar function $V$ is perpendicular to the level sets \ref{set:level_set} of $V$.
	\end{property}
    
	\newdef{Directional derivative}{\index{directional derivative}
	    	Let $\hat{a}$ be a unit vector. The directional derivative $\nabla_{\hat{a}}V$ is defined as the change of the function $V$ in the direction of $\hat{a}$:
	    	\begin{gather}
			\label{vectorcalculus:directional_derivative}
		        \nabla_{\hat{a}}V = (\hat{a}\cdot\nabla)V.
		\end{gather}
	}
	\begin{example}
		Let $\varphi(\vector{x})$ be a scalar field. Let $\vec{t}$ denote the tangent vector to a curve $\vec{r}(s)$ with natural parameter\footnote{See definition \ref{diff:natural_parameter}.} $s$. The variation of the scalar field $\varphi(\vector{x})$ along $\vec{r}(s)$ is given by
	        \begin{gather}
			\pderiv{\varphi}{s} = \nabla\varphi\cdot\deriv{\vec{r}}{s}.
		\end{gather}
	\end{example}
    
	\newdef{Conservative vector field}{\index{conservative}
	    	A vector field obtained as the gradient of a scalar function.
	}
	\begin{property}
		A vector field is conservative if and only if its line integral is path independent, i.e. it only depends on the value at the end points. This is an instance of Stokes' theorem \ref{forms:theorem:stokes_theorem}.
	\end{property}
	
	\newdef{Gradient of a tensor}{\index{gradient}
		Let $T$ be a tensor field with coordinates $x^i$. Let $\vector{e}^{\,i}(x^1, x^2, x^3)$ be a curvilinear orthogonal frame\footnote{See definition \ref{diff:frame}.}. The gradient of $T$ is defined as follows:
		\begin{gather}
			\nabla T = \pderiv{T}{x^i}\otimes\vector{e}^{\,i}.
		\end{gather}
	}
	
	\newdef{Divergence}{\index{divergence}
		\begin{gather}
			\label{vectorcalculus:divergence}
			\nabla\cdot\vector{A} = \pderiv{A_x}{x} + \pderiv{A_y}{y} + \pderiv{A_z}{z}
		\end{gather}
	}
	\newdef{Solenoidal vector field}{\index{solenoidal}
		A vector field $\vector{V}(\vector{x})$ is said to be solenoidal if it satisfies
		\begin{gather}
			\nabla\cdot\vector{V} = 0.
		\end{gather}
		It is also known as a \textbf{divergence free vector field}.
	}

	\newdef{Rotor / curl}{\index{curl}\index{rotor|see{curl}}
		\begin{gather}
			\label{vectorcalculus:rotor}
			\nabla\times\vector{A} = \left(\pderiv{A_z}{y} - \pderiv{A_y}{z}, \pderiv{A_x}{z} - \pderiv{A_z}{x}, \pderiv{A_y}{x} - \pderiv{A_x}{y}\right)
		\end{gather}
	}
    
	\newdef{Irrotational vector field}{\index{irrotational}
		A vector field $\vector{V}(\vector{x})$ is said to be irrotational if it satisfies:
	    	\begin{gather}
	    		\nabla\times\vector{V} = 0
	    	\end{gather}
	}
	\begin{remark}
		All conservative vector fields are irrotational but irrotational vector fields are only conservative if the domain is simply-connected\footnote{See definition \ref{topology:simply_connected}.}.
	\end{remark}

\subsection{Laplacian}

	\newdef{Laplacian}{\index{Laplace!operator}
		\begin{gather}
			\label{vectorcalculus:laplacian}
			\bigtriangleup V\equiv\nabla^2V = \mpderiv{2}{V}{x} + \mpderiv{2}{V}{y} + \mpderiv{2}{V}{z}
		\end{gather}
		\begin{gather}
			\label{vectorcalculus:vector_laplacian}
		        \nabla^2\vector{A} = \nabla\left(\nabla\cdot\vector{A}\right) - \nabla\times \left(\nabla\times\vector{A}\right)
		\end{gather}
	}
	\remark{The operator defined by equation \ref{vectorcalculus:vector_laplacian} is sometimes called the \textbf{vector Laplacian}.}

	\newformula{Laplacian in different coordinate systems}{\ 
	    	\begin{itemize}
		        \item Cylindrical coordinates $(\rho,\phi,z)$:
		    	        \begin{gather}
		        	    	\label{laplacian:cylindrical}
					\stylefrac{1}{\rho}\pderiv{}{\rho}\left(\rho\pderiv{}{\rho}\right) + \stylefrac{1}{\rho^2}\mpderiv{2}{}{\phi} + \mpderiv{2}{}{z}
				\end{gather}
		        \item Spherical coordinates $(r,\phi,\theta)$:
	        		\begin{gather}
					\label{laplacian:spherical}
                    			\stylefrac{1}{r^2}\left[\pderiv{}{r}\left(r^2\pderiv{}{r}\right) + \stylefrac{1}{\sin^2\theta}\mpderiv{2}{}{\phi} + \stylefrac{1}{\sin\theta}\pderiv{}{\theta}\left(\sin\theta\pderiv{}{\theta}\right)\right]
				\end{gather}
		\end{itemize}
	}
    
\subsection[Mixed properties]{Mixed properties}\label{vectorcalculus:mixed_properties}
	
	\begin{gather}
		\label{vectorcalculus:rotor_of_gradient}
        	\nabla \times \left(\nabla V\right) = 0
	\end{gather}
	\begin{gather}
		\label{vectorcalculus:divergence_of_rotor}
	        \nabla \cdot \left(\nabla \times \vector{V}\right) = 0
	\end{gather}
    
	In Cartesian coordinates equation \ref{vectorcalculus:vector_laplacian} can be rewritten as follows:
	\begin{gather}
		\label{vectorcalculus:vector_laplacian_carthesian}
		\nabla^2\vector{A} = \left(\bigtriangleup A_x, \bigtriangleup A_y, \bigtriangleup A_z\right)
	\end{gather}
    
\subsection{Helmholtz decomposition}

	\newformula{Helmholtz decomposition}{\index{Helmholtz!decomposition}
		Let $\vector{P}$ be a vector field that decays rapidly (faster than $1/r$) when $r\rightarrow\infty$. $\vector{P}$ can be written as follows:
	        \begin{gather}
			\label{vectorcalculus:helmholtz_decomposition}
		        \vector{P} = \nabla\times\vector{A} + \nabla V.
		\end{gather}
	}

\subsection{Curvilinear coordinates}

	In this section the differential operators are generalized to curvilinear coordinates. To do this we need the scale factors as formally defined in equation \ref{diff:scale_factor}. Also there is no Einstein summation used, all summations are written explicitly.
    
	\newformula{Unit vectors}{
	    	\begin{gather}
			\pderiv{\vec{r}}{q^i} = h_i\hat{e}_i
		\end{gather}
	}
	\newformula{Gradient}{
	    	\begin{gather}
			\nabla V = \sum_{i=1}^3\stylefrac{1}{h_i}\pderiv{V}{q^i}\hat{e}_i
		\end{gather}
	}
	\newformula{Divergence}{
	    	\begin{gather}
			\nabla\cdot\vector{A} = \stylefrac{1}{h_1h_2h_3}\left(\pderiv{}{q^1}(A_1h_2h_3) + \pderiv{}{q^2}(A_2h_3h_1) + \pderiv{}{q^3}(A_3h_1h_2)\right)
		\end{gather}
	}
	\newformula{Rotor}{
	   	\begin{gather}
			(\nabla\times\vector{A})_i = \stylefrac{1}{h_jh_k}\left(\pderiv{}{q^j}(A_kh_k) - \pderiv{}{q^k}(A_jh_j)\right)
		\end{gather}
	        where $i\neq j\neq k$.
	}

\section{Integration}
\subsection{Line integrals}\index{line!integral}\index{path!integral|see{line integral}}

	\newformula{Line integral of a continuous function}{\label{vectorcalculus:line_integral_scalar}
	    	Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ be a continuous function. Let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $f$ over $\Gamma$ as follows:
        	\begin{gather}
			\int_\Gamma f(s)ds = \int_a^b f(\vector{\varphi}(t))||\vector{\varphi}'(t)||dt
		\end{gather}
	}
	\newformula{Line integral of a continuous vector field}{\label{vectorcalculus:line_integral_vector}
	    	Let $\vector{F}$ be a continuous vector field. Let $\Gamma$ be a piecewise smooth curve with parametrization $\vector{\varphi}(t), t\in [a, b]$. We define the line integral of $F$ over $\Gamma$ as follows:
        	\begin{gather}
			\int_\Gamma \vector{F}(\vector{s})\cdot d\vector{s} = \int_a^b \vector{F}(\vector{\varphi}(t))\cdot\vector{\varphi}'(t)dt
		\end{gather}
	}

\subsection[Integral theorems]{Integral theorems\footnotemark}
	\footnotetext{These theorems follow from the more general Stokes' theorem \ref{forms:theorem:stokes_theorem}.}

	\begin{theorem}[Fundamental theorem of calculus for line integrals]\index{Fundamental theorem!for line integrals}
	    	Let $\vec\Gamma:\mathbb{R}\rightarrow\mathbb{R}^3$ be a smooth curve defined on the interval $[a,b]$.
		\begin{gather}
			\label{vectorcalculus:fundamental_theorem}
		        \int_{\Gamma(a)}^{\Gamma(b)}\nabla f(\vector{r})\cdot d\vector{r} = \varphi(\Gamma(b)) - \varphi(\Gamma(a))
		\end{gather}
	\end{theorem}
        
	\begin{theorem}[Kelvin-Stokes' theorem]\index{Stokes!Kelvin-Stokes theorem}
	    	\begin{gather}
			\label{vectorcalculus:stokes_theorem}
		        \oint_{\partial S}\vector{A}\cdot d\vector{l} = \iint_S \left(\nabla \times \vector{A}\right)dS
		\end{gather}
	\end{theorem}
    
	\begin{theorem}[Divergence theorem\footnotemark]\index{divergence!theorem}
	    	\footnotetext{Also known as \textbf{Gauss' theorem} or the \textbf{Gauss-Ostrogradsky theorem}.}
	    	\begin{gather}
			\label{vectorcalculus:divergence_theorem}
		        \oiint_{\partial V}\vector{A}\cdot d\vector{S} = \iiint_V \left(\nabla \cdot \vector{A}\right)dV
		\end{gather}
	\end{theorem}
	\begin{result}[Green's identity]\index{Green!identity}
	    	\begin{gather}
			\label{vectorcalculus:green_indentity}
		        \oiint_{\partial V}\left(\psi\nabla\phi - \phi\nabla\psi\right)\cdot d\vector{S} = \iiint_V \left(\psi\nabla^2\phi - \phi\nabla^2\psi\right) dV
		\end{gather}
	\end{result}

\section{Tensors}
\subsection{Tensor product}\index{tensor!product}
	\nomenclature[O_zsymbinten]{$V\otimes W$}{Tensor product of the vector spaces $V$ and $W$.}

	There are two possible ways to introduce the concept of a \textit{tensor} (on finite-dimensional spaces). One way is to interpret tensors as multilinears maps and another way is to interpret the components as expansion coefficients with respect to some tensor space basis.
    
	\begin{definition}\label{tensor:tensor_product}
    		The tensor product of vector spaces $V$ and $W$ is defined as\footnotemark\ the set of multilinear maps on the Cartesian product $V^*\times W^*$. Let $v, w$ be vectors in respectively $V$ and $W$. Let $g, h$ be vectors in the corresponding dual spaces. The tensor product of $v$ and $w$ is then defined as:
    		\footnotetext{\textit{isomorphic to} would be a better terminology. See the "universal property" \ref{tensor:prop:universal_property}. For a complete proof and explanation, see \cite{jeevanjee}.}
        	\begin{gather}
        		(v\otimes w)(g, h) = v(g)w(h)
	        \end{gather}
	\end{definition}
	
	\newdef{Tensor component}{
    		One way to define the tensor components is as follows:
    		
    		\hspace{1cm} Let $\mathbf{T}$ be a tensor that takes $r$ vectors and $s$ covectors as input and returns a scalar. The different components are given by  $\mathbf{T}(e_i, ..., e_j, e^k, ...., e^l) = T_{i...j}^{\ \ \ \ k...l}$.
	}
    
	The above definition can be restated as a universal property (it is also the right generalization for infinite-dimensional spaces):
	\begin{uproperty}\index{universal!property}\label{tensor:prop:universal_property}
	   	Let $Z$ be a vector space. For every bilinear map $T:V\times W\rightarrow Z$ there exists a unique linear map $f:V\otimes W\rightarrow Z$ such that $T = f\circ\varphi$ for a certain bilinear map $\varphi:V\times W\rightarrow V\otimes W$ (which is part of the definition).
	\end{uproperty}
	\begin{result}
	    	The tensor product is unique up to a linear isomorphism. This results in the commutativity of the tensor product:
	    	\begin{gather}
			\label{tensor:commutativity}
	        	V\otimes W \cong W\otimes V.
		\end{gather}
	\end{result}
    
	\begin{notation}[Tensor power]\index{tensor!power}
	    	\begin{gather}
	    		V^{\otimes n} = \underbrace{V\otimes...\otimes V}_{n\text{ copies}}
	    	\end{gather}
	\end{notation}
	\newdef{Tensors of mixed type}{
	    	More generally, the tensor product of $r$ copies of $V$ and $s$ copies of $V^*$ is the vector space $\mathcal{T}^r_s(V) = V^{\otimes r}\otimes V^{*\otimes s}$. These tensors are said to be of \textbf{type} $(r, s)$.
	}
	
	\begin{remark}
		Generally the space $\mathcal{T}^1_1V$ is only isomorphic to the space $\text{End}(V^*)$. The isomorphism is given by the map $\hat{T}:V^*\rightarrow V^*:\omega\mapsto\mathbf{T}(\cdot, \omega)$ for every $\mathbf{T}\in\mathcal{T}^1_1V$. Furthermore the spaces $\mathcal{T}^0_1V$ and $V^*$ are isomorphic.
		
		For finite-dimensional vector spaces the space $\mathcal{T}^1_1V$ is\footnote{See property \ref{linalgebra:dual_space_dimension}.} also isomorphic to $\text{End}(V)$ and the space $\mathcal{T}^1_0V$ will also be isomorphic to $V$ itself.
	\end{remark}
	\begin{definition}[Scalar]\index{scalar}
	    	The scalars (elements of the base field $K$) are by definition the $(0,0)$-tensors.
	\end{definition}

	\begin{adefinition}\index{tensor!type}\label{tensor:type}
    		The tensor space $\mathcal{T}^r_s(V)$ is spanned by the elements
    		\[\underbrace{e_i\otimes...\otimes e_j}_{r\text{ basis vector}}\ \ \otimes\underbrace{\varepsilon^k\otimes...\otimes \varepsilon^l_{\textcolor{white}{a}}}_{s\text{ dual basis vectors}}\]
    		where the operation $\otimes$ satisfies following properties:
        	\begin{enumerate}
        		\item Associativity: $u\otimes(v\otimes w) = u \otimes v\otimes w$
		        \item Multilinearity: $a(v\otimes w) = (av)\otimes w = v\otimes (aw)$ and $v\otimes (u+w) = v\otimes u + v\otimes w$
	        \end{enumerate}
	        The expansion coefficients in this basis are written as $T^{i...j}_{\ \ \ \ k...l}$
	\end{adefinition}

	\newprop{Dimension of tensor product}{
	    	From the previous construction it follows that the dimension of $\mathcal{T}^r_s(V)$ is equal to $rs$.
	}
    
    	We now have to proof that the values of the tensor operating on $r$ basis vectors and $s$ basis covectors are equal to the corresponding expansion coefficients:
      	\begin{proof}
        	Consider a general tensor $\mathbf{T} = T_{i...j}^{\ \ \ \ k...l}e^i\otimes...\otimes e^j\otimes e_k\otimes...\otimes e_l$. Applying \ref{tensor:tensor_product} and using the definition of the dual vectors \ref{linalgebra:dual_basis_2} we have:
		\begin{align*}
            		\mathbf{T}(e_a, ..., e_b, \varepsilon^m, ..., \varepsilon^n) &= T_{i...j}^{\ \ \ \ k...l}e^i(e_a)...e^j(e_b)e_k(e^m)...e_l(e^n)\\
                	&= T_{i...j}^{\ \ \ \ k...l}\delta_a^i...\delta_b^j\delta_k^m...\delta_l^n\\
                	&= T_{a...b}^{\ \ \ \ m...n}
	        \end{align*}
		This is exactly the same result as the one we get by applying the first definition.\qed
      	\end{proof}
      	
      	\newdef{Tensor algebra}{\index{tensor!algebra}\label{tensor:tensor_algebra}
      		The tensor algebra over a vector space $V$ is defined as follows:
      		\begin{gather}
      			T(V) = \bigoplus_{k\geq0}V^{\otimes k}
      		\end{gather}
      	}

\subsection{General definition}

	On infinite-dimensional spaces there exists a more general definition (that coincides with the previous one on finite-dimensional spaces\footnote{This can be checked using the universal property.}):
	\begin{construct}[Tensor product]\index{tensor!product}
		Consider two vector spaces $V, W$ over a field $K$. First construct the free vector space $F(V\times W)$ over $K$. Then construct the subspace $N$ of $F(V\times W)$ spanned by the following elements:
		\begin{itemize}
			\item $(v+v', w) - (v, w) - (v', w)$
			\item $(v, w+w') - (v, w) - (v, w')$
			\item $(kv, w) - k(v, w)$
			\item $(v, lw) - l(v, w)$
		\end{itemize}
		where $v\in V, w\in W$ and  $k,l\in K$. The tensor product $V\otimes W$ is then given by the quotient $F(V\times W)/N$.
	\end{construct}

\section{Transformation rules}

	Let the basis for $V$ transform as $e_i' = A^j_{\ i}e_j$ and $e_i = B^j_{\ i}e_j'$. Because the basis transformation should be well-defined, the operators $A$ and $B$ are each other's inverses: $B = A^{-1}$.

	\begin{definition}[Contravariant]\label{tensorcalculus:contravariant}\index{contravariant}
		A tensor component that transforms by the following rule is called contravariant:
	        \begin{gather}
		        v^i = A^i_{\ j}v'^j.
		\end{gather}
	\end{definition}
    
	\begin{definition}[Covariant]\label{tensorcalculus:covariant}\index{covariant}
		A tensor component that transforms by the following rule is called covariant:
	        \begin{gather}
		        p_i = B^j_{\ i}\ p'_j.
		\end{gather}
	\end{definition}
    
	\begin{example}[Mixed tensor]
		As an example of a mixed tensor we give the transformation formula for the mixed third-order tensor $T_{\ ij}^k$:
	        \begin{gather}
	        	T_{\ ij}^k = A^k_{\ w}B^u_{\ i}B^v_{\ j}T_{\ \ uv}'^w.
	        \end{gather}
	\end{example}

	\begin{method}[Quotient rule]
		Assume we have an equation such as $K_iA^{jk} = B_i^{\ jk}$ or $K_i^{\ j}A_{jl}^{\ \ k} = B_{il}^{\ \ k}$ with $A$ and $B$ two known tensors\footnotemark. The quotient rule asserts the following: ''If the equation of interest holds under all transformations, then $K$ is a tensor of the indicated rank and covariant/contravariant character''.
		\footnotetext{This rule does not necessarily hold when $B = 0$ because transformation rules are not defined for the zero-tensor.}
	\end{method}
	\sremark{This rule is a useful substitute for the ''illegal'' division of tensors.}

\section{Tensor operations}
\subsection{General operations}

	\newdef{Contraction}{\index{contraction}
	    	Let $A$ be a tensor of type $(n, m)$. Setting a sub- and superscript equal and summing over this index gives a new tensor of type $(n-1, m-1)$. This operation is called the contraction of $A$. It is given by the evaluation map\footnote{See also definition \ref{cat:dual} for a generalization of this map.}
	    	\begin{gather}
	    		\label{tensor:contraction}
	    		V\otimes V^*:e_i\otimes e^j\mapsto e^j(e_i)
	    	\end{gather}
	}
    
	\newdef{Direct product}{\index{direct product}
	    	Let $A$ and $B$ be two random tensors. The tensor constructed by the componentwise multiplication of $A$ and $B$ is called the direct product of $A$ and $B$.
	}
	\begin{example}
		Let $A_{\ k}^i$ and $B_{\ lm}^j$ be two tensors. The direct product is equal to: \[C_{\ k\ lm}^{i\ j} = A_{\ k}^iB_{\ lm}^j\]
	\end{example}
    
	\newformula{Operator product}{
    		It is also possible to combine operators working on different vector spaces so to make them work on the tensor product space. To do this we use following definition:
        	\begin{gather}
	        	\label{tensor:operator_product}
			(\hat{A}\otimes\hat{B})(v\otimes w) = (\hat{A}v)\otimes(\hat{B}w).
	        \end{gather}
	}
	\begin{remark*}
	    	Consider an operator $\hat{A}$ working on a space $V_1$. When working with a combined space $V_1\otimes V_2$ the corresponding operator is in fact $\hat{A}\otimes\mathbbm{1}$ but it is often still denoted by $\hat{A}$ in physics.
	\end{remark*}
	
	\begin{notation}
		Consider a tensor with two indices $T_{ij}$. The antisymmetric part is sometimes written as follows:
		\begin{gather}
			T_{[ij]} = \frac{1}{2}\left(T_{ij} - T_{ji}\right).
		\end{gather}
	\end{notation}
	
	\begin{property}[Derivative of tensor product]
		The derivative of a tensor product can be defined using the Leibniz rule:
		\begin{gather}
        		\nabla\cdot(\vector{A}\otimes\vector{B}) = (\nabla\cdot\vector{A})\vector{B}+(\vector{A}\cdot\nabla)\vector{B}
		\end{gather}
	\end{property}    
	
\subsection{Determinant}

	\newdef{Form}{\index{form}
		An $n$-form is a totally antisymmetric element $\omega\in\mathcal{T}^0_nV$.
	}
	\newdef{Volume form}{\index{volume!form}
		A form of rank $\dim(V)$ is also called a \textbf{top form} or \textbf{volume form}.
	}

	\newdef{Determinant}{\index{determinant}
		Consider a finite-dimensional vector space $V$ with basis $\{e_i\}_{i\leq n}$. Let $\varphi$ be a tensor in $\mathcal{T}^1_1V\cong\text{End}(V)$ and let $\omega$ be a volume form on $V$. The determinant of $\varphi$ is then defined as follows:
		\begin{gather}
			\det\varphi = \frac{\omega(\varphi(e_1), ..., \varphi(e_n))}{\omega(e_1, ..., e_n)}.
		\end{gather}
		This definition is well-defined, i.e. it is independent of the choice of volume form and basis. Furthermore it coincides with definition \ref{linalgebra:operator_determinant}.
		
		One should note that the determinant is only well-defined for $(1,1)$-tensors. Although other types of tensors can also be represented as matrices, definition \ref{linalgebra:operator_determinant} would not be independent of a choice of basis anymore. A more general concept can be defined using principal bundles and more precisely frame bundles (see section \ref{manifolds:section:principal_bundles}).
	}
	
	\begin{result}
		\begin{gather}
			\omega(e_1, ..., e_{i-1}, X, e_{i+1}, ..., e_n) = X_i
		\end{gather}
	\end{result}
    
\subsection{Levi-Civita tensor}
	
	\newdef{Levi-Civita tensor}{\index{Levi-Civita!symbol}
    		Let $e^i$ be the dual of $e_i$. In $n$ dimensions, we define the Levi-Civita tensor as follows:
    		\begin{gather}
        		\label{tensor:levi_civita_symbol}
			\boldsymbol{\varepsilon} = \varepsilon_{12...n}e^1\otimes e^2\otimes...\otimes e^n
        	\end{gather}
		where
        	\[
        	\varepsilon_{i...n} = 
        	\left\{
        	\begin{array}{rcl}
			1&\qquad&\text{if }(i...n)\text{ is an even permutation of }(12...n)\\
                	-1&\qquad&\text{if }(i...n)\text{ is an odd permutation of }(12...n)\\
	                0&\qquad&\text{if any of the indices occurs more than once}
		\end{array}
        	\right.
		\]
	}
	\begin{remark}\label{tensor:remark:levi_civita_symbol}
		The Levi-Civita symbol is not a tensor, but a pseudotensor. This means that the sign changes under reflections (or any transformation with determinant $-1$). To turn it into a proper tensor one should multiply it by a factor $\sqrt{g}$ where $g$ is the determinant of the metric.
    	\end{remark}

	\newformula{Cross product}{
		By using the Levi-Civita symbol, we can define the $i^{th}$ component of the cross product\footnote{Following from remark \ref{tensor:remark:levi_civita_symbol} we can see that the cross product is in fact not a vector, but a pseudovector.} as follows:
		\begin{gather}
			(\vector{v}\times\vector{w})_i = \varepsilon_{ijk}v_jw_k.
		\end{gather}
	}

\subsection{Complexification}

	\newdef{Complexification}{\index{complexification}\label{tensor:complexification}
		Let $V$ be a real vector space. The complexification of $V$ is defined as the following tensor product:
		\begin{gather}
			V^{\mathbb{C}} = V\otimes\mathbb{C}.
		\end{gather}
		This can still be considered a real vector space, but we can also turn it into a complex vector space by generalizing the scalar product as follows:
		\begin{gather}
			\alpha(v\otimes\beta) = v\otimes(\alpha\beta)
		\end{gather}
		for all $\alpha\in\mathbb{C}$.
	}
	\begin{property}
		By noting that every element $v_{\mathbb{C}}\in V^{\mathbb{C}}$ can be written as \[v_{\mathbb{C}} = (v_1\otimes1) + (v_2\otimes i)\] we can decompose the complexification as follows:
		\begin{gather}
			V^{\mathbb{C}} \cong V\oplus iV.
		\end{gather}
	\end{property}

\section{Symmetrized tensors}
    
\subsection{Antisymmetric tensors}

	\newdef{Antisymmetric tensor}{\index{antisymmetry}
		Tensors that change sign under the interchange of any two indices.
	}
	\begin{notation}[Symmetric tensors]
		\nomenclature[S_SnV]{$S^n(V)$}{Space of symmetric rank $n$ tensors over a vector space $V$.}
		The space of symmetric $(n, 0)$-tensors is denoted by $S^n(V)$. The space of symmetric $(0,n)$-tensors is denoted by $S^n(V^*)$.
	\end{notation}
	\begin{notation}\label{tensor:not:antysimmetric_space}
		\nomenclature[S_zsymLambda]{$\Lambda^n(V)$}{Space of antisymmetric rank $n$ tensors over a vector space $V$.}
		The space of antisymmetric $(n, 0)$-tensors is denoted by $\Lambda^n(V)$. The space of antisymmetric $(0, n)$-tensors is denoted by $\Lambda^n(V^*)$.
	\end{notation}
    
	\begin{property}
    		Let $n = \dim(V)$. The space $\Lambda^r(V)$ equals the zero space for all $r\geq n$.
	\end{property}
    
\subsection{Wedge product}

	\begin{formula}[Antisymmetrization]\label{tensors:antisymmetrization}
		Let $\{P_i\}_i = S_k$ be the set of all permutations of the sequence $(1, ..., k)$. We define the antisymmetrization operator Alt as follows:
		\begin{gather}
			\text{Alt}(e_1\otimes...\otimes e_k) = \sum_i \sgn(P_i)e_{P_i(1)}\otimes...\otimes e_{P_i(k)}.
		\end{gather}
	\end{formula}

	\newdef{Wedge product}{\index{wedge!product}
		Let $\{e_i\}_{1\leq i\leq \dim(V)}$ be a basis for $V$. The wedge product of basisvectors is defined as follows:
		\begin{gather}
			\label{tensor:wedge_product}
    			e_1 \wedge ... \wedge e_k = \text{Alt}(e_1\otimes...\otimes e_k)
    		\end{gather}
		From this definition it immediately follows that the wedge product is (totally) antisymmetric.
	}
    
	\begin{construct}
    		Let $\{e_i\}_{1 \leq i\leq \dim(V)}$ be a basis for $V$. It is clear from the above definition that a basis for $\Lambda^r(V)$ is given by
		\[
			\{e_{i_1}\wedge...\wedge e_{i_r}\ :\ \forall k: 1\leq i_k \leq \dim(V)\}.
		\]
		The dimension of this space is given by
		\begin{gather}
			\label{tensor:wedge_dimension}
			\dim\Lambda^r(V) = \binom{n}{r}.
		\end{gather}
	\end{construct}
	\begin{remark}
		For $k=0$, the above construction is not useful, so we just define $\Lambda^0(V) = \mathbb{R}$.
	\end{remark}
	
	\begin{formula}
		Let $v\in\Lambda^r(V)$ and $w\in\Lambda^m(V)$. The wedge product \ref{tensor:wedge_product} can be generalized as follows:
		\begin{gather}
			v\wedge w = \frac{1}{r!m!}\text{Alt}(v\otimes w)
		\end{gather}
		where the antisymmetrization operator $\text{Alt}$ is defined in equation \ref{tensors:antisymmetrization}.
	\end{formula}
	
	\begin{definition}[Blades]\index{blade}\index{pure!vector}
		Elements of $\Lambda^k(V)$ that can be written as the wedge product of $k$ vectors are generally known as $k$-\textbf{blades} or \textbf{pure $k$-vectors}.
	\end{definition}

	\newformula{Levi-Civita symbol}{\index{Levi-Civita!symbol}
	    	The Levi-Civita tensor in $n$ dimensions as introduced in \ref{tensor:levi_civita_symbol} can now be rewritten more concisely as:
		\begin{gather}
			\boldsymbol{\varepsilon} = e_1\wedge...\wedge e_n
		\end{gather}
	}

	\begin{formula}\index{cross!product}
	    	In 3 dimensions there exists an important isomorphism $J:\Lambda^2(\mathbb{R}^3)\rightarrow\mathbb{R}^3$:
		\begin{gather}
			\label{tensor:wedge_to_cross}
				J(\lambda)^i = \frac{1}{2}\varepsilon^i_{\ jk}\lambda^{jk}
		\end{gather}
		where $\lambda\in\Lambda^2(\mathbb{R}^3)$.

		Looking at the definition of the cross product \ref{linalgebra:cross_product}, we can see that $\vector{v}\times\vector{w}$ is actually the same as $J(\vector{v}\wedge\vector{w})$. One can thus use the wedge product to generalize the cross product to higher dimensions.
	\end{formula}
    
	\begin{example}
	    	Let $a, b$ and $c$ be three vectors in $V$. Now consider the following expression:
		\[
			(c\wedge b)(L(a), \cdot)
		\]
		where $L(a)$ is the metric dual of $a$ (see \ref{linalgebra:metric_dual}). Evaluating this formula using the properties of the wedge and tensor products leads to the well-known BAC-CAB rule of triple cross products:
		\[
			(c\cdot a)b - (b\cdot a)c
		\]
	\end{example}

\subsection{Exterior algebra}
	
	\newdef{Exterior power}{\index{exterior!power}\index{form}
		In the theory of exterior algebras, the space $\Lambda^k(V)$ is often called the $k^{th}$ exterior power of $V$. Its elements are called (exterior) \textbf{$k$-forms}.
	}
	\newdef{Exterior algebra}{\index{exterior!algebra}\index{Grassmann!algebra}\label{tensor:exterior_algebra}
		We can define a graded vector space\footnote{See definition \ref{linalgebra:graded_vector_space}.} $\Lambda^*(V)$ as follows:
		\begin{gather}
			\Lambda^*(V) = \bigoplus_{k\geq0}\Lambda^k(V)
		\end{gather}
		Then we can turn this graded vector space into a graded algebra by taking the wedge product as the multiplication:
		\begin{gather}
			\wedge:\Lambda^k(V)\times\Lambda^l(V)\rightarrow \Lambda^{k+l}(V)
		\end{gather}
		This algebra is called the exterior algebra or \textbf{Grassmann algebra} over $V$.
	}

	\newadef{$\dag$}{\label{tensor:adef_exterior_algebra}
    		Let $T(V)$ be the tensor algebra over the vector space $V$, i.e.
	    	\begin{gather}
			T(V) = \bigoplus_{k\geq0} V^{\otimes k}
		\end{gather}
		The exterior algebra over $V$ is generally defined as the quotient of $T(V)$ by the two-sided ideal $I$ generated by $\{v\otimes v:v\in V\}$.
	}
	
	\begin{property}
		The exterior algebra is both a unital associative algebra (with identity $1\in\mathbb{R}$) and a coalgebra. Furthermore, it is also commutative in the graded sense (see \ref{group:graded_commutativity}).
	\end{property}
	
	\begin{property}
		The graded commutativity implies that the wedge product of any odd exterior form with itself is identically 0. The wedge product of an even exterior form with itself vanishes if and only if the form can be decomposed as a product of 1-forms, i.e. it is a pure $k$-form.
	\end{property}
	
\subsection{Hodge star}

	Equation \ref{tensor:wedge_dimension} tells us that the spaces $\Lambda^k(V)$ and $\Lambda^{n-k}(V)$ have the same dimension, hence there exists an isomorphism between them. This map is given by the Hodge star operator. We should however note that this map can only be defined independent of the choice of (ordered) basis if we restrict ourselves to vector spaces equipped with a nondegenerate Hermitian form \ref{linalgebra:NDH_form}.

	When equipped with an inner product and hence an orthonormal basis $\{e_i\}_{i\leq\dim V}$, every finite-dimensional vector space admits a canonical volume form given by $\text{Vol} = e_1\wedge...\wedge e_n$. This will be the convention adopted in the remainder of this section.

	\newdef{Orientation}{\index{orientation}\label{tensor:orientation}
		Let Vol be the standard volume form on the vector space $V$. From the definition of a volume form it follows that every other $\dim(V)$-form is a scalar multiple of $\text{Vol}$ (lets denote it by $r$). Hence the choice of volume form induces an orientation on $V$: If $r>0$ then the orientation is said to be \textbf{positive}, if $r<0$ then the orientation is said to be \textbf{negative}.
	}
	
	\begin{formula}[Inner product]\index{inner!product}
		Let $V$ be equipped with an inner product $\langle\cdot,\cdot\rangle$. Then we can define an inner product on $\Lambda^k(V)$ by
		\begin{gather}
			\label{tensor:wedge_inner_product}
			\langle v_1\wedge...\wedge v_k | w_1\wedge...\wedge w_k \rangle_k = \det(\langle v_i, w_j\rangle).
		\end{gather}
		For an orthogonal basis this formula factorizes into
		\begin{gather}
			\langle v_1\wedge...\wedge v_k | w_1\wedge...\wedge w_k \rangle_k = \langle v_1|w_1 \rangle\cdots \langle v_k|w_k \rangle.
		\end{gather}
	\end{formula}
	\newdef{Hodge star}{\index{Hodge star}
		The Hodge star $\ast: \Lambda^k(V)\rightarrow\Lambda^{n-k}(V)$ is defined as the isomorphism such that for all $\omega\in\Lambda^k(V)$ and $\rho\in\Lambda^{n-k}(V)$ we have the following equality:
		\begin{gather}
			\label{tensor:hodge_star}
			\omega\wedge\rho = \langle\ast\omega, \rho\rangle_{n-k}\text{Vol}(V)
		\end{gather}
		where $\langle\cdot,\cdot\rangle$ is the inner product \ref{tensor:wedge_inner_product} on $\Lambda^{n-k}(V)$. Furthermore, this isomorphism is unique.
		\begin{proof}
			Because $\omega\wedge\rho$ is an element of $\Lambda^n(V)$ it is a scalar multiple of $\text{Vol}(V)$. This implies that it can be written as \[c(\rho)\text{Vol}(V)\] The map $c : \Lambda^{n-k}(V)\rightarrow\mathbb{R}:\rho\mapsto c(\rho)$ is a linear map and thus a continuous map, so we can apply Riesz' representation theorem to identify $c$ with a unique element $\ast\omega\in\Lambda^{n-k}(V)$ such that \[c(\rho) = \langle\ast\omega, \rho\rangle_{n-k}\]
		\qed
		\end{proof}
	}
	
	\begin{formula}
		Let $\{e_i\}_{i\leq n}$ be a positively oriented ordered orthonormal (possibly in a Lorentzian signature) basis for $V$. An explicit formula for the Hodge star is given by the following construction. Let $\{i_1, ..., i_k\}$ and $\{j_1 ,...,j_{n-k}\}$ be two complementary index sets with increasing subindices. Let $\omega = e_{i_1}\wedge...\wedge e_{i_k}$.
		\begin{gather}
			\label{tensor:explicit_hodge_star}
			\boxed{\ast\omega =\sgn(\tau)\prod_{m = 1}^{n-k}\langle e_{j_m}|e_{j_m} \rangle e_{j_1}\wedge...\wedge e_{j_{n-k}}}
		\end{gather}
		where $\tau$ is the permutation that maps $e_{i_1}\wedge...\wedge e_{i_k}\wedge e_{j_1}\wedge...\wedge e_{j_{n-k}}$ to $\text{Vol}(V)$.
	\end{formula}
	
	\begin{result}
		\label{tensor:hodge_star_vectorcalculus}
		Consider three vectors $u, v, w\in\mathbb{R}^3$.
		\begin{align}
			\ast(v\wedge w) &= v\times w \label{tensor:cross_by_hodge_star}\\
			\ast(v\times w) &= v\wedge w\\
			\ast(u\wedge v\wedge w) &= u\cdot(v\times w)
		\end{align}
	\end{result}
	\begin{remark}
		Formula \ref{tensor:wedge_to_cross} is an explicit evaluation of the first equation \ref{tensor:cross_by_hodge_star}.
		\begin{proof}
			The sign $\sgn(\tau)$ can be written using the Levi-Civita symbol $\varepsilon_{ijk}$ as defined in \ref{tensor:levi_civita_symbol}. The factor $\frac{1}{2}$ is introduced to correct for the double counting due to the contraction over both the indices $j$ and $k$.
		\end{proof}
	\end{remark}
	
	\begin{property}
		Consider an inner product space $V$, then
		\begin{gather}
			\boxed{\ast\ast\ \omega = (-1)^{k(n-k)}\omega}
		\end{gather}
		In $n=4$ this leads to $\ast\ast\omega = \omega$ which means that the Hodge star is an involution in 4-dimensional inner product spaces.
	\end{property}
	
	\newdef{Self-dual}{\index{dual!self-dual}
		Let $V$ be a 4-dimensional inner product space. Consider $\omega\in\Lambda^2(V)$. Then $\omega$ is said to be self-dual if $\ast\omega = \omega$. Furthermore every $v\in\Lambda^2(V)$ can be uniquely decomposed as the sum of a self-dual and an anti-self-dual 2-form.
	}
	
\subsection{Grassmann numbers}

	Although this section does not really belong to the chapter about tensors, we have included it here as it is an application of the concept of exterior algebras. The concept of Grassmann numbers (or variables) is used in QFT when performing calculations in the fermionic sector.
	
	\newdef{Grassmann numbers}{\index{Grassmann!number}\label{tensor:grassmann_number}
		Let $V$ be a complex vector space spanned by a set of generators $\theta_i$. The Grassmann algebra with Grassmann variables $\theta_i$ is the exterior algebra over $V$. The wedge symbol of Grassmann variables is often ommitted when writing the product: $\theta_i\wedge\theta_j \equiv \theta_i\theta_j$.
	}
	\begin{remark}
		Furthermore, from the anti-commutativity it follows that we can regard the Grassmann variables as being non-zero square-roots of zero.
	\end{remark}
	
	\begin{property}
		Consider a one-dimensional Grassmann algebra. When constructing the polynomial ring $\mathbb{C}[\theta]$ generated by $\theta$, we see that, due to the anti-commutativity, $\mathbb{C}[\theta]$ is spanned only by $1$ and $\theta$. All higher degree terms vanish because $\theta^2 = 0$. This implies that the most general polynomial over a one-dimenisonal Grassmann algebra can be written as
		\begin{gather}
			p(\theta) = a + b\theta
		\end{gather}
	\end{property}
	
	\begin{definition}\index{DeWitt!convention}
		We can equip the exterior algebra $\Lambda$ with Grassmann variables $\theta_i$ with an involution similar to that on $\mathbb{C}$:
		\begin{gather}
			(\theta_i\theta_j...\theta_k)^* = \theta_k...\theta_j\theta_i
		\end{gather}
		Elements $z\in\Lambda$ such that $z^* = z$ are called \textbf{(super)real}, elements such that $z^* = -z$ are called \textbf{(super)imaginary}. This convention is called the \textit{DeWitt} convention.
	\end{definition}

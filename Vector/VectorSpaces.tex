\section{Vector spaces}

    \newdef{$K$-vector space}{\label{linalgebra:vector_space}
        Let $K$ be a field. A $K$-vector space $V$ is a set equipped with two operations, \textbf{(vector) addition} $V\times V\rightarrow V$ and \textbf{scalar multiplication} $K\times V\rightarrow V$, that satisfy the following axioms:
        \begin{enumerate}
            \item $V$ forms an Abelian group under vector addition.
            \item Scalar multiplication is associative: $\lambda(\mu v) = (\lambda\mu)v$ for all $\lambda,\mu\in K$ and $v\in V$.
            \item The identity of the field $K$ acts as a neutral element for scalar multiplication: $1_Kv = v$ for all $v\in V$.
            \item Scalar multiplication is distributive with respect to vector addition: $\lambda(v+w) = \lambda v + \lambda w$ for all $\lambda\in K$ and $v,w\in V$.
            \item Vector addition is distributive with respect to scalar multiplication: $(\lambda+\kappa)v = \lambda v + \kappa w$ for all $\lambda,\kappa\in K$ and $v\in V$.
        \end{enumerate}
    }
    From here on the underlying field $K$ will be left implicit unless the results depend on it.

    \remark{The above definition can be restated in abstract algebraic terms. A $K$-vector space is a module \ref{algebra:module} over $K$.}

\subsection{Linear independence}

    \newdef{Linear combination}{\label{linalgebra:linear_combination}
        The vector $w$ is a linear combination of elements in the set $\{v_i\}_{i\leq n}\subset V$ if it can be written as
        \begin{gather}
            w = \sum_{i=1}^n\lambda_i v_i
        \end{gather}
        for some $\{\lambda_i\}_{i\leq n}\subset K$. One can generalize this to general subsets $S\subseteq V$, but the number of nonzero elements $\lambda_i$ is always required to be finite.\footnote{Generalizations are possible in the context of topological vector spaces (see Chapters \ref{chapter:topology} and \ref{chapter:functional}), where one can define the notion of convergence.} (See the remark about \textit{Hamel bases} in next section.)
    }
    \newdef{Linear independence}{\label{linalgebra:linear_independence}
        A finite set $\{v_i\}_{i\leq n}$ is said to be linearly independent if the following relation holds:
        \begin{gather}
            \sum_{i=1}^n\lambda_i v_i = 0 \iff \forall i\leq n:\lambda_i = 0.
        \end{gather}
        A general set $S\subset V$ is said to be linearly independent if every finite subset of it is linearly independent.
    }

    \newdef{Span}{\index{span}
        A set of vectors $S\subseteq V$ is said to span $V$ if every vector $v\in V$ can be written as a linear combination of elements in $S$.
    }

    \newdef{Frame}{\index{frame}
        A $k$-frame is an ordered set of $k$ linearly independent vectors.
    }

\subsection{Bases}

    \newdef{Basis}{\index{basis}
        A subset $\mathcal{B}\subset V$ that is linearly independent and spans $V$.
    }
    \begin{property}
        Every spanning set contains a basis.
    \end{property}

    \begin{remark}[Hamel basis]\index{Schauder|see{basis}}\index{Hamel|see{basis}}
        In the previous definition the concept of a Hamel basis was implicitly used. This concept is based on two conditions:
        \begin{enumerate}
            \item The basis is linearly independent.
            \item Every element in the vector space can be written as a linear combination of a \underline{finite} subset of the basis.
        \end{enumerate}
        For bases consisting of a finite number of vectors, one does not have to worry. However, for infinite bases one has to keep this in mind. An alternative construction that allows for combinations of a countably infinite number of elements, is given by that of a \textit{Schauder basis}.
    \end{remark}
    Nonetheless, it can be shown that every vector space admits a Hamel basis:
    \begin{construct}[\difficult{Hamel basis}]\index{basis!Hamel}\label{linalgebra:hamel_basis}
        Let $V$ be a vector space and consider the set of all linearly independent subsets of $V$. Under the relation of inclusion this set becomes a partially ordered set \ref{set:poset}. Zorn's lemma \ref{set:zorns_lemma} then says that there exists at least one maximal linearly independent set.

        Now, one can show that this maximal subset $S$ is also a spanning set of $V$. Choose a vector $v\in V$ that is not already in $S$. From the maximality of $S$ it follows that $S\cup v$ is linearly dependent and, hence, there exists a finite sequence of scalars $(a^1,\ldots,a^n,b)$ and a finite sequence of elements $(e_1,\ldots,e_n)$ in $S$ such that:
        \begin{gather}
            \sum_{i=0}^n a^ie_i + bv = 0,
        \end{gather}
        where not all scalars are zero. This implies that $b\neq0$, because otherwise the set $\{e_i\}_{i\leq n}$ and, hence, also $S$ would be linearly dependent. It follows that $v$ can be written as\footnote{It is this step that requires $R$ to be a division ring in Property \ref{algebra:module_basis} because otherwise one would in general not be able to divide by $b\in R$.}
        \begin{gather}
            v = -\frac{1}{b}\sum_{i=0}^na^ie_i.
        \end{gather}
        Because $v$ was randomly chosen, one can conclude that $S$ is a spanning set for $V$.
    \end{construct}
    \begin{remark*}
        This construction assumes the axiom of choice in set theory, only ZF does not suffice. It can even be shown that the existence of a Hamel basis for every vector space is equivalent to the axiom of choice.
    \end{remark*}

    \begin{property}
        Every basis of a vector space has the same number of elements. For infinite-dimensional spaces this means that all bases have the same \textit{cardinality}.
    \end{property}
    \newdef{Dimension}{\index{dimension}\label{linalgebra:dimension}
        Let $V$ be a finite-dimensional vector space and let $\mathcal{B}$ be a basis for $V$ with $n$ elements. With the previous property in mind, the dimension of $V$ is defined as follows:
        \begin{gather}
            \dim(V) := n.
        \end{gather}
    }

    \newdef{Subspace}{\label{linalgebra:subspace}
        Let $V$ be a vector space. A subset $W$ of $V$ is called a subspace if $W$ is itself a vector space under (the restriction of) the operations of $V$:
        \begin{gather}
            W\leq V\iff\forall w_1,w_2\in W,\forall\lambda\in K:\lambda w_1 + w_2 \in W.
        \end{gather}
    }

\subsection{Sum and direct sum}

    \newdef{Sum}{\index{sum}
        \nomenclature[O_zsymbinsum]{$X+Y$}{sum of the vector spaces $X$ and $Y$}
        Let $V$ be a vector space and consider a finite collection of subspaces $\{W_1,\ldots,W_k\}$. The sum of these subspaces is defined as follows:
        \begin{gather}
            W_1+\cdots+W_k := \left\{\sum_{i=1}^kw_i\,\middle\vert\,w_i\in W_i\right\}.
        \end{gather}
        For an infinite collection of subspaces the linear combinations have to be finite.
    }
    \newdef{Direct sum}{\index{direct!sum}\label{linalgebra:direct_sum}
        \nomenclature[O_zsymbinsump]{$X\oplus Y$}{direct sum of the vector spaces $X$ and $Y$}
        If every element $v$ of the sum can be written as a unique linear combination, the sum is called a direct sum.
    }
    \newnot{Direct sum}{
        The direct sum of vector spaces is denoted by
        \begin{gather*}
            W_1\oplus\cdots\oplus W_k\equiv\bigoplus_{i=1}^kW_i.
        \end{gather*}
    }

    \begin{formula}
        Let $V$ be a finite-dimensional vector space and consider two subspaces $W_1,W_2\leq V$. The dimensions of these spaces can be related in the following way:
        \begin{gather}
            \dim(W_1+W_2) = \dim(W_1) + \dim(W_2) - \dim(W_1\cap W_2).
        \end{gather}
    \end{formula}
    \begin{property}
        Let $V$ be a vector space and assume that $V$ can be decomposed as $W=W_1\oplus W_2$. If $\mathcal{B}_1$ is a basis of $W_1$ and if $\mathcal{B}_2$ is a basis of $W_2$, then $\mathcal{B}_1\cup\mathcal{B}_2$ is a basis of $W$.
    \end{property}

    \newdef{Complement}{\index{complement}
        Let $V$ be a vector space and let $W$ be a subspace of $V$. A subspace $W'$ of $V$ is called a complement of $W$ if $V = W\oplus W'$.
    }
    \begin{property}[Existence of complements]\label{linalgebra:complement}
        Let $V$ be a vector space and let $U,W$ be two subspaces of $V$. If $V = U+W$, there exists a subspace $Y\leq U$ such that $V = Y\oplus W$. In particular, every subspace of $V$ has a complement in $V$.
    \end{property}

\section{Linear maps}

    \remark{Linear maps are also called \textbf{linear transformations} or \textbf{linear mappings}.}

\subsection{Homomorphisms}

    \newdef{Homomorphism space}{\index{morphism!of vector spaces}\label{linalgebra:hom_space}
        \nomenclature[S_VectK]{$\mathbf{Vect}_K$}{category of vector spaces and linear maps over a field $K$}
        Let $V,W$ be two vector spaces. The set of all linear maps between $V$ and $W$ is called the homomorphism space from $V$ to $W$:
        \begin{gather}
            \hom_K(V,W) := \big\{f:V\rightarrow W\,\big\vert\,f\text{ is linear}\big\}.
        \end{gather}
        The collection of $K$-vector spaces and linear maps between them form a category $\mathbf{Vect}_K$.
    }
    \begin{formula}\label{linalgebra:hom_dimension}
        Let $V,W$ be two finite-dimensional vector spaces.
        \begin{gather}
            \dim\big(\hom_K(V,W)\big) =\dim(V)\dim(W)
        \end{gather}
    \end{formula}

    \newdef{Endomorphism ring}{\index{endomorphism}
        The space $\hom_K(V,V)$ with composition of maps as multiplication forms a ring, the endomorphism ring. It is denoted by $\End_K(V)$ or $\End(V)$ when the underlying field is clear.
    }
    \begin{property}[Commutator]\index{commutator}
        The endomorphism ring $\End(V)$ can also be endowed with the structure of a Lie algebra (Property \ref{lie:end_as_lie_algebra}) by equipping it with the commutator
        \begin{gather}
            [A,B] := A\circ B - B\circ A.
        \end{gather}
    \end{property}

    \begin{property}
        Let $V$ be finite-dimensional vector space and let $f:V\rightarrow V$ be an endomorphism. The following statements are equivalent:
        \begin{itemize}
            \item $f$ is injective.
            \item $f$ is surjective.
            \item $f$ is bijective.
        \end{itemize}
    \end{property}

    \newdef{Automorphism}{\index{automorphism}\index{general linear group}\label{linalgebra:automorphism}
        \nomenclature[S_GL]{$\GL(V)$}{general linear group, the group of automorphisms of a vector space $V$}
        An isomorphism from $V$ to $V$ is called an automorphism. The set of all automorphisms  on $V$ is denoted by $\Aut(V)$. It forms a group under composition. Often this group is called the general linear group\footnote{It is isomorphic to the general linear group of invertible matrices \ref{linalgebra:GL_matrices} (hence the similar name and notation).} $\GL_K(V)$ or $\GL(V)$ when the underlying field is clear.
    }
    \remark{Sometimes automorphisms are also called \textbf{linear operators}. However, this terminology is also used for a general linear map in operator theory (Chapter \ref{chapter:operator_algebras}) and so this terminology is not adopted in this text.}

    \newdef{Kernel}{\index{kernel}
        Consider a linear map $f:V\rightarrow W$. The kernel of $f$ is defined as the following subspace of $V$:
        \begin{gather}
            \ker(f) := \{v\in V\mid f(v) = 0\}.
        \end{gather}
    }
    \begin{property}
        A linear map $f:V\rightarrow W$ is injective if and only if $\text{ker}(f) = 0$.
    \end{property}

    \newdef{Rank}{\index{rank}\label{linalgebra:image_rank}
        The dimension of the image of a linear map.
    }
    \newdef{Nullity}{\index{nullity}
        The dimension of the kernel of a linear map.
    }

    \begin{theorem}[Dimension theorem\footnotemark]\index{rank-nullity theorem}\label{linalgebra:dimension_theorem}
        \footnotetext{Also called the \textbf{rank-nullity theorem}.}
        Let $f:V\rightarrow W$ be a linear map.
        \begin{gather}
            \dim(\im(f)) + \dim(\ker(f)) = \dim(V)
        \end{gather}
    \end{theorem}
    \begin{result}\label{linalgebra:dimension_isomorphism}
        Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.
    \end{result}

    \newdef{Minimal polynomial}{\index{minimal!polynomial}
        Let $f\in\End(V)$ with $V$ a finite-dimensional vector space. The monic polynomial $\mu_f$ of lowest order such that $\mu_f(f)=0$ is called the minimal polynomial of $f$.
    }
    \begin{property}\label{linalgebra:minimal_polynomial_divisor}
        Let $f\in\End(V)$ with minimal polynomial $\mu_f$. If $\varphi(f) = 0$ for some polynomial $\varphi$, the minimal polynomial $\mu_f$ divides $\varphi$.
    \end{property}

    \begin{property}[Jordan-Chevalley decomposition]\index{Jordan-Chevalley decomposition}\index{semisimple!operator}\index{nilpotent}\label{linalgebra:jordan_chevalley}
        Every endomorphism $A$ can be decomposed as follows:
        \begin{gather}
            A = A_{ss} + A_n,
        \end{gather}
        where
        \begin{itemize}
            \item $A_{ss}$ is \textbf{semisimple}: for every invariant subspace of $A_{ss}$ there exists an invariant complementary subspace.
            \item $A_n$ is \textbf{nilpotent}: $\exists k\in\mathbb{N}:A_n^k = 0$.
        \end{itemize}
        Furthermore, this decomposition is unique and the endomorphisms $A_{ss},A_n$ can be written as polynomials in $A$.
    \end{property}

\subsection{Dual maps}

    \newdef{Dual space}{\index{dual!space}\index{linear!form}\index{functional}
        Let $V$ be a vector space. The (algebraic) dual $V^*$ of $V$ is defined as the following vector space:
        \begin{gather}
            \label{linalgebra:dual_space}
            V^*:=\hom_K(V,K)=\big\{f:V\rightarrow K\,\big\vert\,f\text{ is linear}\big\}.
        \end{gather}
        The elements of $V^*$ are called \textbf{linear forms} or (linear) \textbf{functionals}.
    }
    \begin{property}[Dimension]\label{linalgebra:dual_space_dimension}
        From Theorem \ref{linalgebra:hom_dimension} it follows that $\dim(V^*)=\dim(V)$ whenever $V$ is finite-dimensional. If $V$ is infinite-dimensional, this property is \underline{never} valid. In the infinite-dimensional case $\mathrm{card}(V^*)>\mathrm{card}(V)$ always holds.
    \end{property}

    \newdef{Dual basis}{\index{basis}\label{linalgebra:dual_basis}
        Let $\mathcal{B} = \{e_1,e_2,\ldots,e_n\}$ be a basis for a finite-dimensional vector space $V$. One can construct a basis $\mathcal{B}^* = \{\varepsilon_1,\varepsilon_2,\ldots,\varepsilon_n\}$ for $V^*$, called the dual basis of $\mathcal{B}$, as follows:
        \begin{gather}
            \varepsilon_i:\sum_{j=1}^na_ie_i\mapsto a_i.
        \end{gather}
        The relation between a basis and its associated dual basis can also be expressed as
        \begin{gather}
            \label{linalgebra:dual_basis_2}
            \varepsilon^i(e_j) = \delta^i_j.
        \end{gather}
    }
    \newdef{Natural pairing}{\index{natural!pairing}\label{linalgebra:natural_pairing}
        The definition of the dual basis extends to a natural pairing of $V$ and its dual $V^*$ in terms of the following bilinear map:
        \begin{gather}
            \langle v,v^* \rangle := v^*(v).
        \end{gather}
        (See Definition \ref{cat:dual} for a generalization of this map.)
    }

    \newdef{Dual map}{\index{dual!map}\index{transpose}\label{linalgebra:transpose}
        Let $f:V\rightarrow W$ be a linear map. The linear map
        \begin{gather}
            f^*:W^*\rightarrow V^*:\varphi\rightarrow\varphi\circ f
        \end{gather}
        is called the dual map or \textbf{transpose} of $f$. It is also often denoted by $f^T$.
    }